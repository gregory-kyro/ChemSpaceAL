{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mps'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import pkg_resources\n",
        "pkg_resources.require(\"pandas==1.5.3\")\n",
        "import pandas as pd\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Generative_ML/current_data/' #@param {type:\"string\"}\n",
        "BASE_PATH = '/Users/morgunov/batista/Summer/pipeline/' #@param {type:\"string\"}\n",
        "TRAINING_PATH = BASE_PATH + 'raw_data/combined_train.csv' #@param {type:\"string\"}\n",
        "VALIDATION_PATH = BASE_PATH + 'raw_data/combined_val.csv' #@param {type:\"string\"}\n",
        "CHECKPOINTS_PATH = BASE_PATH + 'checkpoints/' #@param {type:\"string\"}\n",
        "PICKLES_PATH = BASE_PATH + 'pickles/' #@param {type:\"string\"}\n",
        "INFERENCE_PATH = BASE_PATH + 'inferences/' #@param {type:\"string\"}\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "REGEX_PATTERN = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|!|\\*|\\$|\\%[0-9]{2}|[0-9])\" #@param {type:\"string\"}\n",
        "\n",
        "date, time = str(pd.to_datetime('today', utc=True)).split()\n",
        "FILE_SUFFIX = '_'+'_'.join(date.split('-')[1:]) + '_' + ':'.join(time.split(':')[:2]) # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG_DICT = {\n",
        "    \"device\": DEVICE,\n",
        "    \"att_bias\": False,\n",
        "    \"gpt_bias\": True,\n",
        "    \"att_drop_rate\": 0.1,\n",
        "    \"gpt_drop_rate\": 0.1,\n",
        "    \"n_layer\": 8,\n",
        "    \"n_head\": 8,\n",
        "    \"n_embed\": 256,\n",
        "    \"ff_mult\": 4, # multiplier for Feed Forward number of hidden units inside multihead,\n",
        "    \"doGELU\": True, # else ReLU\n",
        "    \"attention_times\": [],\n",
        "    \"do_flash\": True,\n",
        "    \"desc_path\": CHECKPOINTS_PATH + f'smiles_dataset_attributes{FILE_SUFFIX}.yaml',\n",
        "    \"ckpt_path\": CHECKPOINTS_PATH + f'GPT_pretrain{FILE_SUFFIX}.pt',\n",
        "    \"train_path\": TRAINING_PATH,\n",
        "    \"val_path\": VALIDATION_PATH,\n",
        "    \"wandb_project\": f'GPT_pretrain_pipeline',\n",
        "    \"wandb_runname\": f'GPT_pretrain{FILE_SUFFIX}',\n",
        "    \"slice_data\": 10_000, #False for all data\n",
        "    \"epochs\": 2,\n",
        "    \"batch_size\": 16, #512,\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"betas\": (0.965, 0.99), #(0.9, 0.95)\n",
        "    \"rho\": 0.04, # For SophiaG\n",
        "    \"weight_decay\": 0.1,\n",
        "    \"lr_decay\": False,\n",
        "    \"num_workers\": 0 # number of worker processes to use for loading data\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94C4_8BipUaL"
      },
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzRj6bGQpUaM"
      },
      "source": [
        "## Imports & Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# install necessary packages\n",
        "!pip install rdkit\n",
        "!pip install pandas==1.5.3\n",
        "!pip install molsets\n",
        "!pip install wandb\n",
        "\n",
        "# clone Sophia optimizer GitHub repository\n",
        "!git clone https://github.com/Liuhong99/Sophia.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "zVkT-uNwAUsD",
        "outputId": "8fe1a635-cbbe-40ef-db2e-d842a50b5552"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/molsets-1.0-py3.11.egg/moses/metrics/utils.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  _mcf.append(_pains, sort=True)['smarts'].values]\n"
          ]
        }
      ],
      "source": [
        "# import necessary packages\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem\n",
        "import moses\n",
        "from moses.utils import get_mol\n",
        "from Sophia.sophia import SophiaG\n",
        "import yaml\n",
        "\n",
        "# set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pfZIi6qpUaQ"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size=None, block_size=None, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "    def export_attributes(self, export_path):\n",
        "        with open(export_path, 'w') as f:\n",
        "            yaml.dump(vars(self), f)\n",
        "\n",
        "    def load_attributes(self, load_path):\n",
        "        with open(load_path, 'r') as f:\n",
        "            config_dict = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "        self.__dict__.update(config_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset loading & sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3MhywaNlpUaQ"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0):\n",
        "    block_size = model.get_block_size() # define size of context window used for input conditioning\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # limit conditioning input to the most recent block_size elements\n",
        "        logits, _= model(x_cond) # give input to model and get logits (unnormalized scores or probabilities)\n",
        "        logits = logits[:, -1, :] / temperature # extract the logits for the next token in the sequence\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        ix = torch.multinomial(probs, 1)\n",
        "        x = torch.cat((x, ix), dim=1) # concatenate the chosen token index with the existing sequence\n",
        "    return x\n",
        "\n",
        "def check_novelty(gen_smiles, train_smiles):\n",
        "    if len(gen_smiles) == 0:\n",
        "        novel_ratio = 0\n",
        "    else:\n",
        "        duplicates = [1 for mol in gen_smiles if mol in train_smiles]\n",
        "        novel = len(gen_smiles) - sum(duplicates)\n",
        "        novel_ratio = novel*100/len(gen_smiles)\n",
        "    return novel_ratio\n",
        "\n",
        "def canonic_smiles(smiles_or_mol):\n",
        "    mol = get_mol(smiles_or_mol)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return Chem.MolToSmiles(mol)\n",
        "\n",
        "\n",
        "class SMILESDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data=None, chars=None, block_size=None, len_data = None):\n",
        "        if chars is None:\n",
        "            self.desc_only = True\n",
        "            return\n",
        "        self.desc_only = False\n",
        "\n",
        "        self.vocab_size = len(chars)\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.data = data\n",
        "        self.len_data = len_data\n",
        "\n",
        "    def export_desc_attributes(self, export_path):\n",
        "        attr_dict = {\n",
        "            \"desc_only\": self.desc_only,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"block_size\": self.block_size,\n",
        "            \"stoi\": self.stoi,\n",
        "            \"itos\": self.itos,\n",
        "            \"len_data\": self.len_data\n",
        "        }\n",
        "        with open(export_path, 'w') as f:\n",
        "            yaml.dump(attr_dict, f)\n",
        "\n",
        "    def load_desc_attributes(self, load_path):\n",
        "        with open(load_path, 'r') as f:\n",
        "            attr_dict = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "        self.__dict__.update(attr_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        assert not self.desc_only, \"Dataset is not initialized\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert not self.desc_only, \"Dataset is not initialized\"\n",
        "        smiles = self.data[idx].strip()\n",
        "        # define regular expressin pattern used to identify characters in the SMILES strings\n",
        "        regex = re.compile(REGEX_PATTERN)\n",
        "        smiles_matches = regex.findall(smiles)\n",
        "\n",
        "        # smiles = str('!') + smiles\n",
        "        # smiles += str('<')*(self.block_size - len(smiles_matches)) # pad SMILES string by appending '<' to the end until self.block_size is achieved\n",
        "        \n",
        "        if len(smiles_matches) > self.block_size+1: # if the number of matches found by the regular expression pattern applied to the SMILES string exceeds self.block_size:\n",
        "            smiles = smiles[:self.block_size+1]\n",
        "        \n",
        "        embedded_smile = [self.stoi[s] for s in smiles_matches]\n",
        "        x = torch.tensor(embedded_smile[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(embedded_smile[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTFNFFNGpUaR"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7QbYJpQ3pUaR"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "        self.config = config\n",
        "\n",
        "        self.query = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n",
        "        self.key = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n",
        "        self.value = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(config.att_drop_rate)\n",
        "        self.resid_drop = nn.Dropout(config.att_drop_rate)\n",
        "\n",
        "        self.proj = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size()\n",
        "        # apply attention functions to get tensors with dimensions (B, n_head, T, head_size)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head)\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head)\n",
        "        if self.config.do_flash:\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "            v = v.transpose(1, 2)\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.config.att_drop_rate if self.training else 0, is_causal=True)\n",
        "            y = y.transpose(1, 2)\n",
        "        else:\n",
        "            # (B h T s) @ (B h s T) -> (B h T T)\n",
        "            att = torch.einsum('bths,bihs->bhti', q, k) / np.sqrt(k.size(-1))\n",
        "            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            # (B h T T) @ (B h T s) -> (B h T s)\n",
        "            y = torch.einsum('bhtq,bqhs->bths', att, v)\n",
        "            self.att_weights = att\n",
        "        self.attended = y\n",
        "        y = y.contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        self.out = y\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.mlp = nn.Sequential(nn.Linear(config.n_embed, config.ff_mult*config.n_embed), nn.GELU() if config.doGELU else nn.ReLU(),\n",
        "            nn.Linear(config.ff_mult*config.n_embed, config.n_embed), nn.Dropout(config.att_drop_rate))\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.attn(self.ln1(x))\n",
        "        x = x + y # perform a residual connection by summing input and attention output\n",
        "        x = x + self.mlp(self.ln2(x)) # apply layer normalization and then MLP, create a residual connection with input\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "        self.type_emb = nn.Embedding(2, config.n_embed)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embed))\n",
        "\n",
        "        self.drop = nn.Dropout(config.gpt_drop_rate)\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(config.n_embed)\n",
        "        self.head = nn.Linear(config.n_embed, config.vocab_size, bias=config.gpt_bias)\n",
        "        self.block_size = config.block_size # define the context size\n",
        "        self.apply(self._init_weights) # initialize weights and apply to all relevant modules in the model\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay, no_decay = set(), set()\n",
        "        no_decay = set()\n",
        "\n",
        "        whitelist_weight_modules = (torch.nn.Linear)\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        # for named module of the model:\n",
        "        for mn, m in self.named_modules():\n",
        "            # for named parameter of each module:\n",
        "            for pn, p in m.named_parameters():\n",
        "                # construct full parameter name by concatenating module name and parameter name, separated by a dot\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias') or ('bias' in pn):\n",
        "                    no_decay.add(fpn)\n",
        "                elif (pn.endswith('weight') or ('weight' in pn)) and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "        no_decay.add('pos_emb')\n",
        "        param_dict = {pn:p for pn, p in self.named_parameters()}\n",
        "        assert len(decay & no_decay) == 0\n",
        "        # assert that all parameters from both sets have been correctly separated\n",
        "        assert len(param_dict.keys() - (decay | no_decay)) == 0\n",
        "        optim_groups = [{\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "                        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}]\n",
        "        optimizer = SophiaG(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, rho=train_config.rho, weight_decay=train_config.weight_decay)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None, prop = None, scaffold = None):\n",
        "        b, t = idx.size()\n",
        "\n",
        "        assert t <= self.block_size\n",
        "\n",
        "        token_embeddings = self.tok_emb(idx) # pass input tensor through token embedding layer\n",
        "        # select a subset of the position embedding matrix based on the length of the input sequence\n",
        "        position_embeddings = self.pos_emb[:, :t, :]\n",
        "        # pass a tensor of ones of shape (b, t) through the type embedding layer,\n",
        "        # maps a binary type indicator to a learnable embedding vector, all type indicators\n",
        "        # are set to 1, indicating same type for all tokens in input sequence\n",
        "        type_embeddings = self.type_emb(torch.ones((b,t), dtype=torch.long, device=idx.device))\n",
        "        x = self.drop(token_embeddings + position_embeddings + type_embeddings)\n",
        "        \n",
        "        for layer in self.blocks:\n",
        "            x = layer(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWbFS84KpUaS"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mrnqKEZwpUaS"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_dataset, test_dataset):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.config = model.config\n",
        "        self.stoi = train_dataset.stoi\n",
        "        self.itos = train_dataset.itos\n",
        "\n",
        "    def train(self, wandb):\n",
        "        model, config = self.model, self.config\n",
        "        optimizer = model.configure_optimizers(config)\n",
        "        scaler = GradScaler() # define variable used for gradient scaling in mixed-precision training\n",
        "        self.tokens = 0 # initialize a counter used for learning rate decay\n",
        "\n",
        "        def run_epoch(split):\n",
        "            is_train = split == 'train'\n",
        "            model.train(is_train)\n",
        "            data = self.train_dataset if is_train else self.test_dataset\n",
        "            loader = DataLoader(data, shuffle=True, pin_memory=True, batch_size=config.batch_size, num_workers=config.num_workers)\n",
        "            losses = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            # for batch index, batch in progress bar:\n",
        "            for it, (x, y) in pbar:\n",
        "                # move the input data tensor, target data tensor, property tensor, and scaffold tensor to GPU\n",
        "                x, y = x.to(config.device), y.to(config.device)\n",
        "                # allow model to use lower-precision computations for improved memory usage\n",
        "                if config.device == 'cuda':\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        with torch.set_grad_enabled(is_train):\n",
        "                            logits, loss = model(x, y)\n",
        "                            loss = loss.mean()\n",
        "                            losses.append(loss.item())\n",
        "                else:\n",
        "                    with torch.cpu.amp.autocast():\n",
        "                        with torch.set_grad_enabled(is_train):\n",
        "                            logits, loss = model(x, y)\n",
        "                            loss = loss.mean()\n",
        "                            losses.append(loss.item())\n",
        "\n",
        "                if is_train:\n",
        "                    model.zero_grad()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.unscale_(optimizer) # unscale the gradients of the optimizer's parameters to their original values\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients of model parameters to prevent them from exploding, setting maximum gradient norm to be 1.0\n",
        "                    scaler.step(optimizer) # update the optimizer's parameters based on calculated gradients\n",
        "                    scaler.update() # update the scale factor of the gradient scaler\n",
        "                    if config.lr_decay:\n",
        "                        self.tokens += (y >= 0).sum() # increment the number of processed tokens by the count of valid tokens (not padding or special tokens)\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens)) # perform a linear warm-up\n",
        "                        else:\n",
        "                            # calculate the progress of training in terms of the number of tokens processed\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            # calculate the scaling factor for the learning rate (between 0.1 and 1.0)\n",
        "                            # to gradually reduce learning rate as training progresses\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + np.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult # multiply the base learning rate by the scaling factor to obtain the updated learning rate\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "                    if wandb is not None: # log training progress using Weights & Biases\n",
        "                        wandb.log({'step_train_loss': loss, 'train_step': it + epoch*len(loader), 'learning_rate': lr})\n",
        "                    # update the description of the progress bar with epoch, iteration, and training loss\n",
        "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "            return float(np.mean(losses))\n",
        "\n",
        "        # initialize best loss as infinity\n",
        "        best_loss = float('inf')\n",
        "        for epoch in range(config.epochs):\n",
        "            print(f'{epoch=}')\n",
        "            train_loss = run_epoch('train')\n",
        "            if self.test_dataset is not None:\n",
        "                test_loss = run_epoch('test')\n",
        "            if wandb is not None:\n",
        "                wandb.log({'epoch_valid_loss': test_loss, 'epoch_train_loss': train_loss, 'epoch': epoch + 1})\n",
        "            good_model = self.test_dataset is None or test_loss < best_loss\n",
        "            if good_model:\n",
        "                best_loss = test_loss\n",
        "                torch.save(self.model.state_dict(), self.config.ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OuGEvPXDpUaT"
      },
      "outputs": [],
      "source": [
        "def load_data(config_dict):\n",
        "    if (cut:=config_dict[\"slice_data\"]):\n",
        "        train_data = pd.read_csv(config_dict[\"train_path\"])[:cut]\n",
        "        val_data = pd.read_csv(config_dict[\"val_path\"])[:cut]\n",
        "    else:\n",
        "        train_data = pd.read_csv(config_dict[\"train_path\"])\n",
        "        val_data = pd.read_csv(config_dict[\"val_path\"])\n",
        "\n",
        "    smiles = train_data['smiles']\n",
        "    vsmiles = val_data['smiles']\n",
        "\n",
        "    # compile pattern into a regular expression object that can be used for matching operations\n",
        "    regex = re.compile(REGEX_PATTERN)\n",
        "    char_set = {'<','!'} # context={'<'}\n",
        "\n",
        "    max_len = 0\n",
        "    for iterator in (smiles.values, vsmiles.values):\n",
        "        for i in iterator:\n",
        "            chars = regex.findall(i.strip())\n",
        "            max_len = max(max_len, len(chars))\n",
        "            for char in chars:\n",
        "                char_set.add(char)\n",
        "\n",
        "    chars = sorted(list(char_set))\n",
        "    max_len += 1    #accounting for the start token, which hasn't been added yet\n",
        "\n",
        "    smiles = ['!' + i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in smiles]\n",
        "    vsmiles = ['!' + i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in vsmiles]\n",
        "\n",
        "    train_dataset = SMILESDataset(data=smiles, chars=chars, block_size=max_len, len_data=len(train_data))\n",
        "    valid_dataset = SMILESDataset(data=vsmiles, chars=chars, block_size=max_len, len_data=len(val_data))\n",
        "    train_dataset.export_desc_attributes(config_dict[\"desc_path\"])\n",
        "    return train_dataset, valid_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zH-gwID9pUaT"
      },
      "outputs": [],
      "source": [
        "def pretrain_GPT(train_dataset, valid_dataset, config_dict):\n",
        "  \"\"\"\n",
        "  OUTPUTS:\n",
        "  1) checkpoint of trained model parameters\n",
        "  2) Weights & Biases logged run\n",
        "  \"\"\"\n",
        "\n",
        "  mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, warmup_tokens=0.1*train_dataset.len_data*train_dataset.block_size, final_tokens=config_dict[\"epochs\"]*train_dataset.len_data*train_dataset.block_size, **config_dict)\n",
        "  model = GPT(mconf)\n",
        "  model.to(config_dict[\"device\"])\n",
        "  torch.compile(model)\n",
        "  trainer = Trainer(model, train_dataset, valid_dataset)\n",
        "\n",
        "  %env WANDB_EXECUTABLE=python3\n",
        "  wandb.init(project=config_dict[\"wandb_project\"], name=config_dict[\"wandb_runname\"])\n",
        "  trainer.train(wandb=wandb)\n",
        "  return model, tconf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mWOb6fsKpUaU"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = load_data(CONFIG_DICT) # takes ~1 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "uvCDiyssBbZl",
        "outputId": "c92e9da7-7aca-442d-80db-e8668899cbd0"
      },
      "outputs": [],
      "source": [
        "# define function to train GPT model\n",
        "model, tconf = pretrain_GPT(\n",
        "                train_dataset = train_dataset,\n",
        "                valid_dataset = val_dataset,\n",
        "                config_dict = CONFIG_DICT\n",
        "        )\n",
        "\n",
        "#GK wandb API Key: c99c9a01523f93287716691fa3360b1f4566e115\n",
        "#RB wandb API Key: 4d3d628c6b5a4b3554c7a89ea50df8a4a6be0f85\n",
        "#AM wandb API key: 5be14d5930441de4707f6a58e4f7c2e229dab1d1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR2QZRicBmed"
      },
      "source": [
        "##Generation & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RccqlCxBqFH"
      },
      "outputs": [],
      "source": [
        "def generate_SMILES(model_config_dict, inference_config_dict):\n",
        "  props = inference_config_dict[\"props\"]\n",
        "  scaffold = inference_config_dict[\"scaffold\"]\n",
        "\n",
        "  # define a regular expression that matches molecular tokens in SMILES strings\n",
        "  pattern = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|!|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "  regex = re.compile(pattern)\n",
        "\n",
        "  dataset = SMILESDataset()\n",
        "  dataset.load_desc_attributes(inference_config_dict['desc_path'])\n",
        "  use_scaf = False if scaffold is None else True\n",
        "\n",
        "  mconf = GPTConfig(dataset.vocab_size, dataset.max_len, num_props=len(props), scaffold=use_scaf, scaffold_maxlen=dataset.scaf_max_len, **model_config_dict)\n",
        "  model = GPT(mconf).to(model_config_dict['device'])\n",
        "  torch.compile(model)\n",
        "\n",
        "  # load parameters into the model\n",
        "  model.load_state_dict(torch.load(inference_config_dict[\"model_params\"], map_location=torch.device(model_config_dict['device'])))\n",
        "  block_size = model.get_block_size() #inference_config_dict[\"block_size\"]\n",
        "  assert block_size == dataset.max_len, \"Warning: model block size and dataset block size are different\"\n",
        "  # calculate number of generation iterations from total number of SMILES to generate and batch size\n",
        "  gen_iter = math.ceil(inference_config_dict[\"gen_size\"] / inference_config_dict[\"batch_size\"])\n",
        "  stoi = dataset.stoi # define dictionary to map strings to integers\n",
        "  itos = dataset.itos # define dictionary to map integers to strings\n",
        "  # is a scaffold is defined for conditioning:\n",
        "  if scaffold is not None:\n",
        "      # pad '<' to end of scaffold string to achieve maximum scaffold length\n",
        "      scaffold += str('<')*(dataset.scaf_max_len - len(regex.findall(scaffold)))\n",
        "      # convert the scaffold SMILES to a tensor of integers and repeat along the batch dimension, move to GPU\n",
        "      scaffold=torch.tensor([stoi[s] for s in regex.findall(scaffold)])[None,...].repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n",
        "\n",
        "  if props is None:\n",
        "    p = None\n",
        "  elif len(props) == 1:\n",
        "    # create a tensor for conditioning with a single property value\n",
        "    p = torch.tensor([[props[0]]]).repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n",
        "  else:\n",
        "    # create a tensor for conditioning with multiple property values\n",
        "    p = torch.tensor([props]).repeat(inference_config_dict[\"batch_size\"], 1).unsqueeze(1).to(model_config_dict['device'])\n",
        "\n",
        "  molecules = []\n",
        "  for i in tqdm(range(gen_iter)):\n",
        "          # create an input tensor by converting 'context' to a tensor of token indices,\n",
        "          # repeat this batch times along the batch dimension\n",
        "          x = torch.tensor([stoi[s] for s in regex.findall(inference_config_dict[\"context\"])], dtype=torch.long)[None,...].repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n",
        "          # call sample function to generate molecules conditioned on the input\n",
        "          y = sample(model, x, block_size, temperature=inference_config_dict[\"temp\"], prop=p, scaffold=scaffold)\n",
        "          # for each generated molecule:\n",
        "          for gen_mol in y:\n",
        "                  # convert generated molecule from list of integers to list of strings and concatenate to one string\n",
        "                  completion = ''.join([itos[int(i)] for i in gen_mol])\n",
        "                  # remove padding tokens\n",
        "                  completion = completion.replace('<', '')\n",
        "                  completion = completion.replace('!', '')\n",
        "                  # convert the string representation of the molecule to an rdkit Mol object\n",
        "                  mol = get_mol(completion)\n",
        "                  # if an rdkit Mol object was created:\n",
        "                  if mol:\n",
        "                          # append the Mol object to the list\n",
        "                          molecules.append(mol)\n",
        "  # create dataframe where first column contains rdkit Mols and second column contins SMILES\n",
        "  results = pd.DataFrame([{'molecule' : i, 'smiles': Chem.MolToSmiles(i)} for i in molecules])\n",
        "  # iterate over each SMILES and ensure that equivalent molecules have same SMILES\n",
        "  canon_smiles = [canonic_smiles(s) for s in results['smiles']]\n",
        "  # create set of unique SMILES strings\n",
        "  unique_smiles = list(set(canon_smiles))\n",
        "  data = pd.read_csv(inference_config_dict[\"train_data\"]) # load training data\n",
        "  novel_ratio = check_novelty(unique_smiles, set(data['smiles'])) # calculate novelty ratio from generated SMILES and training SMILES\n",
        "  results['qed'] = results['molecule'].apply(lambda x: QED.qed(x)) # quantitative estimate of drug-likeliness (QED)\n",
        "  results['sas'] = results['molecule'].apply(lambda x: sascorer.calculateScore(x)) #synthetic accessibility score (SAS)\n",
        "  results['logp'] = results['molecule'].apply(lambda x: Crippen.MolLogP(x)) #(measure of hydrophobicity)\n",
        "  results['tpsa'] = results['molecule'].apply(lambda x: CalcTPSA(x)) #topological polar surface area (TPSA)\n",
        "  results['validity'] = np.round(len(results)/(inference_config_dict[\"batch_size\"]*gen_iter), 3)\n",
        "  results['unique'] = np.round(len(unique_smiles)/len(results), 3)\n",
        "  results['novelty'] = np.round(novel_ratio/100, 3)\n",
        "  # save the dataframe as a csv file\n",
        "  results.to_csv(inference_config_dict[\"save_path\"], index = False)\n",
        "  # print all evaluation metrics using function from moses package\n",
        "  print(moses.get_all_metrics(list(results['smiles'].values), device=model_config_dict['device']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO6ReKqdBt85"
      },
      "outputs": [],
      "source": [
        "inference_config_dict = {\n",
        "    # \"model_params\": train_config_dict[\"ckpt_path\"],\n",
        "    \"model_params\": BASE + 'current_data/GPT_pretrain_07-06.pt',\n",
        "    \"train_data\": train_config_dict[\"train_path\"],\n",
        "    # \"desc_path\": model_config_dict[\"desc_path\"],\n",
        "    \"desc_path\": BASE + 'checkpoints/descriptors_10k.yaml',\n",
        "    \"save_path\": BASE + 'current_data/GPT_pretrain_inference_07-06.csv',\n",
        "    \"checkpoint_dir\": BASE + 'current_data/',\n",
        "    \"batch_size\": 1,\n",
        "    \"gen_size\": 10000,\n",
        "    \"temp\": 1,\n",
        "    \"context\": \"!\",\n",
        "    \"scaffold\": None,\n",
        "    \"props\": []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pCo5E2K-EtW"
      },
      "outputs": [],
      "source": [
        "# run function to generate SMILES strings\n",
        "generate_SMILES(\n",
        "              model_config_dict = model_config_dict,\n",
        "              inference_config_dict = inference_config_dict\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmPym8y6p18t"
      },
      "source": [
        "#Sampling for DiffDock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5iCVkclJXWf",
        "outputId": "249fc9e9-44c7-4b62-a008-684b2f1ac929"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew77adUnI5WJ",
        "outputId": "ace942ba-d296-4161-82b0-75face393b85"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "hPBcXwkBIZIL",
        "outputId": "f5748b39-65c2-4662-b6fb-5fec02e93a6b"
      },
      "outputs": [],
      "source": [
        "import rdkit.Chem\n",
        "import rdkit.Chem.Descriptors\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def descriptors_for_gpt_predictions(path_to_predicted, path_to_save):\n",
        "    gpt_mols = pd.read_csv(path_to_predicted)\n",
        "    keySet = None\n",
        "    keyToData = {}\n",
        "    pbar = tqdm(gpt_mols.iterrows(), total=len(gpt_mols))\n",
        "    for index, row in pbar:\n",
        "        smile = row['smiles']\n",
        "        mol = rdkit.Chem.MolFromSmiles(smile)\n",
        "        if not mol: continue\n",
        "        mol_data = rdkit.Chem.Descriptors.CalcMolDescriptors(mol)\n",
        "        if keySet is None:\n",
        "            keySet = set(mol_data.keys())\n",
        "        for key in keySet:\n",
        "            keyToData.setdefault(key, []).append(mol_data[key])\n",
        "        keyToData.setdefault('smiles', []).append(smile)\n",
        "    gpt_df = pd.DataFrame(keyToData)\n",
        "    gpt_df.to_pickle(path_to_save)\n",
        "    return gpt_df\n",
        "\n",
        "descriptors_for_gpt_predictions(path_to_predicted=inference_config_dict['save_path'], path_to_save=inference_config_dict['save_path'].split('.')[0]+'_descriptors.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2Te1IDeJnao"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "def project_into_pca_space(path_to_pca, path_to_mols):\n",
        "    scaler, pca = pickle.load(open(path_to_pca, 'rb'))\n",
        "    gptMols = pd.read_pickle(path_to_mols)\n",
        "    return gptMols['smiles'], pca.transform(scaler.transform(gptMols[scaler.get_feature_names_out()]))\n",
        "\n",
        "gpt_smiles, pca_transformed = project_into_pca_space(path_to_pca=inference_config_dict['checkpoint_dir'] + 'scaler_pca_tuple.pickle', path_to_mols=inference_config_dict['save_path'].split('.')[0]+'_descriptors.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XGAgRrOKPo3"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def _cluster_mols_experimental_loss(mols, n_clusters, n_iter):\n",
        "    min_loss, best_kmeans = float('inf'), None\n",
        "    for _ in range(n_iter):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
        "        if kmeans.inertia_ < min_loss:\n",
        "            min_loss = kmeans.inertia_\n",
        "            best_kmeans = kmeans\n",
        "    return best_kmeans\n",
        "\n",
        "def _cluster_mols_experimental_variance(mols, n_clusters, n_iter):\n",
        "    max_variance, best_kmeans = float('-inf'), None\n",
        "    for _ in range(n_iter):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
        "        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n",
        "        if (variance:=np.var(counts)) > max_variance:\n",
        "            max_variance = variance\n",
        "            best_kmeans = kmeans\n",
        "    return best_kmeans\n",
        "\n",
        "def _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile):\n",
        "    inertias = []\n",
        "    variances = []\n",
        "    km_objs = []\n",
        "    for _ in range(n_iter):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n",
        "        variances.append(np.var(counts))\n",
        "        km_objs.append(kmeans)\n",
        "    loss_var_kmeans_triples = sorted(zip(inertias, variances, km_objs), key=lambda x: x[0])\n",
        "    lowest_n = loss_var_kmeans_triples[:int(len(loss_var_kmeans_triples) * mixed_objective_loss_quantile)]\n",
        "    sorted_by_variance = sorted(lowest_n, key=lambda x: x[1])\n",
        "    return sorted_by_variance[0][2]\n",
        "\n",
        "def _cluster_mols_experimental(mols, n_clusters, save_path, n_iter=1, objective='loss', mixed_objective_loss_quantile=0.1):\n",
        "    if n_iter == 1:\n",
        "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
        "    elif objective == 'loss':\n",
        "        kmeans = _cluster_mols_experimental_loss(mols, n_clusters, n_iter)\n",
        "    elif objective == 'variance':\n",
        "        kmeans = _cluster_mols_experimental_variance(mols, n_clusters, n_iter)\n",
        "    elif objective == 'mixed':\n",
        "        kmeans = _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile)\n",
        "    else:\n",
        "        raise ValueError(f'Unknown objective {objective}')\n",
        "\n",
        "    pickle.dump(kmeans, open(save_path, 'wb'))\n",
        "    return kmeans\n",
        "\n",
        "# out = _cluster_mols_experimental(mols=pca_transformed, n_clusters=100, n_iter=1_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l24k__P4KPhk"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from pprint import pprint as pp\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def cluster_and_sample(mols, mols_smiles, n_clusters, n_samples, kmeans_save_path, clusters_save_path, ensure_correctness=False, path_to_pca=None):\n",
        "    \"\"\"\n",
        "        Clusters a given list of molecules, samples from each cluster, and saves the resulting data to specified files.\n",
        "\n",
        "        This function performs K-Means clustering on the input list of molecules and then samples a specified number of molecules\n",
        "        from each cluster. The function ensures that the number of samples requested from each cluster doesn't exceed the total number\n",
        "        of available molecules. The clustered data and sampled data are saved to specified file paths using pickle.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mols : array-like or sparse matrix, shape (n_samples, n_features)\n",
        "            The input samples where n_samples is the number of samples and n_features is the number of features.\n",
        "\n",
        "        mols_smiles : list of str\n",
        "            A list of SMILES strings corresponding to the input molecules.\n",
        "\n",
        "        n_clusters : int\n",
        "            The number of clusters to form as well as the number of centroids to generate.\n",
        "\n",
        "        n_samples : int\n",
        "            The number of samples to draw from each cluster.\n",
        "\n",
        "        kmeans_save_path : str\n",
        "            The path (including file name) where the resulting KMeans object should be saved.\n",
        "\n",
        "        clusters_save_path : str\n",
        "            The path (including file name) where the resulting clusters should be saved.\n",
        "\n",
        "        ensure_correctness : bool, optional (default=False)\n",
        "            If True, performs additional correctness checks, such as comparing SMILES string derived features to features in mols array.\n",
        "            This requires 'path_to_pca' to be set.\n",
        "\n",
        "        path_to_pca : str, optional (default=None)\n",
        "            If ensure_correctness is True, this should be the path to a PCA model used to transform the molecules' descriptors.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cluster_to_samples : dict\n",
        "            A dictionary where the keys are cluster labels and the values are lists of sampled SMILES strings from each cluster.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        AssertionError\n",
        "            If the number of requested samples exceeds the total number of molecules provided.\n",
        "            If ensure_correctness is True but path_to_pca is None.\n",
        "            If the number of labels returned by the KMeans algorithm differs from the number of molecules.\n",
        "            If features calculated from a smile string differ from features in the mols array.\n",
        "            If the total number of sampled molecules doesn't equal to n_clusters * n_samples.\n",
        "\n",
        "    \"\"\"\n",
        "    assert n_clusters * n_samples <= len(mols), f\"{n_clusters=} * {n_samples=} = {n_clusters*n_samples} requested but only {len(mols)} molecules provided\"\n",
        "    if ensure_correctness:\n",
        "        assert path_to_pca is not None, \"path_to_pca must be provided to ensure correctness\"\n",
        "        scaler, pca = pickle.load(open(path_to_pca, 'rb'))\n",
        "\n",
        "    kmeans = _cluster_mols_experimental(mols=mols, n_iter=100, n_clusters=n_clusters, save_path=kmeans_save_path, objective='mixed', mixed_objective_loss_quantile=0.05)\n",
        "    assert len(kmeans.labels_) == len(mols_smiles), \"Number of labels differs from number of molecules\"\n",
        "\n",
        "    cluster_to_mols = {}\n",
        "    for mol, label, smile in zip(mols, kmeans.labels_, mols_smiles):\n",
        "        cluster_to_mols.setdefault(label, []).append(smile)\n",
        "        if ensure_correctness: # recalculate descriptors from a smile string and compare to the descriptors in the array\n",
        "            smile_features = pca.transform(scaler.transform(pd.DataFrame({k: [v] for k, v in rdkit.Chem.Descriptors.CalcMolDescriptors(rdkit.Chem.MolFromSmiles(smile)).items()})[scaler.get_feature_names_out()]))\n",
        "            assert np.allclose(smile_features[0], mol), \"Features calculated from a smile string differ from features in the array\"\n",
        "\n",
        "    # What happens below is sampling from each cluster. All the extra code is to ensure that the number of samples requested from each cluster\n",
        "    # doesn't exceed the total number of available molecules. This is done by calculating the average number of molecules per cluster and then\n",
        "    # calculating the number of extra molecules that need to be sampled from each cluster. The extra molecules are then distributed among the\n",
        "    # clusters uniformly. If the number of extra molecules is greater than the number of molecules in a cluster, all\n",
        "    # molecules from that cluster are sampled.\n",
        "    avg_len = np.mean([len(v) for v in cluster_to_mols.values()])\n",
        "    cluster_to_samples = {}\n",
        "    extra_mols = 0\n",
        "    left_to_sample = n_clusters*n_samples\n",
        "    cluster_to_len = {cluster:len(mols) for cluster, mols in cluster_to_mols.items()}\n",
        "    for i, (cluster, _) in enumerate(sorted(cluster_to_len.items(), key=lambda x: x[1], reverse=False)):\n",
        "        smiles = cluster_to_mols[cluster]\n",
        "    # for i, (cluster, smiles) in enumerate(cluster_to_mols.items()):\n",
        "        if extra_mols > 0:\n",
        "            cur_extra = int(1+extra_mols/(len(cluster_to_mols) - i) * len(smiles)/avg_len)\n",
        "            cur_samples = n_samples + cur_extra\n",
        "            extra_mols -= cur_extra\n",
        "        else:\n",
        "            cur_samples = n_samples\n",
        "        if cur_samples > left_to_sample:\n",
        "            cur_samples = left_to_sample\n",
        "\n",
        "        if len(smiles) > cur_samples:\n",
        "            cluster_to_samples[cluster] = random.sample(smiles, cur_samples)\n",
        "            left_to_sample -= cur_samples\n",
        "        else:\n",
        "            cluster_to_samples[cluster] = smiles\n",
        "            left_to_sample -= len(smiles)\n",
        "            extra_mols += cur_samples - len(smiles)\n",
        "\n",
        "    assert (n_sampled:=sum(len(vals) for vals in cluster_to_samples.values())) == n_clusters*n_samples, f\"Sampled {n_sampled} but were requested {n_clusters*n_samples}\"\n",
        "    pickle.dump(cluster_to_mols, open(clusters_save_path, 'wb'))\n",
        "    pickle.dump(cluster_to_samples, open(clusters_save_path.split('.')[0] + '_samples.pickle', 'wb'))\n",
        "    return cluster_to_samples\n",
        "\n",
        "\n",
        "cluster_and_sample(mols=pca_transformed, mols_smiles=gpt_smiles, n_clusters=100, n_samples=10,\n",
        "                   kmeans_save_path=inference_config_dict['checkpoint_dir'] + 'k100means_07_07.pickle', ensure_correctness=False,\n",
        "                   clusters_save_path=inference_config_dict['checkpoint_dir'] + 'cluster_to_samples_07_07.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzLWORD_OwCw"
      },
      "source": [
        "#Docking pose generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m68TViE-NqDN",
        "outputId": "d73bef59-c252-4113-9cf3-0f303605c6f1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "cluster_to_samples = pickle.load(open(inference_config_dict['checkpoint_dir'] + 'cluster_to_samples_07_07_samples.pickle', 'rb'))\n",
        "print(cluster_to_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "mQaRu5WvOn5H",
        "outputId": "0d6977a5-7759-4cc9-ba33-75c2e5e46597"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "keyToData = {}\n",
        "for cluster, mols in cluster_to_samples.items():\n",
        "    for mol in mols:\n",
        "        keyToData.setdefault('smiles', []).append(mol)\n",
        "        keyToData.setdefault('cluster_id', []).append(cluster)\n",
        "pd.DataFrame(keyToData).to_csv(inference_config_dict['checkpoint_dir']+'mols_sampled_for_difdock_07_07.csv')\n",
        "pd.read_csv(inference_config_dict['checkpoint_dir']+'mols_sampled_for_difdock_07_07.csv').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBgupbjAPYu7"
      },
      "source": [
        "##Set up notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMwALd1zQBQ3",
        "outputId": "902f65c7-ece7-411b-8c67-f07696e10e28"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# install necessary packages\n",
        "!pip install rdkit\n",
        "!pip install pandas==1.5.3\n",
        "!pip install molsets\n",
        "!pip install wandb\n",
        "\n",
        "!pip install pyg==0.7.1 --quiet\n",
        "!pip install pyyaml==6.0 --quiet\n",
        "!pip install scipy==1.7.3 --quiet\n",
        "!pip install networkx==2.6.3 --quiet\n",
        "!pip install biopython==1.79 --quiet\n",
        "!pip install rdkit-pypi==2022.03.5 --quiet\n",
        "!pip install e3nn==0.5.0 --quiet\n",
        "!pip install spyrmsd==0.5.2 --quiet\n",
        "!pip install pandas==1.5.3 --quiet\n",
        "!pip install biopandas==0.4.1 --quiet\n",
        "\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster\\\n",
        " -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "\n",
        " # import necessary packages\n",
        "from rdkit import Chem\n",
        "import shutil\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn.conv import MessagePassing, GatedGraphConv\n",
        "from torch_geometric.nn import global_add_pool\n",
        "from torch_geometric.utils import add_self_loops, dense_to_sparse\n",
        "from torch_geometric.nn.aggr import AttentionalAggregation\n",
        "from torch._C import NoneType\n",
        "from torch.optim import Adam\n",
        "from torch_geometric.data import Data, DataListLoader\n",
        "from torch_geometric.nn import DataParallel as GeometricDataParallel\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, pairwise_distances\n",
        "import os\n",
        "import random\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import pkg_resources\n",
        "pkg_resources.require(\"pandas==1.5.3\")\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import QED, Crippen\n",
        "from rdkit.Contrib.SA_Score import sascorer\n",
        "from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "from rdkit.Chem.Fingerprints import FingerprintMols\n",
        "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity\n",
        "from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmiles\n",
        "import moses\n",
        "from moses.utils import get_mol\n",
        "\n",
        "if not os.path.exists(\"/content/DiffDock\"):\n",
        "    %cd /content\n",
        "    !git clone https://github.com/gcorso/DiffDock.git\n",
        "    %cd /content/DiffDock\n",
        "    !git checkout a6c5275 # remove/update for more up to date code\n",
        "\n",
        "# clone ESM repository\n",
        "if not os.path.exists(\"/content/DiffDock/esm\"):\n",
        "    %cd /content/DiffDock\n",
        "    !git clone https://github.com/facebookresearch/esm\n",
        "    %cd /content/DiffDock/esm\n",
        "    !git checkout ca8a710\n",
        "    !sudo pip install -e .\n",
        "    %cd /content/DiffDock\n",
        "\n",
        "# set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42dkxj7rRd1z"
      },
      "source": [
        "##Run DiffDock to get top poses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeGCZ7P_leGr"
      },
      "outputs": [],
      "source": [
        "def get_top_poses(ligands_csv_path, protein_pdb_path):\n",
        "    data = pd.read_csv(ligands_csv_path)\n",
        "    ligand_files = []\n",
        "\n",
        "    os.environ['HOME'] = 'esm/model_weights'\n",
        "    os.environ['PYTHONPATH'] = f'{os.environ.get(\"PYTHONPATH\", \"\")}:/content/DiffDock/esm'\n",
        "\n",
        "    for i in range(len(data)):  # change 1 to len(data) for processing all ligands\n",
        "        print(str((i / len(data)) * 100)[:5], ' %')\n",
        "        smiles = data['smiles'][i]\n",
        "        rdkit_mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "        if rdkit_mol is not None:\n",
        "            with open('/content/input_protein_ligand.csv', 'w') as out:\n",
        "                out.write('protein_path,ligand\\n')\n",
        "                out.write(f'{protein_pdb_path},{smiles}\\n')\n",
        "\n",
        "            # Clear out old results if running multiple times\n",
        "            shutil.rmtree('/content/DiffDock/results', ignore_errors=True)\n",
        "\n",
        "            # ESM Embedding Preparation\n",
        "            os.chdir('/content/DiffDock')\n",
        "            !python /content/DiffDock/datasets/esm_embedding_preparation.py --protein_ligand_csv /content/input_protein_ligand.csv --out_file /content/DiffDock/data/prepared_for_esm.fasta\n",
        "\n",
        "            # ESM Extraction\n",
        "            !python /content/DiffDock/esm/scripts/extract.py esm2_t33_650M_UR50D /content/DiffDock/data/prepared_for_esm.fasta /content/DiffDock/data/esm2_output --repr_layers 33 --include per_tok --truncation_seq_length 30000\n",
        "\n",
        "            # Inference\n",
        "            !python /content/DiffDock/inference.py --protein_ligand_csv /content/input_protein_ligand.csv --out_dir /content/DiffDock/results/user_predictions_small --inference_steps 20 --samples_per_complex 10 --batch_size 6\n",
        "\n",
        "            # Move results\n",
        "            for root, dirs, files in os.walk('/content/DiffDock/results/user_predictions_small'):\n",
        "                for file in files:\n",
        "                    if file.startswith('rank1_confidence'):\n",
        "                        shutil.move(os.path.join(root, file), os.path.join('/content', 'drive', 'MyDrive', 'Generative_ML', 'diffdock_best_poses', f'complex{i}'))\n",
        "                        ligand_files.append(f'/content/drive/MyDrive/Generative_ML/diffdock_best_poses/complex{i}')\n",
        "\n",
        "    return ligand_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cfcaA3OR0Wy",
        "outputId": "faaaa1d7-6ac2-4111-c97b-66a43bedf511"
      },
      "outputs": [],
      "source": [
        "# get top DiffDock poses\n",
        "top_diffdock_poses = get_top_poses('/content/drive/MyDrive/Generative_ML/data/molgpt_generated_nocond_06_10_fintetune2.csv',\n",
        "                                   '/content/drive/MyDrive/Generative_ML/data/6o56.pdb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME15L20kCol2"
      },
      "outputs": [],
      "source": [
        "# write list of file paths to txt file\n",
        "with open('/content/drive/MyDrive/Generative_ML/diffdock_poses.txt', 'w') as file:\n",
        "    for path in top_diffdock_poses:\n",
        "        file.write(path + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veWeIgUNtYxK"
      },
      "source": [
        "#Active learning based on pseudo protein-ligand binding energy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tamyd6Hstkpl"
      },
      "source": [
        "##Set up notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjg__oTktYHh",
        "outputId": "b1407854-17f9-4b2f-f06e-96acf4eca70f"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "import condacolab\n",
        "\n",
        "!mamba install pymol-open-source --yes\n",
        "\n",
        "!pip install prolif\n",
        "!pip install rdkit\n",
        "\n",
        "from pymol import cmd\n",
        "import prolif\n",
        "from prolif.plotting.network import LigNetwork\n",
        "from rdkit import Chem\n",
        "from IPython.display import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F04vDvgVps1o"
      },
      "source": [
        "##Count connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhu7U2OSps1z"
      },
      "outputs": [],
      "source": [
        "interaction_scores = {\n",
        "    'Hydrophobic': -2.5,\n",
        "    'HBDonor': -3.5,\n",
        "    'HBAcceptor': -3.5,\n",
        "    'Anionic': -7.5,\n",
        "    'Cationic': -7.5,\n",
        "    'CationPi': -2.5,\n",
        "    'PiCation': -2.5,\n",
        "    'VdWContact': -1.0,\n",
        "    'XBAcceptor': -3.0,\n",
        "    'XBDonor': -3.0,\n",
        "    'FaceToFace': -3.0,\n",
        "    'EdgeToFace': -1.0,\n",
        "    'MetalDonor': -3.0,\n",
        "    'MetalAcceptor': -3.0,\n",
        "}\n",
        "\n",
        "def get_contacts(protein, ligand):\n",
        "  cmd.delete('all')\n",
        "  cmd.load(protein, 'protein')\n",
        "  cmd.h_add('protein')\n",
        "  cmd.remove('sol')\n",
        "  cmd.save('/content/protein.pdb')\n",
        "\n",
        "  prot = prolif.Molecule(Chem.MolFromPDBFile('/content/protein.pdb', removeHs=False))\n",
        "  lig = Chem.SDMolSupplier(ligand, removeHs=False)\n",
        "  lig = prolif.Molecule.from_rdkit(lig[0])\n",
        "\n",
        "  fp = prolif.Fingerprint(interactions=list(interaction_scores.keys()))\n",
        "  fp.run_from_iterable([lig], prot, progress=False)\n",
        "  try:\n",
        "    df = fp.to_dataframe(return_atoms=True)\n",
        "    df_stacked = df.stack(level=[0, 1, 2])\n",
        "    df_reset = df_stacked.to_frame().reset_index()\n",
        "    df_reset.columns = ['Frame', 'ligand', 'protein', 'interaction', 'value']\n",
        "    df_reset['score'] = df_reset['interaction'].apply(lambda x: interaction_scores[x])\n",
        "    return df_reset['score'].sum()\n",
        "  except:\n",
        "    print('Complex has no meaningful protein-ligand connections')\n",
        "    return int(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkmoW9MvqfrX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "ligand_list = []\n",
        "for i in os.listdir('/content/drive/MyDrive/Generative_ML/best_poses/'):\n",
        "    comp_num = str(i)\n",
        "    for j in os.listdir('/content/drive/MyDrive/Generative_ML/best_poses/' + comp_num):\n",
        "        if j.startswith('index0'):\n",
        "            ligand_list.append('/content/drive/MyDrive/Generative_ML/best_poses/' + comp_num + '/' + j + '/' + 'rank1.sdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "gzmdwfnjsqDZ",
        "outputId": "652eb1f9-6cb0-45b0-afe5-f621e6af1344"
      },
      "outputs": [],
      "source": [
        "from pymol import cmd\n",
        "import prolif\n",
        "from rdkit import Chem\n",
        "count = 0\n",
        "good_ligands = {}\n",
        "for i in ligand_list:\n",
        "    if i.split('/')[6].split('.')[0] not in good_ligands:\n",
        "        score = get_contacts('/content/drive/MyDrive/Generative_ML/data/HNH.pdb', i)\n",
        "        good_ligands[i.split('/')[6].split('.')[0]] = score\n",
        "        count += 1\n",
        "        if int(count%(len(ligand_list)/100)) == 0:\n",
        "            print(int(count/len(ligand_list)*100), ' %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkXG7o9H8r_Q"
      },
      "outputs": [],
      "source": [
        "sorted_ligands = {k: v for k, v in sorted(good_ligands.items(), key=lambda item: item[1])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "JIG6BZpeAYH1",
        "outputId": "84545fdc-f326-44bf-9a79-8fd50d510207"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "\n",
        "data = list(sorted_ligands.values())\n",
        "\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "\n",
        "plt.hist(data, bins=50, density=True, color='gray', alpha=0.7, edgecolor='black')\n",
        "\n",
        "smoothed_data = np.linspace(min(data), max(data), 1000)\n",
        "kde = gaussian_kde(data)\n",
        "smoothed_line = kde(smoothed_data)\n",
        "\n",
        "plt.plot(smoothed_data, smoothed_line, linewidth=2.5, color='black')\n",
        "\n",
        "plt.xlabel(\"Values\", fontsize=18)\n",
        "plt.ylabel(\"Density\", fontsize=18)\n",
        "\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "\n",
        "plt.title(\"Density Plot of Scores\", fontsize=20)\n",
        "\n",
        "plt.grid(linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "plt.tick_params(axis='both', which='both', direction='in', length=4)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f'There are {list(sorted_ligands.values()).count(0)} ligands with 0 connections ({np.round(list(sorted_ligands.values()).count(0) / len(sorted_ligands)*100, 1)}%)')\n",
        "print(f'There are {len([value for value in list(sorted_ligands.values()) if value < -10.0])} ligands with scores less than -10.0 ({np.round(len([value for value in list(sorted_ligands.values()) if value < -10.0]) / len(sorted_ligands)*100, 1)}%)')\n",
        "print(f'The mean score for all ligands is {np.round(np.mean(list(sorted_ligands.values())), 1)}')\n",
        "print(f'The lowest score for all ligands is {np.min(list(sorted_ligands.values()))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgnUcM4DwKpO"
      },
      "outputs": [],
      "source": [
        "good_ligands = {key: value for key, value in sorted_ligands.items() if value != 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgLp7SfIEa1t"
      },
      "outputs": [],
      "source": [
        "good_idx = [k[7:] for k in good_ligands if k.startswith('complex')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbHputaPw1Jh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "diffdock_input = pd.read_csv('/content/drive/MyDrive/Generative_ML/current_data/mols_sampled_for_difdock_07_07.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IUOuhmAxsVS"
      },
      "outputs": [],
      "source": [
        "good_data = diffdock_input.iloc[good_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSiSFTlIE6HM",
        "outputId": "cb3a2018-0103-4932-fc29-f4bf1fcfd5ff"
      },
      "outputs": [],
      "source": [
        "good_data['score'] = [good_ligands.get(f'complex{complex_number}', 0) for complex_number in good_data.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "RjRBxwuKx8jb",
        "outputId": "cdbf96fd-109e-412f-ae58-49bc89d5b1d6"
      },
      "outputs": [],
      "source": [
        "good_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUjA5PGgZbQd"
      },
      "source": [
        "## Prepare training dataset for next round of active learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QweC9J9fTYd7",
        "outputId": "13f1c990-c8b4-46b1-a695-b1becf67be3d"
      },
      "outputs": [],
      "source": [
        "cluster_to_scores = {}\n",
        "for index, row in good_data.iterrows():\n",
        "    cluster_to_scores.setdefault(row['cluster_id'], []).append(row['score'])\n",
        "cluster_to_score = {cluster_id: np.mean(scores) for cluster_id, scores in cluster_to_scores.items()}\n",
        "import random\n",
        "cluster_to_score = {k: random.uniform(-20, 0) for k in range(50)}\n",
        "print(cluster_to_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ9BaRa8GyK7"
      },
      "source": [
        "Here, we need to map the good data back to the original clusters and create a training set for active learning. THen, perform active learning, generate molecules, and repeat process. Finally, generate 2 million molecules to train BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84XMCXxAQyfa"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def _preprocess_scores_uniformly(scores, remove_positives=False, lowest_score=1):\n",
        "    \"\"\"\n",
        "        Preprocesses a dictionary of scores by negating and normalizing them.\n",
        "\n",
        "        The function negates all scores and optionally removes positive scores. If the minimum value among the negated scores\n",
        "        is less than zero, it shifts all values by subtracting the minimum value and adding 'lowest_score'. The final step is\n",
        "        to normalize the scores so that their total sum equals to 1.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        scores : dict\n",
        "            A dictionary of scores where the keys are identifiers and the values are their corresponding scores.\n",
        "\n",
        "        remove_positives : bool, optional (default=False)\n",
        "            If True, all positive scores are removed after negation.\n",
        "\n",
        "        lowest_score : int, optional (default=1)\n",
        "            This value is added to all scores if the minimum score is less than zero.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        normalized : dict\n",
        "            The normalized dictionary of scores.\n",
        "\n",
        "    \"\"\"\n",
        "    negated = {k: -v for k, v in scores.items()}\n",
        "    min_value = min(negated.values())\n",
        "    if min_value < 0:\n",
        "        if remove_positives:\n",
        "            negated = {k: v for k, v in negated.items() if v > 0}\n",
        "        else:\n",
        "            negated = {k: v - min_value + lowest_score for k, v in negated.items()}\n",
        "    total = sum(negated.values())\n",
        "    normalized = {k: v / total for k, v in negated.items()}\n",
        "    return normalized\n",
        "\n",
        "def _preprocess_scores_softmax(scores):\n",
        "    negated = {k: -v for k, v in scores.items()}\n",
        "    max_value = max(negated.values())\n",
        "    exponentiate = {k: np.exp(v - max_value) for k, v in negated.items()}\n",
        "    total = sum(exponentiate.values())\n",
        "    softmax = {k: v / total for k, v in exponentiate.items()}\n",
        "    return softmax\n",
        "\n",
        "def balance_cluster_to_n(cluster_to_n, cluster_to_len):\n",
        "    \"\"\"\n",
        "        Balances the target number of samples for each cluster to ensure it doesn't exceed the actual size of the cluster.\n",
        "\n",
        "        The function first calculates the surplus (i.e., the excess of the target number over the actual size) for each cluster.\n",
        "        Then, it distributes the total surplus proportionally among the clusters that have a deficit (i.e., the target number is less than the actual size).\n",
        "        If after this distribution, there's still a deficit (i.e., the sum of target numbers is less than the sum of actual sizes), the function\n",
        "        increases the target number of the largest clusters one by one until the sum of target numbers equals to the sum of actual sizes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cluster_to_n : dict\n",
        "            A dictionary mapping cluster identifiers to their target number of samples.\n",
        "\n",
        "        cluster_to_len : dict\n",
        "            A dictionary mapping cluster identifiers to the actual size of each cluster.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        balanced : dict\n",
        "            A dictionary mapping cluster identifiers to their balanced target number of samples.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        AssertionError\n",
        "            If the sum of target numbers before and after balancing don't match.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    surplus = {key: cluster_to_n[key] - cluster_to_len[key] for key in cluster_to_n if cluster_to_n[key] > cluster_to_len[key]}\n",
        "    balanced = {k:v for k, v in cluster_to_n.items()}\n",
        "    n_to_cluster = {v: k for k, v in cluster_to_n.items()}\n",
        "\n",
        "    for key in surplus:\n",
        "        balanced[key] = cluster_to_len[key]\n",
        "\n",
        "    total_surplus = sum(surplus.values())\n",
        "    initial_n_sum = sum(n for key, n in cluster_to_n.items() if key not in surplus)\n",
        "\n",
        "    for key in balanced:\n",
        "        if key in surplus: continue\n",
        "        surplus_to_add = total_surplus * cluster_to_n[key] / initial_n_sum\n",
        "        new_n = int(cluster_to_n[key] + surplus_to_add)\n",
        "        balanced[key] = min(new_n, cluster_to_len[key])\n",
        "\n",
        "    deficit = sum(cluster_to_n.values()) - sum(balanced.values())\n",
        "    while deficit > 0:\n",
        "        for initial_n in sorted(n_to_cluster, reverse=True):\n",
        "            if deficit == 0:\n",
        "                break\n",
        "            if (cluster:=n_to_cluster[initial_n]) in surplus: continue\n",
        "            if balanced[cluster] < cluster_to_len[cluster]:\n",
        "                balanced[cluster] += 1\n",
        "                deficit -= 1\n",
        "\n",
        "    assert sum(cluster_to_n.values()) == sum(balanced.values()), f\"Before balancing had {sum(cluster_to_n.values())}, post balancing = {sum(balanced.values())}\"\n",
        "    return balanced\n",
        "\n",
        "def sample_clusters_for_active_learning(cluster_to_scores, n_samples, path_to_clusters, probability_type='softmax', remove_positives=False, lowest_score=1):\n",
        "    \"\"\"\n",
        "        Sample molecules from clusters for active learning purposes, considering previously docked molecules and balancing the sampling among clusters.\n",
        "\n",
        "        This function uses either softmax or uniform probabilities to determine how many molecules to sample from each cluster. The function then samples\n",
        "        the required number of new molecules (i.e., those not present in docked_mols) from each cluster. The sampling is balanced to ensure the target number\n",
        "        doesn't exceed the actual size of the cluster.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cluster_to_scores : dict\n",
        "            A dictionary mapping cluster identifiers to their scores.\n",
        "\n",
        "        n_samples : int\n",
        "            The total number of molecules to sample.\n",
        "\n",
        "        path_to_clusters : str\n",
        "            The path to a pickle file storing a dictionary that maps each cluster to a list of molecules.\n",
        "\n",
        "        probability_type : str, optional (default='softmax')\n",
        "            The type of probability distribution used to determine the number of samples per cluster.\n",
        "            Options are 'softmax' and 'uniform'.\n",
        "\n",
        "        remove_positives : bool, optional (default=False)\n",
        "            Only used when probability_type is 'uniform'. If True, positive scores are removed after negation.\n",
        "\n",
        "        lowest_score : int, optional (default=1)\n",
        "            Only used when probability_type is 'uniform'. This value is added to all scores if the minimum score is less than zero.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        training : list\n",
        "            A list of randomly sampled molecules for active learning.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        KeyError\n",
        "            If an unsupported probability_type is provided.\n",
        "        AssertionError\n",
        "            If the number of sampled molecules doesn't equal to n_samples.\n",
        "\n",
        "    \"\"\"\n",
        "    if probability_type == 'softmax':\n",
        "        probability_function = _preprocess_scores_softmax\n",
        "    elif probability_type == 'uniform':\n",
        "        probability_function = lambda x: _preprocess_scores_uniformly(x, remove_positives, lowest_score)\n",
        "    else:\n",
        "        raise KeyError(\"Only uniform and softmax probabilities are supported\")\n",
        "    cluster_to_mols = pickle.load(open(path_to_clusters, 'rb'))\n",
        "    cluster_to_samples = pickle.load(open(path_to_clusters.split('.')[0] + '_samples.pickle', 'rb'))\n",
        "    docked_mols = {smile for smiles in cluster_to_samples.values() for smile in smiles}\n",
        "    cluster_to_new_mols = {k: [smile for smile in v if smile not in docked_mols] for k, v in cluster_to_mols.items()}\n",
        "\n",
        "    probabilities = probability_function(cluster_to_scores)\n",
        "    cluster_to_n = {k: int(v * n_samples) for k, v in probabilities.items()}\n",
        "    max_cluster_id, max_prob = None, 0\n",
        "    for cluster, prob in probabilities.items():\n",
        "        if prob > max_prob:\n",
        "            max_cluster_id, max_prob = cluster, prob\n",
        "    cluster_to_n[max_cluster_id] += n_samples - sum(cluster_to_n.values())\n",
        "\n",
        "    cluster_to_len = {k: len(v) for k, v in cluster_to_new_mols.items()}\n",
        "    balanced = balance_cluster_to_n(cluster_to_n, cluster_to_len)\n",
        "\n",
        "    training = []\n",
        "    for i, (cluster, n) in enumerate(balanced.items()):\n",
        "        training.extend(random.sample(cluster_to_new_mols[cluster], n))\n",
        "    assert len(training) == n_samples, f\"{len(training)=} != {n_samples=}\"\n",
        "    return training\n",
        "\n",
        "training_sampled = sample_clusters_for_active_learning(cluster_to_score, n_samples=1000, probability_type='uniform', path_to_clusters= '/content/drive/MyDrive/Generative_ML/current_data/cluster_to_samples_07_07.pickle')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFY3C94tXbki",
        "outputId": "f2c60b17-8c77-4933-f92f-4200ad9462ec"
      },
      "outputs": [],
      "source": [
        "def combine_sampled_and_good_ligands(sampled, good_ligands, good_ligand_multiplier:int, save_path):\n",
        "    # assert isinstance(good_ligand_multiplier, int), \"A multiplier should be an integer\"\n",
        "    keyToData = {}\n",
        "    for mol in sampled:\n",
        "        keyToData.setdefault('smiles', []).append(mol)\n",
        "    for mol in good_ligands:\n",
        "        for _ in range(good_ligand_multiplier):\n",
        "            keyToData.setdefault('smiles', []).append(mol)\n",
        "    pd.DataFrame(keyToData).to_csv(save_path)\n",
        "\n",
        "\n",
        "combine_sampled_and_good_ligands(training_sampled, good_ligands, good_ligand_multiplier=2, save_path='/content/drive/MyDrive/Generative_ML/current_data/active_learning_training.csv')\n",
        "pd.read_csv('/content/drive/MyDrive/Generative_ML/current_data/active_learning_training.csv').shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkPN4kOAZU6_"
      },
      "source": [
        "Now train and generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CXn_Jqt_76M"
      },
      "source": [
        "#BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJqiexX6AAsv"
      },
      "source": [
        "##Set up notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A23L1dGiAO0V"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# install necessary packages\n",
        "!pip install rdkit\n",
        "!pip install pandas==1.5.3\n",
        "!pip install molsets\n",
        "!pip install wandb\n",
        "\n",
        "# clone Sophia optimizer GitHub repository\n",
        "!git clone https://github.com/Liuhong99/Sophia.git\n",
        "\n",
        "# import necessary packages\n",
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, pairwise_distances\n",
        "import os\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import pkg_resources\n",
        "pkg_resources.require(\"pandas==1.5.3\")\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import QED, Crippen\n",
        "from rdkit.Contrib.SA_Score import sascorer\n",
        "from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "from rdkit.Chem.Fingerprints import FingerprintMols\n",
        "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity\n",
        "from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmiles\n",
        "import moses\n",
        "from moses.utils import get_mol\n",
        "from Sophia.sophia import SophiaG\n",
        "import yaml\n",
        "\n",
        "# set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gbEP-i0Av6M"
      },
      "source": [
        "##Utils & Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FK7qHr5Axl3"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, prop=None, scaffold=None):\n",
        "    block_size = model.get_block_size() # define size of context window used for input conditioning\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # limit conditioning input to the most recent block_size elements\n",
        "        logits, _= model(x_cond, prop = prop, scaffold = scaffold) # give input to model and get logits (unnormalized scores or probabilities)\n",
        "        logits = logits[:, -1, :] / temperature # extract the logits for the next token in the sequence\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        ix = torch.multinomial(probs, 1)\n",
        "        x = torch.cat((x, ix), dim=1) # concatenate the chosen token index with the existing sequence\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def fill_mask(model, x, temperature=1.0, prop=None, scaffold=None):\n",
        "    block_size = model.get_block_size()  # Get model's maximum context length\n",
        "    model.eval()\n",
        "\n",
        "    # Check if mask token is present\n",
        "    mask_index = model.config.mask_index  # Assuming 'mask_index' is available in your model's config\n",
        "    while mask_index in x[0]:  # If mask token is present\n",
        "        # Find all mask positions\n",
        "        mask_positions = (x[0] == mask_index).nonzero(as_tuple=True)\n",
        "\n",
        "        for pos in mask_positions:  # Iterate over each mask position\n",
        "            # Limit conditioning input to the most recent block_size elements\n",
        "            x_cond = x if x.size(1) <= block_size else x[:, -block_size:]\n",
        "            # Get token probabilities\n",
        "            logits, _ = model(x_cond, prop=prop, scaffold=scaffold)\n",
        "\n",
        "            # Extract the logits for the masked token position, apply temperature and softmax to get probabilities\n",
        "            logits = logits[0, pos, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            ix = torch.multinomial(probs, 1)  # Sample token from probability distribution\n",
        "\n",
        "            x[0, pos] = ix  # Replace mask token with sampled token\n",
        "\n",
        "    return x  # Return unmasked tensor\n",
        "\n",
        "def check_novelty(gen_smiles, train_smiles):\n",
        "    if len(gen_smiles) == 0:\n",
        "        novel_ratio = 0\n",
        "    else:\n",
        "        duplicates = [1 for mol in gen_smiles if mol in train_smiles]\n",
        "        novel = len(gen_smiles) - sum(duplicates)\n",
        "        novel_ratio = novel*100/len(gen_smiles)\n",
        "    return novel_ratio\n",
        "\n",
        "def canonic_smiles(smiles_or_mol):\n",
        "    mol = get_mol(smiles_or_mol)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return Chem.MolToSmiles(mol)\n",
        "\n",
        "class SMILESDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data=None, content=None, block_size=None, prop=None, scaffold=None, scaffold_maxlen=None, len_data=None, mask_prob=0.15):\n",
        "        if content is None:\n",
        "            self.desc_only = True\n",
        "            return\n",
        "        self.desc_only = False\n",
        "        self.chars = sorted(list(set(content)))\n",
        "        data_size, vocab_size = len(content), len(self.chars)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.stoi = {ch:i for i,ch in enumerate(self.chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(self.chars)}\n",
        "        self.max_len = block_size\n",
        "        self.data = data\n",
        "        self.prop = prop\n",
        "        self.sca = scaffold\n",
        "        self.scaf_max_len = scaffold_maxlen\n",
        "        self.len_data = len_data\n",
        "        self.mask_prob = mask_prob\n",
        "\n",
        "        # add X token to vocabulary\n",
        "        self.stoi['X'] = len(self.stoi)\n",
        "        self.itos[len(self.itos)] = 'X'\n",
        "        self.vocab_size = len(self.stoi)\n",
        "\n",
        "    def export_desc_attributes(self, export_path):\n",
        "        attr_dict = {\n",
        "            \"desc_only\": self.desc_only,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"max_len\": self.max_len,\n",
        "            \"stoi\": self.stoi,\n",
        "            \"itos\": self.itos,\n",
        "            \"scaf_max_len\": self.scaf_max_len,\n",
        "            \"len_data\": self.len_data\n",
        "        }\n",
        "        with open(export_path, 'w') as f:\n",
        "            yaml.dump(attr_dict, f)\n",
        "\n",
        "    def load_desc_attributes(self, load_path):\n",
        "        with open(load_path, 'r') as f:\n",
        "            attr_dict = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "        self.__dict__.update(attr_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        assert not self.desc_only, \"Dataset is not initialized\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert not self.desc_only, \"Dataset is not initialized\"\n",
        "        smiles, prop, scaffold = self.data[idx].strip(), self.prop[idx], self.sca[idx]\n",
        "        if scaffold:\n",
        "          scaffold=scaffold.strip()\n",
        "        pattern =  \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]|X)\"\n",
        "        regex = re.compile(pattern)\n",
        "\n",
        "        #tokenize input string\n",
        "        tokens_arr=regex.findall(smiles)\n",
        "\n",
        "        #Save ground truth encodings before token masking\n",
        "        smiles_true = smiles+str('<')*(self.max_len - len(tokens_arr))\n",
        "        if len(tokens_arr) > self.max_len:\n",
        "            smiles_true = smiles[:self.max_len]\n",
        "        dix_true=[self.stoi[s] for s in regex.findall(smiles_true)]\n",
        "\n",
        "\n",
        "\n",
        "        # randomly replace tokens\n",
        "        mask_idx = [] #True indicates that the corresponding position will be ignored for computing loss\n",
        "        for s in range(len(tokens_arr)):\n",
        "          if random.random() < .15:\n",
        "            mask_idx.append(False)\n",
        "            num = random.random()\n",
        "            if num >= .2: # 80%\n",
        "                tokens_arr[s]='X'\n",
        "            elif num >= 0.1: # 10%\n",
        "                tokens_arr[s] = self.chars[int(random.random()*len(self.chars))]\n",
        "          else:\n",
        "            mask_idx.append(True)\n",
        "\n",
        "        #pad the mask appropriately\n",
        "        while len(mask_idx)<self.max_len:\n",
        "            mask_idx.append(True)\n",
        "\n",
        "        assert len(mask_idx)==self.max_len\n",
        "        smiles=''.join(tokens_arr)\n",
        "\n",
        "        smiles += str('<')*(self.max_len - len(regex.findall(smiles)))\n",
        "        if len(regex.findall(smiles)) > self.max_len:\n",
        "            smiles = smiles[:self.max_len]\n",
        "\n",
        "        assert len(''.join(regex.findall(smiles)))==len(smiles)\n",
        "        smiles=regex.findall(smiles)\n",
        "\n",
        "\n",
        "        dix =  [self.stoi[s] for s in smiles]\n",
        "        if scaffold:\n",
        "          scaffold += str('<')*(self.scaf_max_len - len(regex.findall(scaffold)))\n",
        "          if len(regex.findall(scaffold)) > self.scaf_max_len:\n",
        "              scaffold = scaffold[:self.scaf_max_len]\n",
        "          scaffold=regex.findall(scaffold)\n",
        "          sca_dix = [self.stoi[s] for s in scaffold]\n",
        "          sca_tensor = torch.tensor(sca_dix, dtype=torch.long)\n",
        "        else:\n",
        "          sca_tensor=torch.tensor(scaffold,dtype=torch.bool)\n",
        "        x = torch.tensor(dix, dtype=torch.long)\n",
        "        y = torch.tensor(dix_true, dtype=torch.long)\n",
        "        prop = torch.tensor([prop], dtype=torch.float)\n",
        "        return x, y, prop, sca_tensor, torch.tensor(mask_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXLdZWy9A5Tk"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLo8qZ7yA4SF"
      },
      "outputs": [],
      "source": [
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size=None, block_size=None, mask_index=None, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.mask_index = mask_index\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "    def export_attributes(self, export_path):\n",
        "        with open(export_path, 'w') as f:\n",
        "            yaml.dump(vars(self), f)\n",
        "\n",
        "    def load_attributes(self, load_path):\n",
        "        with open(load_path, 'r') as f:\n",
        "            config_dict = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "        self.__dict__.update(config_dict)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "        self.config = config\n",
        "\n",
        "        self.query = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n",
        "        self.key = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n",
        "        self.value = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(config.att_drop_rate)\n",
        "        self.resid_drop = nn.Dropout(config.att_drop_rate)\n",
        "\n",
        "        self.proj = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "        num = int(bool(config.num_props)) + int(config.scaffold_maxlen)\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size + num, config.block_size + num))\n",
        "                                .view(1, 1, config.block_size + num, config.block_size + num))\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size()\n",
        "        # apply attention functions to get tensors with dimensions (B, n_head, T, head_size)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head)\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head)\n",
        "        if self.config.do_flash:\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "            v = v.transpose(1, 2)\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.config.att_drop_rate if self.training else 0, is_causal=self.config.is_causal)\n",
        "            y = y.transpose(1, 2)\n",
        "        else:\n",
        "            # (B h T s) @ (B h s T) -> (B h T T)\n",
        "            att = torch.einsum('bths,bihs->bhti', q, k) / math.sqrt(k.size(-1))\n",
        "            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            # (B h T T) @ (B h T s) -> (B h T s)\n",
        "            y = torch.einsum('bhtq,bqhs->bths', att, v)\n",
        "            self.att_weights = att\n",
        "        self.attended = y\n",
        "        y = y.contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        self.out = y\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.mlp = nn.Sequential(nn.Linear(config.n_embed, config.ff_mult*config.n_embed), nn.GELU() if config.doGELU else nn.ReLU(),\n",
        "            nn.Linear(config.ff_mult*config.n_embed, config.n_embed), nn.Dropout(config.att_drop_rate))\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.attn(self.ln1(x))\n",
        "        x = x + y # perform a residual connection by summing input and attention output\n",
        "        x = x + self.mlp(self.ln2(x)) # apply layer normalization and then MLP, create a residual connection with input\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "        self.type_emb = nn.Embedding(2, config.n_embed)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embed))\n",
        "        # if conditioning on at least 1 property:\n",
        "        if config.num_props:\n",
        "            # initialize property linear layer, map property vector to embedding dimension\n",
        "            self.prop_nn = nn.Linear(config.num_props, config.n_embed)\n",
        "\n",
        "        self.drop = nn.Dropout(config.gpt_drop_rate)\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(config.n_embed)\n",
        "        self.head = nn.Linear(config.n_embed, config.vocab_size, bias=config.gpt_bias)\n",
        "        self.block_size = config.block_size # define the context size\n",
        "        self.apply(self._init_weights) # initialize weights and apply to all relevant modules in the model\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay, no_decay = set(), set()\n",
        "        no_decay = set()\n",
        "\n",
        "        whitelist_weight_modules = (torch.nn.Linear)\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        # for named module of the model:\n",
        "        for mn, m in self.named_modules():\n",
        "            # for named parameter of each module:\n",
        "            for pn, p in m.named_parameters():\n",
        "                # construct full parameter name by concatenating module name and parameter name, separated by a dot\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias') or ('bias' in pn):\n",
        "                    no_decay.add(fpn)\n",
        "                elif (pn.endswith('weight') or ('weight' in pn)) and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "        no_decay.add('pos_emb')\n",
        "        param_dict = {pn:p for pn, p in self.named_parameters()}\n",
        "        assert len(decay & no_decay) == 0\n",
        "        # assert that all parameters from both sets have been correctly separated\n",
        "        assert len(param_dict.keys() - (decay | no_decay)) == 0\n",
        "        optim_groups = [{\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "                        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}]\n",
        "        optimizer = SophiaG(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, rho=train_config.rho, weight_decay=train_config.weight_decay)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None, prop=None, scaffold=None):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size\n",
        "        if self.config.num_props:\n",
        "            assert prop.size(-1) == self.config.num_props, f\"number of properties {prop.size(-1)=} doesn't match the expected size {self.config.num_props=}\"\n",
        "        token_embeddings = self.tok_emb(idx)\n",
        "        position_embeddings = self.pos_emb[:, :t, :]\n",
        "        type_embeddings = self.type_emb(torch.ones((b,t), dtype=torch.long, device=idx.device))\n",
        "        x = self.drop(token_embeddings + position_embeddings + type_embeddings)\n",
        "        # Condition on properties\n",
        "        if self.config.num_props:\n",
        "            type_embd = self.type_emb(torch.zeros((b, 1), dtype=torch.long, device=idx.device))\n",
        "            if prop.ndim == 2:\n",
        "                p = self.prop_nn(prop.unsqueeze(1))\n",
        "            else:\n",
        "                p = self.prop_nn(prop)\n",
        "            p += type_embd\n",
        "            x = torch.cat([p, x], 1)\n",
        "        # Condition on scaffold\n",
        "        if self.config.scaffold:\n",
        "            type_embd = self.type_emb(torch.zeros((b, 1), dtype = torch.long, device = idx.device))\n",
        "            scaffold_embeds = self.tok_emb(scaffold)\n",
        "            scaffold_embeds += type_embd\n",
        "            x = torch.cat([scaffold_embeds, x], 1)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for layer in self.blocks:\n",
        "            x = layer(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        if self.config.num_props and self.config.scaffold:\n",
        "            num = int(bool(self.config.num_props)) + int(self.config.scaffold_maxlen)\n",
        "        elif self.config.num_props:\n",
        "            num = int(bool(self.config.num_props))\n",
        "        elif self.config.scaffold:\n",
        "            num = int(self.config.scaffold_maxlen)\n",
        "        else:\n",
        "            num = 0\n",
        "        # Slice the logits tensor along the second dimension to exclude the first num elements\n",
        "        logits = logits[:, num:, :]\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jia_2uzBQOp"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It9Ei0OYBSey"
      },
      "outputs": [],
      "source": [
        "class TrainerConfig:\n",
        "    epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.965, 0.99) #(0.9, 0.95)\n",
        "    rho = 0.04 # For SophiaG\n",
        "    weight_decay = 0.1\n",
        "\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # number of warm-up tokens for learning rate decay\n",
        "    final_tokens = 260e9 # number of tokens at which the learning rate decays to 10% of the original\n",
        "    num_workers = 0 # number of worker processes to use for loading data\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "def loss_function(logits, y, mask_idx):\n",
        "    loss=nn.CrossEntropyLoss(ignore_index=-1)\n",
        "    y[mask_idx]=-1\n",
        "    return loss(logits.view(-1,logits.size(-1)),y.view(-1))\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_dataset, test_dataset, config, stoi, itos):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.config = config\n",
        "        self.stoi = stoi\n",
        "        self.itos = itos\n",
        "        self.model = self.model.to(config.device)\n",
        "\n",
        "    def train(self, wandb):\n",
        "        model, config = self.model, self.config\n",
        "        optimizer = model.configure_optimizers(config)\n",
        "        scaler = GradScaler() # define variable used for gradient scaling in mixed-precision training\n",
        "        self.tokens = 0 # initialize a counter used for learning rate decay\n",
        "\n",
        "        def run_epoch(split):\n",
        "            is_train = split == 'train'\n",
        "            model.train(is_train)\n",
        "            data = self.train_dataset if is_train else self.test_dataset\n",
        "            loader = DataLoader(data, shuffle=True, pin_memory=True, batch_size=config.batch_size, num_workers=config.num_workers)\n",
        "            losses = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            # for batch index, batch in progress bar:\n",
        "            for it, (x, y, p, scaffold, mask_idx) in pbar:\n",
        "                # move the input data tensor, target data tensor, property tensor, and scaffold tensor to GPU\n",
        "                x, y, p, scaffold , mask_idx = x.to(config.device), y.to(config.device), p.to(config.device), scaffold.to(config.device), mask_idx.to(config.device)\n",
        "                # allow model to use lower-precision computations for improved memory usage\n",
        "                if config.device == 'cuda':\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        with torch.set_grad_enabled(is_train):\n",
        "                            logits= model(x, y, p, scaffold)\n",
        "                            loss = loss_function(logits, y, mask_idx)\n",
        "                            loss = loss.mean()\n",
        "                            losses.append(loss.item())\n",
        "                else:\n",
        "                    with torch.cpu.amp.autocast():\n",
        "                        with torch.set_grad_enabled(is_train):\n",
        "                            logits = model(x, y, p, scaffold)\n",
        "                            loss = loss_function(x, y, mask_idx)\n",
        "                            loss = loss.mean()\n",
        "                            losses.append(loss.item())\n",
        "\n",
        "                if is_train:\n",
        "                    model.zero_grad()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.unscale_(optimizer) # unscale the gradients of the optimizer's parameters to their original values\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients of model parameters to prevent them from exploding, setting maximum gradient norm to be 1.0\n",
        "                    scaler.step(optimizer) # update the optimizer's parameters based on calculated gradients\n",
        "                    scaler.update() # update the scale factor of the gradient scaler\n",
        "                    if config.lr_decay:\n",
        "                        self.tokens += (y >= 0).sum() # increment the number of processed tokens by the count of valid tokens (not padding or special tokens)\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens)) # perform a linear warm-up\n",
        "                        else:\n",
        "                            # calculate the progress of training in terms of the number of tokens processed\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            # calculate the scaling factor for the learning rate (between 0.1 and 1.0)\n",
        "                            # to gradually reduce learning rate as training progresses\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult # multiply the base learning rate by the scaling factor to obtain the updated learning rate\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "                    # log training progress using Weights & Biases\n",
        "                    if wandb is not None:\n",
        "                        wandb.log({'step_train_loss': loss, 'train_step': it + epoch*len(loader), 'learning_rate': lr})\n",
        "                    # update the description of the progress bar with epoch, iteration, and training loss\n",
        "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "            return float(np.mean(losses))\n",
        "\n",
        "\n",
        "        # initialize best loss as infinity\n",
        "        best_loss = float('inf')\n",
        "        for epoch in range(config.epochs):\n",
        "            print(f'{epoch=}')\n",
        "            train_loss = run_epoch('train')\n",
        "            if self.test_dataset is not None:\n",
        "                test_loss = run_epoch('test')\n",
        "            if wandb is not None:\n",
        "                wandb.log({'epoch_valid_loss': test_loss, 'epoch_train_loss': train_loss, 'epoch': epoch + 1})\n",
        "            good_model = self.test_dataset is None or test_loss < best_loss\n",
        "            if good_model:\n",
        "                best_loss = test_loss\n",
        "                torch.save(self.model.state_dict(), self.config.ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gthVEc2ZBX2u"
      },
      "outputs": [],
      "source": [
        "def load_data(train_config_dict):\n",
        "    if (cut:=train_config_dict[\"slice_data\"]):\n",
        "        train_data = pd.read_csv(train_config_dict[\"train_path\"])[:cut]\n",
        "        val_data = pd.read_csv(train_config_dict[\"val_path\"])[:cut]\n",
        "    else:\n",
        "        train_data = pd.read_csv(train_config_dict[\"train_path\"])\n",
        "        val_data = pd.read_csv(train_config_dict[\"val_path\"])\n",
        "\n",
        "    smiles = train_data['smiles']\n",
        "    vsmiles = val_data['smiles']\n",
        "\n",
        "    prop = train_data[train_config_dict[\"props\"]].values.tolist()\n",
        "    vprop = val_data[train_config_dict[\"props\"]].values.tolist()\n",
        "\n",
        "    scaffold = train_data['scaffold_smiles']\n",
        "    vscaffold = val_data['scaffold_smiles']\n",
        "\n",
        "    # define a regular expression that matches molecular tokens in SMILES strings\n",
        "    pattern = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]|X)\"\n",
        "    # compile pattern into a regular expression object that can be used for matching operations\n",
        "    regex = re.compile(pattern)\n",
        "\n",
        "    context = {'<'}\n",
        "\n",
        "    max_len, scaffold_max_len = 0, 0\n",
        "    for iterator in (smiles.values, vsmiles.values):\n",
        "        for i in iterator:\n",
        "            chars = regex.findall(i.strip())\n",
        "            max_len = max(max_len, len(chars))\n",
        "            for char in chars:\n",
        "                context.add(char)\n",
        "    for iterator in (scaffold.values, vscaffold.values):\n",
        "        for i in iterator:\n",
        "            chars = regex.findall(i.strip())\n",
        "            scaffold_max_len = max(scaffold_max_len, len(chars))\n",
        "            for char in chars:\n",
        "                context.add(char)\n",
        "    print(max_len)\n",
        "    context = sorted(list(context))\n",
        "\n",
        "    smiles = [i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in smiles]\n",
        "    vsmiles = [i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in vsmiles]\n",
        "    scaffold = [i + str('<')*(scaffold_max_len - len(regex.findall(i.strip()))) for i in scaffold]\n",
        "    vscaffold = [i + str('<')*(scaffold_max_len - len(regex.findall(i.strip()))) for i in vscaffold]\n",
        "\n",
        "    # if not conditioning on scaffolds: define 'scaffold' as a list of length SMILES string filled with 'False' values\n",
        "    scaffold=[False]*len(smiles) if not train_config_dict[\"use_scaf\"] else scaffold\n",
        "    train_dataset = SMILESDataset(smiles, context, max_len, prop=prop, scaffold=scaffold, scaffold_maxlen=scaffold_max_len, len_data=len(train_data), mask_prob=0.15)\n",
        "    valid_dataset = SMILESDataset(vsmiles, context, max_len, prop=vprop, scaffold=vscaffold, scaffold_maxlen=scaffold_max_len, len_data=len(val_data), mask_prob=0.15)\n",
        "    train_dataset.export_desc_attributes(train_config_dict[\"desc_path\"])\n",
        "    return train_dataset, valid_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWAIfqxj9to1"
      },
      "outputs": [],
      "source": [
        "def pretrain_BERT(train_dataset, valid_dataset, model_config_dict, train_config_dict):\n",
        "  \"\"\"\n",
        "  OUTPUTS:\n",
        "  1) checkpoint of trained model parameters\n",
        "  2) Weights & Biases logged run\n",
        "  \"\"\"\n",
        "\n",
        "  mask_index = train_dataset.stoi['X']\n",
        "  mconf = GPTConfig(train_dataset.vocab_size, train_dataset.max_len, mask_index=mask_index, num_props=len(train_config_dict[\"props\"]), scaffold=train_config_dict[\"use_scaf\"], scaffold_maxlen=train_dataset.scaf_max_len, **model_config_dict)\n",
        "  model = GPT(mconf).to(model_config_dict[\"device\"])\n",
        "\n",
        "  if train_config_dict['load_cpt'] != None:\n",
        "    model.load_state_dict(torch.load(train_config_dict['load_cpt']))\n",
        "  torch.compile(model)\n",
        "\n",
        "  tconf = TrainerConfig(warmup_tokens=0.1*train_dataset.len_data*train_dataset.max_len, final_tokens=train_config_dict[\"epochs\"]*train_dataset.len_data*train_dataset.max_len, block_size=train_dataset.max_len, **train_config_dict)\n",
        "  trainer = Trainer(model, train_dataset, valid_dataset, tconf, train_dataset.stoi, train_dataset.itos)\n",
        "\n",
        "  %env WANDB_EXECUTABLE=python3\n",
        "  wandb.init(project=\"mol_transformer\", name=train_config_dict[\"wandb_runname\"])\n",
        "  trainer.train(wandb=wandb)\n",
        "  return model, tconf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eApgFW7e9vd6"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "BASE = '/content/drive/MyDrive/Generative_ML/'\n",
        "\n",
        "model_config_dict = {\n",
        "    \"device\": DEVICE,\n",
        "    \"att_bias\": False,\n",
        "    \"gpt_bias\": True,\n",
        "    \"att_drop_rate\": 0.1,\n",
        "    \"gpt_drop_rate\": 0.1,\n",
        "    \"n_layer\": 8,\n",
        "    \"n_head\": 8,\n",
        "    \"n_embed\": 256,\n",
        "    \"ff_mult\": 4, # multiplier for FF inside multihead,\n",
        "    \"doGELU\": True, # else ReLU\n",
        "    \"attention_times\": [],\n",
        "    \"do_flash\": True,\n",
        "    \"is_causal\": False\n",
        "}\n",
        "\n",
        "train_config_dict = {\n",
        "    \"desc_path\": BASE + 'checkpoints/descriptors_10k.yaml',\n",
        "    \"train_path\": BASE + 'data/MOSES_processed_train.csv',\n",
        "    \"val_path\": BASE + 'data/MOSES_processed_val.csv',\n",
        "    \"slice_data\": False,\n",
        "    \"ckpt_path\": BASE + 'checkpoints/7-03_all_mask.pt',\n",
        "    \"wandb_runname\": \"7_03_all_mask\",\n",
        "    \"use_scaf\": False,\n",
        "    \"props\": [],\n",
        "    \"device\": DEVICE,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 512,\n",
        "    \"lr_decay\": True,\n",
        "    \"num_workers\": 0,\n",
        "    \"load_cpt\": None\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk6icEUs9xFh",
        "outputId": "c321786f-2171-47f3-a73c-48e0f5015aad"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = load_data(train_config_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg3xxdXyRLZ3"
      },
      "outputs": [],
      "source": [
        "# define function to train BERT model\n",
        "model, tconf = pretrain_BERT(\n",
        "                train_dataset = train_dataset,\n",
        "                valid_dataset = val_dataset,\n",
        "                model_config_dict = model_config_dict,\n",
        "                train_config_dict = train_config_dict\n",
        "        )\n",
        "\n",
        "#GK wandb API Key: c99c9a01523f93287716691fa3360b1f4566e115\n",
        "#RB wandb API Key: 4d3d628c6b5a4b3554c7a89ea50df8a4a6be0f85"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjaK3DzoSLB5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywY37VNWSLS9"
      },
      "source": [
        "##Generation & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChKsmt1qSLS9"
      },
      "outputs": [],
      "source": [
        "def generate_SMILES(model_config_dict, inference_config_dict):\n",
        "  props = inference_config_dict[\"props\"]\n",
        "  scaffold = inference_config_dict[\"scaffold\"]\n",
        "\n",
        "  # define a regular expression that matches molecular tokens in SMILES strings\n",
        "  pattern = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]|X)\"\n",
        "  regex = re.compile(pattern)\n",
        "\n",
        "  dataset = SMILESDataset()\n",
        "  dataset.load_desc_attributes(inference_config_dict['desc_path'])\n",
        "  use_scaf = False if scaffold is None else True\n",
        "\n",
        "  mconf = GPTConfig(dataset.vocab_size, dataset.max_len, num_props=len(props), scaffold=use_scaf, scaffold_maxlen=dataset.scaf_max_len, **model_config_dict)\n",
        "  model = GPT(mconf).to(model_config_dict['device'])\n",
        "  torch.compile(model)\n",
        "\n",
        "  # load parameters into the model\n",
        "  model.load_state_dict(torch.load(inference_config_dict[\"model_params\"], map_location=torch.device(model_config_dict['device'])))\n",
        "  block_size = model.get_block_size() #inference_config_dict[\"block_size\"]\n",
        "  assert block_size == dataset.max_len, \"Warning: model block size and dataset block size are different\"\n",
        "  # calculate number of generation iterations from total number of SMILES to generate and batch size\n",
        "  gen_iter = math.ceil(inference_config_dict[\"gen_size\"] / inference_config_dict[\"batch_size\"])\n",
        "  stoi = dataset.stoi # define dictionary to map strings to integers\n",
        "  itos = dataset.itos # define dictionary to map integers to strings\n",
        "  # is a scaffold is defined for conditioning:\n",
        "  if scaffold is not None:\n",
        "      # pad '<' to end of scaffold string to achieve maximum scaffold length\n",
        "      scaffold += str('<')*(dataset.scaf_max_len - len(regex.findall(scaffold)))\n",
        "      # convert the scaffold SMILES to a tensor of integers and repeat along the batch dimension, move to GPU\n",
        "      scaffold=torch.tensor([stoi[s] for s in regex.findall(scaffold)])[None,...].repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n",
        "\n",
        "  if props is None:\n",
        "    p = None\n",
        "  elif len(props) == 1:\n",
        "    # create a tensor for conditioning with a single property value\n",
        "    p = torch.tensor([[props[0]]]).repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n",
        "  else:\n",
        "    # create a tensor for conditioning with multiple property values\n",
        "    p = torch.tensor([props]).repeat(inference_config_dict[\"batch_size\"], 1).unsqueeze(1).to(model_config_dict['device'])\n",
        "\n",
        "  molecules = []\n",
        "  for i in tqdm(range(gen_iter)):\n",
        "          # create an input tensor by converting 'context' to a tensor of token indices,\n",
        "          # repeat this batch times along the batch dimension\n",
        "          x = torch.tensor([stoi[s] for s in regex.findall(inference_config_dict[\"context\"])], dtype=torch.long)[None,...].repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n",
        "\n",
        "          if ['X'] in inference_config_dict['context']:\n",
        "            # call sample function to generate molecules conditioned on the input\n",
        "            y = fill_mask(model, x, block_size, temperature=inference_config_dict[\"temp\"], prop=p, scaffold=scaffold)\n",
        "          else:\n",
        "            y = sample(model, x, block_size, temperature=inference_config_dict[\"temp\"], prop=p, scaffold=scaffold)\n",
        "\n",
        "          # for each generated molecule:\n",
        "          for gen_mol in y:\n",
        "                  # convert generated molecule from list of integers to list of strings and concatenate to one string\n",
        "                  completion = ''.join([itos[int(i)] for i in gen_mol])\n",
        "                  # remove padding tokens\n",
        "                  completion = completion.replace('<', '')\n",
        "                  # convert the string representation of the molecule to an rdkit Mol object\n",
        "                  mol = get_mol(completion)\n",
        "                  # if an rdkit Mol object was created:\n",
        "                  if mol:\n",
        "                          # append the Mol object to the list\n",
        "                          molecules.append(mol)\n",
        "  # create dataframe where first column contains rdkit Mols and second column contins SMILES\n",
        "  results = pd.DataFrame([{'molecule' : i, 'smiles': Chem.MolToSmiles(i)} for i in molecules])\n",
        "  # iterate over each SMILES and ensure that equivalent molecules have same SMILES\n",
        "  canon_smiles = [canonic_smiles(s) for s in results['smiles']]\n",
        "  # create set of unique SMILES strings\n",
        "  unique_smiles = list(set(canon_smiles))\n",
        "  data = pd.read_csv(inference_config_dict[\"train_data\"]) # load training data\n",
        "  novel_ratio = check_novelty(unique_smiles, set(data['smiles'])) # calculate novelty ratio from generated SMILES and training SMILES\n",
        "  results['qed'] = results['molecule'].apply(lambda x: QED.qed(x)) # quantitative estimate of drug-likeliness (QED)\n",
        "  results['sas'] = results['molecule'].apply(lambda x: sascorer.calculateScore(x)) #synthetic accessibility score (SAS)\n",
        "  results['logp'] = results['molecule'].apply(lambda x: Crippen.MolLogP(x)) #(measure of hydrophobicity)\n",
        "  results['tpsa'] = results['molecule'].apply(lambda x: CalcTPSA(x)) #topological polar surface area (TPSA)\n",
        "  results['validity'] = np.round(len(results)/(inference_config_dict[\"batch_size\"]*gen_iter), 3)\n",
        "  results['unique'] = np.round(len(unique_smiles)/len(results), 3)\n",
        "  results['novelty'] = np.round(novel_ratio/100, 3)\n",
        "  # save the dataframe as a csv file\n",
        "  results.to_csv(inference_config_dict[\"save_path\"], index = False)\n",
        "  # print all evaluation metrics using function from moses package\n",
        "  print(moses.get_all_metrics(list(results['smiles'].values), device=model_config_dict['device']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N-mabNHSLS-"
      },
      "outputs": [],
      "source": [
        "inference_config_dict = {\n",
        "    # \"model_params\": train_config_dict[\"ckpt_path\"],\n",
        "    \"model_params\": BASE + 'checkpoints/test_6_21.pt',\n",
        "    \"train_data\": train_config_dict[\"train_path\"],\n",
        "    # \"desc_path\": model_config_dict[\"desc_path\"],\n",
        "    \"desc_path\": BASE + 'checkpoints/descriptors_200k.yaml',\n",
        "    \"save_path\": BASE + 'data/test_06_21_t.csv',\n",
        "    \"batch_size\": 1,\n",
        "    \"gen_size\": 10,\n",
        "    \"temp\": 1,\n",
        "    \"context\": \"C1CCXCCC1\",\n",
        "    \"scaffold\": None,\n",
        "    \"props\": []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5DqRGAxSLS-"
      },
      "outputs": [],
      "source": [
        "# run function to generate SMILES strings\n",
        "generate_SMILES(\n",
        "              model_config_dict = model_config_dict,\n",
        "              inference_config_dict = inference_config_dict\n",
        "              )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jzRj6bGQpUaM",
        "7pfZIi6qpUaQ",
        "iTFNFFNGpUaR",
        "zBgupbjAPYu7",
        "4CXn_Jqt_76M",
        "EJqiexX6AAsv",
        "0gbEP-i0Av6M",
        "VXLdZWy9A5Tk",
        "6Jia_2uzBQOp",
        "ywY37VNWSLS9"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
