{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from pprint import pprint as pp\n",
    "import yaml\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "BASE_PATH = '/Users/morgunov/batista/Summer/pipeline/'\n",
    "REGEX_PATTERN = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|@@|\\?|>|!|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "regex = re.compile(REGEX_PATTERN)\n",
    "\n",
    "PRETRAINING_PATH = BASE_PATH + '1. Pretraining/'\n",
    "GENERATION_PATH = BASE_PATH + '2. Generation/'\n",
    "SAMPLING_PATH = BASE_PATH + '3. Sampling/'\n",
    "DIFFDOCK_PATH = BASE_PATH + '4. DiffDock/'\n",
    "SCORING_PATH = BASE_PATH + '5. Scoring/'\n",
    "AL_PATH = BASE_PATH + '6. ActiveLearning/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Convert all files to .csv.gz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid(smiles):\n",
    "    valid_set = set()\n",
    "    for smile in tqdm(smiles, total=len(smiles)):\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is not None:\n",
    "            valid_set.add(smile)\n",
    "    return valid_set\n",
    "\n",
    "def convert_bindingdb(pretrain_path, file_name):\n",
    "    with open(f\"{pretrain_path}datasets/original_files/{file_name}\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    smiles_list, metrics = [], {}\n",
    "    print('Starting to fill smiles_list from BindingDB')\n",
    "    for i, rline in tqdm(enumerate(lines), total=len(lines)):\n",
    "        if i == 0: continue\n",
    "        smile = rline.rstrip().split('\\t')[1]\n",
    "        smiles_list.append(smile)\n",
    "    smiles_set = set(smiles_list)\n",
    "    metrics.update(dict(entries=len(smiles_list), unique=len(smiles_set)))\n",
    "    print('Starting to remove invalid smiles from BindingDB')\n",
    "    valid_set = remove_invalid(smiles_set)\n",
    "    pd.DataFrame({\"smiles\": list(valid_set)}).to_csv(f\"{pretrain_path}datasets/converted/bindingdb.csv.gz\", compression='gzip')\n",
    "    metrics.update(dict(valid=len(valid_set)))\n",
    "    # Dump metrics as a yaml file\n",
    "    with open(f\"{pretrain_path}dataset_metrics/bindingdb_metrics.yaml\", 'w') as f:\n",
    "        yaml.dump(metrics, f)\n",
    "\n",
    "def convert_smiles_to_df(pretrain_path, file_name):\n",
    "    with open(f\"{pretrain_path}datasets/original_files/{file_name}.smiles\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    smiles = []\n",
    "    for i, rline in tqdm(enumerate(lines), total=len(lines)):\n",
    "        smile = rline.rstrip()\n",
    "        smiles.append(smile)\n",
    "    pd.DataFrame({\"smiles\": smiles}).to_csv(f\"{pretrain_path}datasets/converted/{file_name}.csv.gz\", compression='gzip')\n",
    "    return smiles\n",
    "\n",
    "def convert_guacamol(pretrain_path, file_name):\n",
    "    print('test partition')\n",
    "    smiles_test = convert_smiles_to_df(pretrain_path, f\"{file_name}_test\")\n",
    "    print('valid partition')\n",
    "    smiles_valid = convert_smiles_to_df(pretrain_path, f\"{file_name}_valid\")\n",
    "    print('train partition')\n",
    "    smiles_train = convert_smiles_to_df(pretrain_path, f\"{file_name}_train\")\n",
    "    print('all partition')\n",
    "    smiles_all = convert_smiles_to_df(pretrain_path, f\"{file_name}_all\")\n",
    "    metrics = {\"test\": len(smiles_test), \"valid\": len(smiles_valid), \"train\": len(smiles_train), \"all\": len(smiles_all)}\n",
    "    print('checking that there are no invalid smiles')\n",
    "    valid_set = remove_invalid(smiles_all)\n",
    "    assert len(valid_set) == len(smiles_all), f\"{len(valid_set)} valid smiles out of {len(smiles_all)}\"\n",
    "    \n",
    "    with open(f\"{pretrain_path}dataset_metrics/guacamol_metrics.yaml\", 'w') as f:\n",
    "        yaml.dump(metrics, f)\n",
    "\n",
    "def convert_chembl(pretrain_path, file_name):\n",
    "    with open(f\"{pretrain_path}datasets/original_files/{file_name}\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    smiles_list, metrics = [], {}\n",
    "    print('Starting to fill smiles_list from ChemBL')\n",
    "    for i, rline in enumerate(lines):\n",
    "        if i == 0: continue\n",
    "        smile = rline.rstrip().split('\\t')[1]\n",
    "        smiles_list.append(smile)\n",
    "    smiles_set = set(smiles_list)\n",
    "    metrics.update(dict(entries=len(smiles_list), unique=len(smiles_set)))\n",
    "    print('Starting to remove invalid smiles from ChemBL')\n",
    "    valid_set = remove_invalid(smiles_set)\n",
    "    pd.DataFrame({\"smiles\": list(valid_set)}).to_csv(f\"{pretrain_path}datasets/converted/chembl.csv.gz\", compression='gzip')\n",
    "    metrics.update(dict(valid=len(valid_set)))\n",
    "    # Dump metrics as a yaml file\n",
    "    with open(f\"{pretrain_path}dataset_metrics/chembl_metrics.yaml\", 'w') as f:\n",
    "        yaml.dump(metrics, f)\n",
    "\n",
    "def convert_moses(pretrain_path, file_name):\n",
    "    pd.read_csv(f\"{pretrain_path}datasets/original_files/{file_name}_train.csv.gz\").rename(columns={\"SMILES\": \"smiles\"}).to_csv(f\"{pretrain_path}datasets/converted/{file_name}_train.csv.gz\")\n",
    "    pd.read_csv(f\"{pretrain_path}datasets/original_files/{file_name}_test.csv.gz\").rename(columns={\"SMILES\": \"smiles\"}).to_csv(f\"{pretrain_path}datasets/converted/{file_name}_test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all(pretrain_path):\n",
    "    print(\"Converting BindingDB\")\n",
    "    convert_bindingdb(pretrain_path, \"BindingDB_All.tsv\")\n",
    "    print(\"Converting GuacaMole\")\n",
    "    convert_guacamol(pretrain_path, \"guacamol_v1\")\n",
    "    print(\"Converting ChemBL\")\n",
    "    convert_chembl(pretrain_path, \"chembl_33_chemreps.txt\")\n",
    "    print(f\"Converting MOSES\")\n",
    "    convert_moses(pretrain_path, \"moses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_all(PRETRAINING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_all(pretrain_path):\n",
    "    chembl = pd.read_csv(f\"{pretrain_path}datasets/converted/chembl.csv.gz\")\n",
    "    bindingdb = pd.read_csv(f\"{pretrain_path}datasets/converted/bindingdb.csv.gz\")\n",
    "    moses_train = pd.read_csv(f\"{pretrain_path}datasets/converted/moses_train.csv.gz\")\n",
    "    moses_test = pd.read_csv(f\"{pretrain_path}datasets/converted/moses_test.csv.gz\")\n",
    "    guacamol = pd.read_csv(f\"{pretrain_path}datasets/converted/guacamol_v1_all.csv.gz\")\n",
    "\n",
    "    combined = pd.concat([chembl, bindingdb, moses_train, moses_test, guacamol])\n",
    "    print(f\"Combined df has {len(combined)} rows\")\n",
    "    combined.drop_duplicates(subset='smiles', inplace=True)\n",
    "    print(f\"Combined df has {len(combined)} rows after dropping duplicates\")\n",
    "    combined.dropna(inplace=True)\n",
    "    print(f\"Combined df has {len(combined)} rows after dropping NaNs\")\n",
    "    combined.to_csv(f\"{pretrain_path}datasets/converted/combined.csv.gz\")\n",
    "    return combined\n",
    "\n",
    "combined = concat_all(PRETRAINING_PATH)\n",
    "# Combined df has 6906124 rows\n",
    "# Combined df has 5622772 rows after dropping duplicates\n",
    "# Combined df has 5622771 rows after dropping NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def calculate_cross_repetitions(pretrain_path):\n",
    "    chembl = pd.read_csv(f\"{pretrain_path}datasets/converted/chembl.csv.gz\")\n",
    "    bindingdb = pd.read_csv(f\"{pretrain_path}datasets/converted/bindingdb.csv.gz\")\n",
    "    moses_train = pd.read_csv(f\"{pretrain_path}datasets/converted/moses_train.csv.gz\")\n",
    "    moses_test = pd.read_csv(f\"{pretrain_path}datasets/converted/moses_test.csv.gz\")\n",
    "    moses_combined = pd.concat([moses_train, moses_test])\n",
    "    guacamol = pd.read_csv(f\"{pretrain_path}datasets/converted/guacamol_v1_all.csv.gz\")\n",
    "\n",
    "    datasets = [('moses', set(moses_combined['smiles'])), ('bindingDB', set(bindingdb['smiles'])), ('ChemBL', set(chembl['smiles'])), ('GuacaMol', set(guacamol['smiles']))]\n",
    "    metrics = {}\n",
    "    for name, ds in datasets:\n",
    "        metrics[f\"Number of molecules in {name}\"] = len(ds)\n",
    "        \n",
    "    pairs = list(itertools.combinations(datasets, 2))\n",
    "    for (name1, ds1), (name2, ds2) in pairs:\n",
    "        metrics[f\"Number of molecules in both {name1} and {name2}\"] = len(ds1 & ds2)\n",
    "\n",
    "    combined = pd.concat([chembl, bindingdb, moses_combined, guacamol])\n",
    "    combined.drop_duplicates(subset=['smiles'], inplace=True)\n",
    "    combined.dropna()\n",
    "    metrics[\"Number of molecules in all datasets\"] = len(combined)\n",
    "    # dump metrics to a yaml file\n",
    "    with open(f\"{pretrain_path}dataset_metrics/cross_repetitions.yaml\", 'w') as f:\n",
    "        yaml.dump(metrics, f)\n",
    "    return metrics\n",
    "calculate_cross_repetitions(PRETRAINING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.title_size = 20\n",
    "        self.axis_title_size = 14\n",
    "        self.tick_font_size = 12\n",
    "        self.text_color=\"#333333\"\n",
    "        self.background = \"white\"\n",
    "        self.grid_color = \"#e2e2e2\"\n",
    "        self.line_color = \"#000000\"\n",
    "        self.font_family = 'Helvetica'\n",
    "        self.show_xgrid = False\n",
    "        self.show_ygrid = False\n",
    "        self.width = 600\n",
    "        self.height = 400\n",
    "        self.title = ''\n",
    "        self.xaxis_title = ''\n",
    "        self.yaxis_title = ''\n",
    "    \n",
    "    def update_parameters(self, params):\n",
    "        for key, val in params.items():\n",
    "            setattr(self, key, val)\n",
    "        \n",
    "\n",
    "    def style_figure(self, figure):\n",
    "        figure.update_layout({\n",
    "            'margin': {'t': 50, 'b': 50, 'l': 50, 'r': 50},\n",
    "            'plot_bgcolor': self.background,\n",
    "            'paper_bgcolor': self.background,\n",
    "            'title': {\n",
    "                'text': self.title,\n",
    "                'font': {\n",
    "                    'size': self.title_size,\n",
    "                    'color': self.text_color,\n",
    "                    'family': self.font_family\n",
    "                },\n",
    "            },\n",
    "            'height': self.height,  # Set fixed size ratio 3:4\n",
    "            'width': self.width, \n",
    "            'font': {\n",
    "                'family': self.font_family,\n",
    "                'size': self.tick_font_size,\n",
    "                'color': self.text_color\n",
    "            },\n",
    "            'legend': {\n",
    "                'font': {\n",
    "                    'family': self.font_family,\n",
    "                    'size': self.tick_font_size,\n",
    "                    'color': self.text_color\n",
    "                },\n",
    "            },\n",
    "        })\n",
    "\n",
    "        # Setting the title size and color and grid for both x and y axes\n",
    "        figure.update_xaxes(\n",
    "            title=self.xaxis_title,\n",
    "            title_font={'size': self.axis_title_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            tickfont={'size': self.tick_font_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            showgrid=self.show_xgrid,\n",
    "            gridwidth=1,\n",
    "            gridcolor=self.grid_color,\n",
    "            linecolor=self.line_color,  # make x axis line visible\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "        figure.update_yaxes(\n",
    "            title=self.yaxis_title,\n",
    "            title_standoff=0,\n",
    "            title_font={'size': self.axis_title_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            tickfont={'size': self.tick_font_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            showgrid=self.show_ygrid,\n",
    "            gridwidth=1,\n",
    "            gridcolor=self.grid_color,\n",
    "            linecolor=self.line_color,  # make y axis line visible\n",
    "            linewidth=2\n",
    "        )\n",
    "        return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, base_path, file_name, remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=None, token_freq_cutoff=None, save_files=False):\n",
    "        self.base_path = base_path\n",
    "        self.file_name = file_name\n",
    "        self.vocab = set()\n",
    "        self.max_block_size = 0\n",
    "        self.block_sizes = []\n",
    "        self.token_to_freq = {}\n",
    "        self.remove_isotopes = remove_isotopes\n",
    "        self.remove_non_bio_friendly = remove_non_bio_friendly\n",
    "        self.save_files = save_files\n",
    "        self.plot_suffix = ''\n",
    "        self.token_len_cutoff = token_len_cutoff\n",
    "        self.token_freq_cutoff = token_freq_cutoff\n",
    "        if self.remove_isotopes:\n",
    "            with open(f\"{self.base_path}exceptions/isotope_exceptions.yaml\", 'r') as f:\n",
    "                self.isotope_exceptions = set(yaml.safe_load(f))\n",
    "            self.plot_suffix += '_no_isotopes'\n",
    "        if self.remove_non_bio_friendly:\n",
    "            with open(f\"{self.base_path}exceptions/non_bio_friendly_exceptions.yaml\", 'r') as f:\n",
    "                self.non_bio_friendly_exceptions = set(yaml.safe_load(f))\n",
    "            self.plot_suffix += '_no_nonbio_friendly'\n",
    "        with open(f\"{self.base_path}exceptions/guacamole_exceptions.yaml\", 'r') as f:\n",
    "            self.guacamole_exceptions = set(yaml.safe_load(f))\n",
    "\n",
    "    def load_and_analyze(self):\n",
    "        self.df = pd.read_csv(f\"{self.base_path}datasets/converted/{self.file_name}.csv.gz\")\n",
    "        \n",
    "        processed_smiles = set()\n",
    "        smile_crumbles = []\n",
    "\n",
    "        pbar_desc = 'Analyzing ' + self.file_name\n",
    "        if self.remove_isotopes:\n",
    "            pbar_desc += ' (no isotopes)'\n",
    "            self.smiles_with_isotopes = set()\n",
    "        if self.remove_non_bio_friendly:\n",
    "            pbar_desc += ' (no non bio friendly)'\n",
    "            self.smiles_with_non_bio_friendly = set()\n",
    "        self.smiles_guac_filter = set()\n",
    "\n",
    "        pbar = tqdm(self.df['smiles'].values, total=len(self.df['smiles'].values), desc=pbar_desc)\n",
    "        for smile in pbar:\n",
    "            if not isinstance(smile, str):\n",
    "                print(f\"Found {smile=} which has {type(smile)=}\")\n",
    "                continue\n",
    "            tokens = regex.findall(smile.strip())\n",
    "\n",
    "            # if any([token.replace('[', '').replace(']', '').replace('-', '').replace('+', '') in self.guacamole_exceptions for token in tokens]): \n",
    "                # self.smiles_guac_filter.add(smile)\n",
    "                # continue\n",
    "            if self.remove_isotopes and any([token in self.isotope_exceptions for token in tokens]):\n",
    "                self.smiles_with_isotopes.add(smile)\n",
    "                continue\n",
    "            if self.remove_non_bio_friendly and any([token in self.non_bio_friendly_exceptions for token in tokens]):\n",
    "                self.smiles_with_non_bio_friendly.add(smile)\n",
    "                continue\n",
    "            if self.token_len_cutoff is not None and len(tokens) > self.token_len_cutoff:\n",
    "                continue\n",
    "\n",
    "            processed_smiles.add(smile)\n",
    "            for token in tokens:\n",
    "                self.vocab.add(token)\n",
    "                if token not in self.token_to_freq:\n",
    "                    self.token_to_freq[token] = 0\n",
    "                self.token_to_freq[token] += 1\n",
    "\n",
    "            self.max_block_size = max(self.max_block_size, len(tokens))\n",
    "            self.block_sizes.append(len(tokens))\n",
    "        \n",
    "        if self.save_files: pd.DataFrame({\"smiles\": list(processed_smiles)}).to_csv(f\"{self.base_path}datasets/converted/{self.file_name}_processed.csv.gz\", compression='gzip')\n",
    "        self.processed_smiles = processed_smiles\n",
    "        if self.token_freq_cutoff is not None:\n",
    "            self._remove_rare_tokens()\n",
    "\n",
    "    def _remove_rare_tokens(self):\n",
    "        new_smiles = set()\n",
    "        self.max_block_size = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "        pbar = tqdm(self.processed_smiles, total=len(self.processed_smiles), desc=f\"Removing rare tokens\")\n",
    "        for smile in pbar:\n",
    "            tokens = regex.findall(smile.strip())\n",
    "            if any([self.token_to_freq[token] < self.token_freq_cutoff for token in tokens]): continue\n",
    "            new_smiles.add(smile)\n",
    "            for token in tokens:\n",
    "                self.vocab.add(token)\n",
    "            self.max_block_size = max(self.max_block_size, len(tokens))\n",
    "        self.no_rare = new_smiles\n",
    "        pd.DataFrame({\"smiles\": list(new_smiles)}).to_csv(f\"{self.base_path}datasets/converted/{self.file_name}_processed_freq{self.token_freq_cutoff}_block{self.token_len_cutoff}.csv.gz\", compression='gzip')\n",
    "\n",
    "    def plot_distribution(self, block_sizes):\n",
    "        graph = Graph()\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x=block_sizes, name=self.file_name, histnorm='probability density'))\n",
    "        fig.update_layout(barmode='overlay', xaxis=dict(dtick=20))\n",
    "        fig.update_traces(opacity=0.75)\n",
    "        percentiles = [25, 50, 75, 95, 99, 99.9, 99.99]\n",
    "        for i, percentile in enumerate(percentiles):\n",
    "            val = np.percentile(block_sizes, percentile)\n",
    "            y_coord = 0.5 if i < len(percentiles) / 2 else 0\n",
    "            y_anno = -40 * (1 + i % 5) #-30 if i % 2 == 0 else -50 #-10 if i < len(percentiles) / 2 else -30\n",
    "            fig.add_annotation(x=val, y=0, text=f'{percentile} percentile<br>{val:.0f} blocks',\n",
    "                showarrow=True, arrowhead=1, ax=-10, ay=y_anno)\n",
    "        stats_text = f\"Total smiles: {len(block_sizes)}<br>Vocab Size: {len(self.vocab)}<br>Max Block Size: {self.max_block_size}\"\n",
    "        guac_fraction = len(self.smiles_guac_filter) / len(self.df[\"smiles\"])\n",
    "        stats_text += f\"<br># excluded (guacamole): {len(self.smiles_guac_filter)} or {guac_fraction:.2%}\"\n",
    "        \n",
    "        if self.remove_isotopes:\n",
    "            isotope_fraction = len(self.smiles_with_isotopes) / len(self.df[\"smiles\"])\n",
    "            stats_text += f\"<br># excluded (isotope-containing): {len(self.smiles_with_isotopes)} or {isotope_fraction:.2%}\"\n",
    "        if self.remove_non_bio_friendly:\n",
    "            non_bio_friendly_fraction = len(self.smiles_with_non_bio_friendly) / len(self.df[\"smiles\"])\n",
    "            stats_text += f\"<br># excluded (non bio friendly): {len(self.smiles_with_non_bio_friendly)} or {non_bio_friendly_fraction:.2%}\"\n",
    "\n",
    "        fig.add_annotation(x=0.75, y=1, text=stats_text,\n",
    "                            showarrow=False, xref='paper', yref='paper', xanchor='left', yanchor='top', font=dict(size=12))\n",
    "        graph.update_parameters({'title': f'Block size distribution for {self.file_name} partition. Isotopes removed: {self.remove_isotopes}',\n",
    "                                'xaxis_title': 'Block size', 'yaxis_title': 'Probability density',\n",
    "                                'width': 1280, 'height': 720, 'show_xgrid': True})\n",
    "        graph.style_figure(fig)\n",
    "        # fig.write_html(self.base_path + f'plots/block_size_distribution_{self.file_name}{self.plot_suffix}.html', include_plotlyjs='cdn')\n",
    "        fig.write_html(self.base_path + f'plots/html/block_size_distribution/{self.file_name}.html', include_plotlyjs='cdn')\n",
    "        fig.write_image(self.base_path + f'plots/jpg/block_size_distribution/{self.file_name}.jpg', scale=3.0)\n",
    "        fig.write_image(self.base_path + f'plots/svg/block_size_distribution/{self.file_name}.svg')\n",
    "\n",
    "    def plot_token_frequencies(self, top_n=20):\n",
    "        sorted_token_freq = dict(sorted(self.token_to_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "        sorted_token_freq = dict(list(sorted_token_freq.items())[:top_n])\n",
    "        \n",
    "        graph = Graph()\n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=list(sorted_token_freq.keys()), \n",
    "                y=list(sorted_token_freq.values()), \n",
    "                name=self.file_name)\n",
    "            ])\n",
    "        graph.update_parameters({'title': f'Top {top_n} character frequencies for {self.file_name} partition. Isotopes removed: {self.remove_isotopes}',\n",
    "                                'xaxis_title': 'Token', 'yaxis_title': 'Frequency',\n",
    "                                'width': 1280, 'height': 720})\n",
    "\n",
    "        graph.style_figure(fig)\n",
    "        # fig.write_html(self.base_path + f'plots/token_frequencies_{self.file_name}{self.plot_suffix}.html', include_plotlyjs='cdn')\n",
    "        fig.write_html(self.base_path + f'plots/html/token_frequencies/{self.file_name}.html', include_plotlyjs='cdn')\n",
    "        fig.write_image(self.base_path + f'plots/jpg/token_frequencies/{self.file_name}.jpg', scale=3.0)\n",
    "        fig.write_image(self.base_path + f'plots/svg/token_frequencies/{self.file_name}.svg')\n",
    "    \n",
    "    def export_vocab(self):\n",
    "        with open(self.base_path + f'vocab_{self.file_name}.txt', 'w') as f:\n",
    "            for token in self.vocab:\n",
    "                f.write(token + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    # dict(file_name='combined', remove_isotopes=True, remove_non_bio_friendly=True, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    # dict(file_name='moses_train', remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    # dict(file_name='bindingdb', remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    # dict(file_name='chembl', remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    # dict(file_name='guacamol_v1_all', remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    # dict(file_name='combined', remove_isotopes=True, remove_non_bio_friendly=True, token_len_cutoff=133, token_freq_cutoff=100, save_files=False),\n",
    "    # dict(file_name='combined', remove_isotopes=True, remove_non_bio_friendly=True, token_len_cutoff=133, token_freq_cutoff=1000, save_files=False),\n",
    "    # dict(file_name='combined_processed_freq100_block133', remove_isotopes=True, remove_non_bio_friendly=True, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    # dict(file_name='combined_processed_freq1000_block133', remove_isotopes=True, remove_non_bio_friendly=True, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    # dict(file_name='combined', remove_isotopes=True, remove_non_bio_friendly=True, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "    dict(file_name='combined', remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=133, token_freq_cutoff=1000, save_files=False),\n",
    "    # dict(file_name='combined', remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=None, token_freq_cutoff=1000, save_files=False),\n",
    "    # dict(file_name='combined_processed_freq', remove_isotopes=False, remove_non_bio_friendly=False, token_len_cutoff=None, token_freq_cutoff=None, save_files=False),\n",
    "]\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    ds = Dataset(PRETRAINING_PATH, **config)\n",
    "    ds.load_and_analyze()\n",
    "    ds.plot_distribution(ds.block_sizes)\n",
    "    ds.plot_token_frequencies(top_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.token_to_freq['[Na+]'], ds.token_to_freq['[K+]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sandbox(pretrain_path):\n",
    "    with open(f\"{pretrain_path}exceptions/non_bio_friendly_exceptions.yaml\", \"r\") as f:\n",
    "        nonbio = set(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "    with open(f\"{pretrain_path}exceptions/isotope_exceptions.yaml\", \"r\") as f:\n",
    "        isotope = set(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "    with open(f\"{pretrain_path}dataset_descriptors/combined_processed_freq1000_block133.yaml\", \"r\") as f:\n",
    "        final = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    with open(f\"{pretrain_path}dataset_descriptors/combined_processed_freq1000_block133_nonan.yaml\", \"r\") as f:\n",
    "        final_nonan = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    print(f\"{len(nonbio)=}, {len(isotope)=}, {len(nonbio & isotope)=}\")\n",
    "    print((nonbio | isotope) & ds.vocab)\n",
    "    atoms = {\"Ag\",\"Al\",\"Am\",\"Ar\",\"At\",\"Au\",\"D\",\"E\",\"Fe\",\"G\",\"K\",\"L\",\"M\",\"Ra\",\"Re\",\"Rf\",\"Rg\",\"Rh\",\"Ru\",\"T\",\"U\",\"V\",\"W\",\"Xe\",\"Y\",\"Zr\",\"a\",\"d\",\"f\",\"g\",\"h\",\"k\",\"m\",\"si\",\"t\",\"te\",\"u\",\"v\",\"y\",}\n",
    "    print(ds.vocab - set(final['stoi'].keys()))\n",
    "    print(ds.vocab - set(final_nonan['stoi'].keys()))\n",
    "\n",
    "sandbox(PRETRAINING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the token_to_freq dictionary by frequency in reversed order\n",
    "sorted_token_freq = dict(sorted(combined_db.token_to_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_partition(pretrain_path, file_name, validation_fraction=0.05):\n",
    "    all_mols = pd.read_csv(f\"{pretrain_path}datasets/converted/{file_name}.csv.gz\", compression='gzip')['smiles'].to_numpy()\n",
    "    np.random.shuffle(all_mols)\n",
    "    validation_index = int((1-validation_fraction)*len(all_mols))\n",
    "    print(f\"{validation_index=}\")\n",
    "    train_mols = all_mols[:validation_index]\n",
    "    val_mols = all_mols[validation_index:]\n",
    "    pd.DataFrame({'smiles': train_mols}).to_csv(f\"{pretrain_path}datasets/splits/{file_name}_train.csv.gz\", compression='gzip')\n",
    "    pd.DataFrame({'smiles': val_mols}).to_csv(f\"{pretrain_path}datasets/splits/{file_name}_val.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_train_val_partition(PRETRAINING_PATH, 'combined_processed_freq100_block133')\n",
    "create_train_val_partition(PRETRAINING_PATH, 'combined_processed_freq1000_block133_nonan')\n",
    "# create_train_val_partition(PRETRAINING_PATH, 'combined_processed_freq1000_block133')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_combined_metrics(pretrain_path, file_name):\n",
    "    train_df = pd.read_csv(f\"{pretrain_path}datasets/splits/{file_name}_train.csv.gz\")\n",
    "    val_df = pd.read_csv(f\"{pretrain_path}datasets/splits/{file_name}_val.csv.gz\")\n",
    "    with open(f\"{pretrain_path}dataset_metrics/{file_name}_metrics.yaml\", 'w') as f:\n",
    "        yaml.dump({\"total\": len(train_df)+len(val_df), \"training partition\": len(train_df), \"validation partition\": len(val_df) }, f)\n",
    "    \n",
    "\n",
    "# export_combined_metrics(PRETRAINING_PATH, 'combined_processed_freq100_block133')\n",
    "export_combined_metrics(PRETRAINING_PATH, 'combined_processed_freq1000_block133_nonan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
