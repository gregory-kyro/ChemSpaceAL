{"cells":[{"cell_type":"markdown","metadata":{"id":"EqWhU3QReHky"},"source":["# Set up the notebook"]},{"cell_type":"code","execution_count":115,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3958,"status":"ok","timestamp":1690216899725,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"QCoUctW3K7-Y","outputId":"2578c062-6eba-4c5f-ca91-c1b82bbcae85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Based on the selected mode (Active Learning) the following values will be used in CONFIG_DICT {'lr_warmup': False, 'lr_decay': True, 'epochs': 10, 'learning_rate': 3e-05} \n","Don't forget to reinitialize CONFIG_DICT!\n"]}],"source":["# @title Setting up variables\n","job_config = {}\n","# BASE_PATH = '/content/drive/MyDrive/Generative_ML/current_data/' #@param {type:\"string\"}\n","BASE_PATH = '/Users/morgunov/batista/Summer/pipeline/'\n","PROTEIN_NAME = 'HNH.pdb' # @param {type:\"string\"}\n","RANDOM_SEED = 42\n","MODE = 'Active Learning' #@param [\"Pretraining\", \"Active Learning\"]\n","# @markdown select the name of the smiles-containing column in the training dataframe. use CAPS if pretraining on MOSES\n","SMILES_KEY = \"smiles\" #@param [\"smiles\", \"SMILES\"]\n","# @markdown following variables are only relevant when doing pretraining\n","TRAIN_FNAME = \"moses_and_binding_no_rare_tokens_train.csv.gz\" #@param [\"moses_and_binding_no_rare_tokens_train.csv.gz\", \"moses_train.csv.gz\"] {allow-input: true}\n","VAL_FNAME = \"moses_and_binding_no_rare_tokens_test.csv.gz\" #@param [\"moses_and_binding_no_rare_tokens_test.csv.gz\", \"moses_test.csv.gz\"] {allow-input: true}\n","TRAIN_CKPT_NAME = 'model1_softsub_al2' # @param {type:\"string\"}\n","CURRENT_CYCLE_PREFIX = \"model1_softsub_al2\" #@param {type:\"string\"}\n","\n","if MODE == 'Pretraining':\n","    job_config.update({'lr_warmup': True, 'lr_decay': True, 'epochs': 30, 'learning_rate': 3e-4,})\n","elif MODE == 'Active Learning':\n","    job_config.update({'lr_warmup': False, 'lr_decay': True, 'epochs': 10, 'learning_rate': 3e-5,})\n","else:\n","    raise KeyError(f'requested {MODE} but only Pretraining and Active Learning are supported')\n","print(f'Based on the selected mode ({MODE}) the following values will be used in CONFIG_DICT', job_config, \"\\nDon't forget to reinitialize CONFIG_DICT!\")\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":116,"metadata":{"cellView":"form","id":"5WVLrvpbdpiQ"},"outputs":[],"source":["#@title Set up paths\n","import torch\n","import os\n","PRETRAINING_PATH = BASE_PATH + '1. Pretraining/'\n","GENERATION_PATH = BASE_PATH + '2. Generation/'\n","SAMPLING_PATH = BASE_PATH + '3. Sampling/'\n","DIFFDOCK_PATH = BASE_PATH + '4. DiffDock/'\n","SCORING_PATH = BASE_PATH + '5. Scoring/'\n","AL_PATH = BASE_PATH + '6. ActiveLearning/'\n","\n","PROTEIN_PATH = DIFFDOCK_PATH + 'proteins/' + PROTEIN_NAME\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n","REGEX_PATTERN = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|@@|\\?|>|!|~|\\*|\\$|\\%[0-9]{2}|[0-9])\""]},{"cell_type":"code","execution_count":117,"metadata":{"cellView":"form","id":"QmNIhQEJK7-b"},"outputs":[],"source":["#@title initialize CONFIG_DICT\n","CONFIG_DICT = {\n","    \"device\": DEVICE,\n","    \"att_bias\": False,\n","    \"gpt_bias\": True,\n","    \"att_drop_rate\": 0.1,\n","    \"gpt_drop_rate\": 0.1,\n","    \"n_layer\": 8,\n","    \"n_head\": 8,\n","    \"n_embed\": 256,\n","    \"ff_mult\": 4, # multiplier for Feed Forward number of hidden units inside multihead,\n","    \"doGELU\": True, # else ReLU\n","    \"attention_times\": [],\n","    \"do_flash\": True,\n","    \"smiles_key\": SMILES_KEY,\n","    \"wandb_project\": f'BetaPipeline',\n","    \"slice_data\": False, #False for all data\n","    \"batch_size\": 512, #512,\n","    \"betas\": (0.965, 0.99), #(0.9, 0.95)\n","    \"rho\": 0.04, # For SophiaG\n","    \"weight_decay\": 0.1,\n","    \"num_workers\": 0 # number of worker processes to use for loading data\n","}\n","CONFIG_DICT.update(job_config)"]},{"cell_type":"code","execution_count":118,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690216903688,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"wENHAYSOnHOC","outputId":"c1822cb3-81be-4fae-b660-8f6db9d25d28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model will be trained on\n"," 1. Pretraining/datasets/moses_and_binding_no_rare_tokens_train.csv.gz\n","... and validated on\n"," 1. Pretraining/datasets/moses_and_binding_no_rare_tokens_test.csv.gz\n","... dataset descriptors will be saved to\n"," 6. ActiveLearning/dataset_descriptors/model1_softsub_al1_threshold11_softmax_sub.yaml\n","... model weights will be saved to\n"," 6. ActiveLearning/model_weights/model1_softsub_al2.pt\n","... wandb run will be named model1_softsub_al2\n"]}],"source":["#@title Run this cell to ensure the variables below are set the way you want them to be!\n","pretrain_config = {\n","    \"train_path\": f\"{PRETRAINING_PATH}datasets/{TRAIN_FNAME}\",\n","    \"val_path\": f\"{PRETRAINING_PATH}datasets/{VAL_FNAME}\",\n","    \"wandb_runname\": TRAIN_CKPT_NAME,\n","    }\n","\n","# @markdown the following setting is relevant only when loading AL descriptors\n","DATASET_DESC_FNAME = \"model1_softsub_al1_threshold11_softmax_sub\" #@param [\"model1_baseline_threshold11_softmax_sub\", \"model1_softsub_al1_threshold11_softmax_sub\"]\n","\n","if MODE == 'Pretraining':\n","    pretrain_config.update({\n","        \"save_ckpt_path\": f\"{PRETRAINING_PATH}model_weights/{TRAIN_CKPT_NAME}.pt\",\n","        \"desc_path\": f\"{PRETRAINING_PATH}dataset_descriptors/{TRAIN_FNAME.split('.')[0][:-6]}.yaml\",\n","        })\n","elif MODE == 'Active Learning':\n","    pretrain_config.update({\n","        \"save_ckpt_path\": f\"{AL_PATH}model_weights/{TRAIN_CKPT_NAME}.pt\",\n","        \"desc_path\": f\"{AL_PATH}dataset_descriptors/{DATASET_DESC_FNAME}.yaml\"\n","        })\n","\n","\n","print('Model will be trained on\\n', '/'.join(pretrain_config['train_path'].split('/')[6:]))\n","print('... and validated on\\n', '/'.join(pretrain_config['val_path'].split('/')[6:]))\n","print('... dataset descriptors will be saved to\\n', '/'.join(pretrain_config['desc_path'].split('/')[6:]))\n","print('... model weights will be saved to\\n', '/'.join(pretrain_config['save_ckpt_path'].split('/')[6:]))\n","print('... wandb run will be named', TRAIN_CKPT_NAME)\n","CONFIG_DICT.update(pretrain_config)"]},{"cell_type":"markdown","metadata":{"id":"94C4_8BipUaL"},"source":["# GPT"]},{"cell_type":"markdown","metadata":{"id":"jzRj6bGQpUaM"},"source":["## Imports & Installations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18833,"status":"ok","timestamp":1690045064396,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"QoqxuL44K7-d","outputId":"d5e81d91-3d56-444b-ce9e-c11d1ea37b68"},"outputs":[],"source":["# install necessary packages\n","!pip install rdkit\n","!pip install pandas==1.5.3\n","!pip install wandb\n","\n","# clone Sophia optimizer GitHub repository\n","!git clone https://github.com/Liuhong99/Sophia.git"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zVkT-uNwAUsD"},"outputs":[],"source":["# import necessary packages\n","import numpy as np\n","import os\n","import re\n","import logging\n","import wandb\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","import torch.optim as optim\n","from torch.cuda.amp import GradScaler\n","from tqdm import tqdm\n","from rdkit import Chem\n","from Sophia.sophia import SophiaG\n","import yaml\n","import pandas as pd\n","\n","# set random seed for reproducibility\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed_all(RANDOM_SEED)"]},{"cell_type":"markdown","metadata":{"id":"7pfZIi6qpUaQ"},"source":["## Definitions"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ceh8AwjpK7-e"},"outputs":[],"source":["class GPTConfig:\n","    def __init__(self, vocab_size=None, block_size=None, **kwargs):\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        for k,v in kwargs.items():\n","            setattr(self, k, v)\n","\n","    def export_attributes(self, export_path):\n","        with open(export_path, 'w') as f:\n","            yaml.dump(vars(self), f)\n","\n","    def load_attributes(self, load_path):\n","        with open(load_path, 'r') as f:\n","            config_dict = yaml.load(f, Loader=yaml.SafeLoader)\n","        self.__dict__.update(config_dict)"]},{"cell_type":"markdown","metadata":{"id":"78g7xtxZK7-e"},"source":["### Dataset loading & sampling"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"3MhywaNlpUaQ"},"outputs":[],"source":["@torch.no_grad()\n","def sample(model, x, steps, temperature=1.0):\n","    block_size = model.get_block_size() # define size of context window used for input conditioning\n","    model.eval()\n","    for k in range(steps):\n","        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # limit conditioning input to the most recent block_size elements\n","        logits, _= model(x_cond) # give input to model and get logits (unnormalized scores or probabilities)\n","        logits = logits[:, -1, :] / temperature # extract the logits for the next token in the sequence\n","        probs = F.softmax(logits, dim=-1)\n","        ix = torch.multinomial(probs, 1)\n","        x = torch.cat((x, ix), dim=1) # concatenate the chosen token index with the existing sequence\n","    return x\n","\n","def check_novelty(gen_smiles, train_smiles):\n","    if len(gen_smiles) == 0:\n","        novel_ratio = 0\n","    else:\n","        duplicates = [1 for mol in gen_smiles if mol in train_smiles]\n","        novel = len(gen_smiles) - sum(duplicates)\n","        novel_ratio = novel*100/len(gen_smiles)\n","    return novel_ratio\n","\n","def canonic_smiles(smiles_or_mol):\n","    mol = get_mol(smiles_or_mol)\n","    if mol is None:\n","        return None\n","    return Chem.MolToSmiles(mol)\n","\n","\n","class SMILESDataset(Dataset):\n","\n","    def __init__(self, data=None, chars=None, block_size=None, len_data = None):\n","        if chars is None:\n","            self.desc_only = True\n","            return\n","        self.desc_only = False\n","        self.vocab = set(chars)\n","        self.vocab_size = len(chars)\n","        self.stoi = {ch:i for i,ch in enumerate(chars)}\n","        # self.stoi['<'] = -100\n","        self.itos = {i:s for s,i in self.stoi.items()}\n","        # self.itos = {i:ch for i,ch in enumerate(chars)}\n","\n","        self.block_size = block_size\n","        self.data = data\n","        self.len_data = len_data\n","\n","    def export_desc_attributes(self, export_path):\n","        attr_dict = {\n","            \"desc_only\": self.desc_only,\n","            \"vocab_size\": self.vocab_size,\n","            \"block_size\": self.block_size,\n","            \"stoi\": self.stoi,\n","            \"itos\": self.itos,\n","            \"len_data\": self.len_data\n","        }\n","        with open(export_path, 'w') as f:\n","            yaml.dump(attr_dict, f)\n","\n","    def load_desc_attributes(self, load_path):\n","        with open(load_path, 'r') as f:\n","            attr_dict = yaml.load(f, Loader=yaml.SafeLoader)\n","        self.__dict__.update(attr_dict)\n","\n","    def __len__(self):\n","        assert not self.desc_only, \"Dataset is not initialized\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        assert not self.desc_only, \"Dataset is not initialized\"\n","        smiles = self.data[idx].strip()\n","        # define regular expressin pattern used to identify characters in the SMILES strings\n","        regex = re.compile(REGEX_PATTERN)\n","        smiles_matches = regex.findall(smiles)\n","\n","        # smiles = str('!') + smiles\n","        # smiles += str('<')*(self.block_size - len(smiles_matches)) # pad SMILES string by appending '<' to the end until self.block_size is achieved\n","\n","        if len(smiles_matches) > self.block_size+1: # if the number of matches found by the regular expression pattern applied to the SMILES string exceeds self.block_size:\n","            smiles = smiles[:self.block_size+1]\n","\n","        embedded_smile = [self.stoi[s] for s in smiles_matches]\n","        x = torch.tensor(embedded_smile[:-1], dtype=torch.long)\n","        y = torch.tensor(embedded_smile[1:], dtype=torch.long)\n","        return x, y\n"]},{"cell_type":"markdown","metadata":{"id":"iTFNFFNGpUaR"},"source":["### Model"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"7QbYJpQ3pUaR"},"outputs":[],"source":["class SelfAttention(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embed % config.n_head == 0\n","        self.config = config\n","\n","        self.query = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n","        self.key = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n","        self.value = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n","\n","        self.attn_drop = nn.Dropout(config.att_drop_rate)\n","        self.resid_drop = nn.Dropout(config.att_drop_rate)\n","\n","        self.proj = nn.Linear(config.n_embed, config.n_embed)\n","        self.n_head = config.n_head\n","\n","        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n","                                .view(1, 1, config.block_size, config.block_size))\n","\n","    def forward(self, x, layer_past=None):\n","        B, T, C = x.size()\n","        # apply attention functions to get tensors with dimensions (B, n_head, T, head_size)\n","        q = self.query(x).view(B, T, self.n_head, C // self.n_head)\n","        k = self.key(x).view(B, T, self.n_head, C // self.n_head)\n","        v = self.value(x).view(B, T, self.n_head, C // self.n_head)\n","        if self.config.do_flash:\n","            q = q.transpose(1, 2)\n","            k = k.transpose(1, 2)\n","            v = v.transpose(1, 2)\n","            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.config.att_drop_rate if self.training else 0, is_causal=True)\n","            y = y.transpose(1, 2)\n","        else:\n","            # (B h T s) @ (B h s T) -> (B h T T)\n","            att = torch.einsum('bths,bihs->bhti', q, k) / np.sqrt(k.size(-1))\n","            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            # (B h T T) @ (B h T s) -> (B h T s)\n","            y = torch.einsum('bhtq,bqhs->bths', att, v)\n","            self.att_weights = att\n","        self.attended = y\n","        y = y.contiguous().view(B, T, C)\n","        y = self.resid_drop(self.proj(y))\n","        self.out = y\n","        return y\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(config.n_embed)\n","        self.ln2 = nn.LayerNorm(config.n_embed)\n","        self.attn = SelfAttention(config)\n","        self.mlp = nn.Sequential(nn.Linear(config.n_embed, config.ff_mult*config.n_embed), nn.GELU() if config.doGELU else nn.ReLU(),\n","            nn.Linear(config.ff_mult*config.n_embed, config.n_embed), nn.Dropout(config.att_drop_rate))\n","\n","    def forward(self, x):\n","        y = self.attn(self.ln1(x))\n","        x = x + y # perform a residual connection by summing input and attention output\n","        x = x + self.mlp(self.ln2(x)) # apply layer normalization and then MLP, create a residual connection with input\n","        return x\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embed)\n","        self.type_emb = nn.Embedding(2, config.n_embed)\n","        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embed))\n","\n","        self.drop = nn.Dropout(config.gpt_drop_rate)\n","        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n","\n","        self.ln_f = nn.LayerNorm(config.n_embed)\n","        self.head = nn.Linear(config.n_embed, config.vocab_size, bias=config.gpt_bias)\n","        self.block_size = config.block_size # define the context size\n","        self.apply(self._init_weights) # initialize weights and apply to all relevant modules in the model\n","\n","    def get_block_size(self):\n","        return self.block_size\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            module.weight.data.normal_(mean=0.0, std=0.02)\n","            if isinstance(module, nn.Linear) and module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def configure_optimizers(self, train_config):\n","        decay, no_decay = set(), set()\n","        no_decay = set()\n","\n","        whitelist_weight_modules = (torch.nn.Linear)\n","        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n","        # for named module of the model:\n","        for mn, m in self.named_modules():\n","            # for named parameter of each module:\n","            for pn, p in m.named_parameters():\n","                # construct full parameter name by concatenating module name and parameter name, separated by a dot\n","                fpn = '%s.%s' % (mn, pn) if mn else pn\n","                if pn.endswith('bias') or ('bias' in pn):\n","                    no_decay.add(fpn)\n","                elif (pn.endswith('weight') or ('weight' in pn)) and isinstance(m, whitelist_weight_modules):\n","                    decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n","                    no_decay.add(fpn)\n","        no_decay.add('pos_emb')\n","        param_dict = {pn:p for pn, p in self.named_parameters()}\n","        assert len(decay & no_decay) == 0\n","        # assert that all parameters from both sets have been correctly separated\n","        assert len(param_dict.keys() - (decay | no_decay)) == 0\n","        optim_groups = [{\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n","                        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}]\n","        optimizer = SophiaG(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, rho=train_config.rho, weight_decay=train_config.weight_decay)\n","        return optimizer\n","\n","    def forward(self, idx, targets=None, prop = None, scaffold = None):\n","        b, t = idx.size()\n","\n","        assert t <= self.block_size\n","\n","        token_embeddings = self.tok_emb(idx) # pass input tensor through token embedding layer\n","        # select a subset of the position embedding matrix based on the length of the input sequence\n","        position_embeddings = self.pos_emb[:, :t, :]\n","        # pass a tensor of ones of shape (b, t) through the type embedding layer,\n","        # maps a binary type indicator to a learnable embedding vector, all type indicators\n","        # are set to 1, indicating same type for all tokens in input sequence\n","        type_embeddings = self.type_emb(torch.ones((b,t), dtype=torch.long, device=idx.device))\n","        x = self.drop(token_embeddings + position_embeddings + type_embeddings)\n","\n","        for layer in self.blocks:\n","            x = layer(x)\n","        x = self.ln_f(x)\n","        logits = self.head(x)\n","        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.view(-1), ignore_index=self.config.loss_ignore_index) if targets is not None else None\n","        # loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n","        return logits, loss"]},{"cell_type":"markdown","metadata":{"id":"zWbFS84KpUaS"},"source":["### Training"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"mrnqKEZwpUaS"},"outputs":[],"source":["class Trainer:\n","\n","    def __init__(self, model, train_dataset, test_dataset=None):\n","        self.model = model\n","        self.train_dataset = train_dataset\n","        self.test_dataset = test_dataset\n","        self.config = model.config\n","        self.stoi = train_dataset.stoi\n","        self.itos = train_dataset.itos\n","\n","    def train(self, wandb):\n","        model, config = self.model, self.config\n","        optimizer = model.configure_optimizers(config)\n","        scaler = GradScaler() # define variable used for gradient scaling in mixed-precision training\n","        self.tokens = 0 # initialize a counter used for learning rate decay\n","\n","        def run_epoch(split):\n","            is_train = split == 'train'\n","            model.train(is_train)\n","            data = self.train_dataset if is_train else self.test_dataset\n","            loader = DataLoader(data, shuffle=True, pin_memory=True, batch_size=config.batch_size, num_workers=config.num_workers)\n","            losses = []\n","            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n","            # for batch index, batch in progress bar:\n","            for it, (x, y) in pbar:\n","                # move the input data tensor, target data tensor, property tensor, and scaffold tensor to GPU\n","                x, y = x.to(config.device), y.to(config.device)\n","                # allow model to use lower-precision computations for improved memory usage\n","                if config.device == 'cuda':\n","                    with torch.cuda.amp.autocast():\n","                        with torch.set_grad_enabled(is_train):\n","                            logits, loss = model(x, y)\n","                            loss = loss.mean()\n","                            losses.append(loss.item())\n","                else:\n","                    with torch.cpu.amp.autocast():\n","                        with torch.set_grad_enabled(is_train):\n","                            logits, loss = model(x, y)\n","                            loss = loss.mean()\n","                            losses.append(loss.item())\n","\n","                if is_train:\n","                    model.zero_grad()\n","                    scaler.scale(loss).backward()\n","                    scaler.unscale_(optimizer) # unscale the gradients of the optimizer's parameters to their original values\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients of model parameters to prevent them from exploding, setting maximum gradient norm to be 1.0\n","                    scaler.step(optimizer) # update the optimizer's parameters based on calculated gradients\n","                    scaler.update() # update the scale factor of the gradient scaler\n","                    if config.lr_decay:\n","                        self.tokens += (y >= 0).sum() # increment the number of processed tokens by the count of valid tokens (not padding or special tokens)\n","                        if config.lr_warmup and self.tokens < config.warmup_tokens:\n","                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens)) # perform a linear warm-up\n","                        else:\n","                            baseline = config.warmup_tokens if config.lr_warmup else 0\n","                            # calculate the progress of training in terms of the number of tokens processed\n","                            progress = float(self.tokens - baseline) / float(max(1, config.final_tokens - baseline))\n","                            # calculate the scaling factor for the learning rate (between 0.1 and 1.0)\n","                            # to gradually reduce learning rate as training progresses\n","                            lr_mult = max(0.1, 0.5 * (1.0 + np.cos(np.pi * progress)))\n","                        lr = config.learning_rate * lr_mult # multiply the base learning rate by the scaling factor to obtain the updated learning rate\n","                        for param_group in optimizer.param_groups:\n","                            param_group['lr'] = lr\n","                    else:\n","                        lr = config.learning_rate\n","                    if wandb is not None: # log training progress using Weights & Biases\n","                        wandb.log({'step_train_loss': loss, 'train_step': it + epoch*len(loader), 'learning_rate': lr})\n","                    # update the description of the progress bar with epoch, iteration, and training loss\n","                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n","            return float(np.mean(losses))\n","\n","        # initialize best loss as infinity\n","        best_loss = float('inf')\n","        for epoch in range(config.epochs):\n","            print(f'{epoch=}')\n","            train_loss = run_epoch('train')\n","            log_dict = {'epoch_train_loss': train_loss, 'epoch': epoch + 1}\n","            if self.test_dataset is not None:\n","                test_loss = run_epoch('test')\n","                log_dict['epoch_valid_loss'] = test_loss\n","            if wandb is not None:\n","                wandb.log(log_dict)\n","            good_model = False\n","            if self.test_dataset is None:\n","                good_model = True\n","            else:\n","                if test_loss < best_loss:\n","                    best_loss = test_loss\n","                    good_model = True\n","            if good_model:\n","                torch.save(self.model.state_dict(), self.config.save_ckpt_path)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"OuGEvPXDpUaT"},"outputs":[],"source":["def load_data(config_dict, mode='pretrain', forced_block_size=None):\n","    if mode == 'pretrain':\n","        if 'gz' in config_dict[\"train_path\"]:\n","            compression = 'gzip'\n","        else:\n","            compression = None\n","        if (cut:=config_dict[\"slice_data\"]):\n","            train_data = pd.read_csv(config_dict[\"train_path\"], compression=compression)[:cut]\n","            val_data = pd.read_csv(config_dict[\"val_path\"], compression=compression)[:cut]\n","        else:\n","            train_data = pd.read_csv(config_dict[\"train_path\"], compression=compression)\n","            val_data = pd.read_csv(config_dict[\"val_path\"], compression=compression)\n","        iterators = (train_data[config_dict['smiles_key']].values, val_data[config_dict['smiles_key']].values)\n","        assert len(train_data) == len(train_data[config_dict['smiles_key']].values), \"There's no reason why this shouldn't be true\"\n","    elif mode == 'al':\n","        print(f\"Loading AL dataset from\", '/'.join(config_dict[\"al_path\"].split('/')[6:]))\n","        al_data = pd.read_csv(config_dict[\"al_path\"])\n","        iterators = (al_data[config_dict['smiles_key']].values, )\n","    else:\n","        raise KeyError(f\"Only pretraining and active learning are currently supported\")\n","\n","    # smiles = train_data[config_dict['smiles_key']]\n","    # vsmiles = val_data[config_dict['smiles_key']]\n","\n","    # compile pattern into a regular expression object that can be used for matching operations\n","    regex = re.compile(REGEX_PATTERN)\n","    char_set = {'<', '!', '~'} # context={'<'}\n","    # char_set = {'<', '!'} # context={'<'}\n","\n","    max_len = 0\n","    for iterator in iterators:\n","        for i in iterator:\n","            chars = regex.findall(i.strip())\n","            max_len = max(max_len, len(chars))\n","            for char in chars:\n","                char_set.add(char)\n","\n","    chars = sorted(list(char_set))\n","    max_len += 1    #accounting for the start token, which hasn't been added yet\n","    if forced_block_size is not None:\n","        assert mode == 'al', \"Cannot force a block size in pretraining\"\n","        max_len = forced_block_size\n","\n","    datasets = []\n","    for iterator in iterators:\n","        padded = ['!' + i + '~' + '<'*(max_len - 1 - len(regex.findall(i.strip()))) for i in iterator]\n","        dataset = SMILESDataset(data=padded, chars=chars, block_size=max_len, len_data=len(iterator))\n","        datasets.append(dataset)\n","    dataset.export_desc_attributes(config_dict[\"desc_path\"])\n","    return datasets\n","\n","    # smiles = ['!' + i + '~' + '<'*(max_len - 1 - len(regex.findall(i.strip()))) for i in smiles]\n","    # vsmiles = ['!' + i + '~' + '<'*(max_len - 1 - len(regex.findall(i.strip()))) for i in vsmiles]\n","\n","    # train_dataset = SMILESDataset(data=smiles, chars=chars, block_size=max_len, len_data=len(train_data))\n","    # valid_dataset = SMILESDataset(data=vsmiles, chars=chars, block_size=max_len, len_data=len(val_data))\n","    # train_dataset.export_desc_attributes(config_dict[\"desc_path\"])\n","    # return train_dataset, valid_dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zH-gwID9pUaT"},"outputs":[],"source":["def train_GPT(train_dataset, config_dict, valid_dataset=None, load_ckpt=False):\n","  \"\"\"\n","  OUTPUTS:\n","  1) checkpoint of trained model parameters\n","  2) Weights & Biases logged run\n","  \"\"\"\n","\n","  mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n","                    warmup_tokens=0.1*train_dataset.len_data*train_dataset.block_size,\n","                    final_tokens=config_dict[\"epochs\"]*train_dataset.len_data*train_dataset.block_size,\n","                    loss_ignore_index=train_dataset.stoi['<'],\n","                    **config_dict)\n","  model = GPT(mconf)\n","  if load_ckpt:\n","    model.load_state_dict(torch.load(config_dict['load_ckpt_path']))\n","  model.to(config_dict[\"device\"])\n","  torch.compile(model)\n","  trainer = Trainer(model, train_dataset, valid_dataset)\n","\n","  %env WANDB_EXECUTABLE=python3\n","  wandb.init(project=config_dict[\"wandb_project\"], name=config_dict[\"wandb_runname\"])\n","  trainer.train(wandb=wandb)\n","  return model, trainer, wandb"]},{"cell_type":"markdown","metadata":{"id":"JS3N-2qgK7-h"},"source":["## Pretraining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWOb6fsKpUaU"},"outputs":[],"source":["train_dataset, val_dataset = load_data(CONFIG_DICT) # takes ~1 min\n","print(train_dataset.vocab_size, train_dataset.block_size)\n","# (190, 131) for combined\n","# (26, 55) for MOSES\n","# (27, 55) MOSES + end token\n","# (45, 131) MOSES + bindingdb + end token"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uvCDiyssBbZl"},"outputs":[],"source":["model = train_GPT(\n","                train_dataset = train_dataset,\n","                valid_dataset = val_dataset,\n","                config_dict = CONFIG_DICT\n","        )\n","#GK wandb API Key: c99c9a01523f93287716691fa3360b1f4566e115\n","#RB wandb API Key: 4d3d628c6b5a4b3554c7a89ea50df8a4a6be0f85\n","#AM wandb API key: 5be14d5930441de4707f6a58e4f7c2e229dab1d1"]},{"cell_type":"markdown","metadata":{"id":"PR2QZRicBmed"},"source":["# Inference (Generation)"]},{"cell_type":"code","execution_count":149,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690216905792,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"GfGDEj6prEep","outputId":"bccd8b18-d1e0-4f70-b7ec-9433b539772b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generation will use the following dataset descriptors\n"," 6. ActiveLearning/dataset_descriptors/model1_softsub_al1_threshold11_softmax_sub.yaml\n","... and following model weights\n"," 6. ActiveLearning/model_weights/model1_softsub_al2.pt\n","... and molecules will be saved to\n"," 2. Generation/smiles/model1_softsub_al2_temp1.0_processed.csv\n","... already scored molecules will be read from\n"," ['5. Scoring/scored_dataframes/model1_baseline.csv', '5. Scoring/scored_dataframes/model1_softsub_al1.csv']\n","... AL training sets will be read from\n"," ['6. ActiveLearning/training_sets/model1_softsub_al1_threshold11_softmax_sub.csv']\n"]}],"source":["# @title Run this cell and check all parameters!\n","CONTEXT = \"!\" # @param {type:\"string\"}\n","TEMPERATURE = 1.0 #@param {type:\"slider\", min:0, max:2, step:0.1}\n","VAL_FNAME = \"moses_and_binding_no_rare_tokens_test.csv.gz\"\n","LOAD_CKPT_NAME = \"model1_softsub_al2.pt\" #@param [\"GPT_pretrain_07_14_23:39_1end_ignore_moses+bindingdb.pt\", \"model1_softsub_al1.pt\", \"model1_softsub_al2.pt\"] {allow-input: true}\n","NUM_TO_GENERATE = 5_0 #@param\n","PREVIOUSLY_SCORED_MOLS = [\"model1_baseline.csv\", \"model1_softsub_al1.csv\",] #@param #\"model1_softsub_al2.csv\"\n","PREVIOUS_AL_TRAIN_SETS = ['model1_softsub_al1_threshold11_softmax_sub.csv',] #@param #'model1_softsub_al2_threshold11_softmax_sub.csv'\n","# @markdown Please use the following naming scheme: \"model1_al{round of AL}\"\n","# GENERATION_FNAME = \"model1_al1\" #@param {type:\"string\"}\n","#\n","if MODE == 'Pretraining':\n","    LOAD_CKPT_PATH = f\"{PRETRAINING_PATH}model_weights/{LOAD_CKPT_NAME}\"\n","elif MODE == 'Active Learning':\n","    LOAD_CKPT_PATH = f\"{AL_PATH}model_weights/{LOAD_CKPT_NAME}\"\n","else:\n","    raise KeyError(f'requested {MODE} but only Pretraining and Active Learning are supported')\n","\n","inference_parameters = {\n","    \"batch_size\": 8,\n","    \"gen_size\": NUM_TO_GENERATE,\n","    \"generation_context\": CONTEXT,\n","    \"load_ckpt_path\": LOAD_CKPT_PATH,\n","}\n","CONFIG_DICT.update({\n","    \"generation_path\": f\"{GENERATION_PATH}smiles/{CURRENT_CYCLE_PREFIX}\",\n","    \"diffdock_scored_path_list\": [f\"{SCORING_PATH}scored_dataframes/{i}\" for i in PREVIOUSLY_SCORED_MOLS],\n","    \"al_trainsets_path_list\": [f\"{AL_PATH}training_sets/{i}\" for i in PREVIOUS_AL_TRAIN_SETS],\n","    \"inference_temp\": TEMPERATURE,})\n","print(\"Generation will use the following dataset descriptors\\n\", '/'.join(CONFIG_DICT['desc_path'].split('/')[6:]))\n","print(\"... and following model weights\\n\", '/'.join(inference_parameters['load_ckpt_path'].split('/')[6:]))\n","print(\"... and molecules will be saved to\\n\", '/'.join(CONFIG_DICT['generation_path'].split('/')[6:])+ f\"_temp{TEMPERATURE}_processed\" +'.csv')\n","print(\"... already scored molecules will be read from\\n\", ['/'.join(i.split('/')[6:]) for i in CONFIG_DICT['diffdock_scored_path_list']])\n","print(\"... AL training sets will be read from\\n\", ['/'.join(i.split('/')[6:]) for i in CONFIG_DICT['al_trainsets_path_list']])"]},{"cell_type":"markdown","metadata":{"id":"Updv9u2YWKJs"},"source":["## Code"]},{"cell_type":"markdown","metadata":{"id":"v1l2xv3wqvp0"},"source":["### Definitions"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["def export_metrics_to_workbook(metrics, fname):\n","    metric_to_col = {\n","        'generated': 'B',\n","        'valid': 'C',\n","        'unique': 'D',\n","        'validity': 'E',\n","        '% unique (rel. to generated)': 'F',\n","        '% unique (rel. to valid)': 'G',\n","        '% novelty (rel. to train set)': 'H',\n","        '% novelty (rel. to train+AL sets)': 'I',\n","        '% repetitions (from AL0 training set)': 'J',\n","        '% repetitions (from AL1 training set)': 'K',\n","        '% repetitions (from AL2 training set)': 'L',\n","        '% repetitions (from AL3 training set)': 'M',\n","        '% repetitions (from AL4 training set)': 'N',\n","        '% repetitions (from AL5 training set)': 'O',\n","        '% repetitions (from AL6 training set)': 'P',\n","        '% repetitions (from AL7 training set)': 'Q',\n","        '% repetitions (from scored from round 0)': 'R',\n","        '% repetitions (from scored from round 1)': 'S',\n","        '% repetitions (from scored from round 2)': 'T',\n","        '% repetitions (from scored from round 3)': 'U',\n","        '% repetitions (from scored from round 4)': 'V',\n","        '% repetitions (from scored from round 5)': 'W',\n","        '% repetitions (from scored from round 6)': 'X',\n","        '% repetitions (from scored from round 7)': 'Y',\n","        '% fraction of AL0 training set in generated': 'Z',\n","        '% fraction of AL1 training set in generated': 'AA',\n","        '% fraction of AL2 training set in generated': 'AB',\n","        '% fraction of AL3 training set in generated': 'AC',\n","        '% fraction of AL4 training set in generated': 'AD',\n","        '% fraction of AL5 training set in generated': 'AE',\n","        '% fraction of AL6 training set in generated': 'AF',\n","        '% fraction of AL7 training set in generated': 'AG',\n","        '% fraction of scored from round 0 in generated': 'AH',\n","        '% fraction of scored from round 1 in generated': 'AI',\n","        '% fraction of scored from round 2 in generated': 'AJ',\n","        '% fraction of scored from round 3 in generated': 'AK',\n","        '% fraction of scored from round 4 in generated': 'AL',\n","        '% fraction of scored from round 5 in generated': 'AM',\n","        '% fraction of scored from round 6 in generated': 'AN',\n","        '% fraction of scored from round 7 in generated': 'AO'}\n","    # Load an excel workbook, open sheet \"generated_logbook\", find the first nonempty row and append the metrics to that row\n","    from openpyxl import load_workbook\n","    wb = load_workbook(filename = f\"{BASE_PATH}Generative_ML Logbook.xlsx\")\n","    for i, sheet in enumerate(('generated_logbook_abs', 'generated_logbook_rel')):\n","        ws = wb[sheet]\n","        row = 3\n","        while ws[f'A{row}'].value is not None:\n","            row += 1\n","        ws[f'A{row}'] = fname\n","        for metric, value in metrics.items():\n","            if isinstance(value, str) and '=' in value:\n","                if i == 0:\n","                    value = value.split(' = ')[0].split(' * ')[1]\n","                else:\n","                    value = value.split(' = ')[1]\n","            ws[f'{metric_to_col[metric]}{row}'] = value\n","    wb.save(filename = f\"{BASE_PATH}Generative_ML Logbook.xlsx\")"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"rEXbs9mvuSDW"},"outputs":[],"source":["from rdkit import Chem\n","\n","def get_mol(smile_string):\n","    mol = Chem.MolFromSmiles(smile_string)\n","    if mol is None:\n","        return None\n","    try:\n","        Chem.SanitizeMol(mol)\n","    except ValueError:\n","        return None\n","    return mol\n","\n","def check_novelty(generated, train_list, sig_digs=3, multiplier=100, denominator=None, subtracted=True, show_work=False):\n","    total_train = set()\n","    for train in train_list:\n","        total_train = total_train | train\n","    repeated = generated & total_train\n","    if denominator is None:\n","        denominator = len(generated)\n","    if subtracted:\n","        if show_work:\n","            out = np.round(multiplier*(1-len(repeated)/denominator), sig_digs)\n","            return f\"{multiplier}*(1-{len(repeated)}/{denominator}) = {out}\"\n","        else:\n","            return np.round(multiplier*(1-len(repeated)/denominator), sig_digs)\n","    else:\n","        if show_work:\n","            out = np.round(multiplier*len(repeated)/denominator, sig_digs)\n","            return f\"{multiplier} * {len(repeated)}/{denominator} = {out}\"\n","        else:\n","            return np.round(multiplier*len(repeated)/denominator, sig_digs)\n","\n","def dump_dic_to_text(dic, path, header=None):\n","    with open(path, 'w') as f:\n","        if header is not None:\n","            f.write(f\"{header}\\n\")\n","        for key, value in dic.items():\n","            f.write(f'{key}: {value}\\n')\n","\n","def generate_SMILES(config_dict, inference_parameters):\n","    regex = re.compile(REGEX_PATTERN)\n","    dataset = SMILESDataset()\n","    dataset.load_desc_attributes(config_dict[\"desc_path\"])\n","\n","    mconf = GPTConfig(dataset.vocab_size, dataset.block_size, **config_dict)\n","    model = GPT(mconf).to(config_dict[\"device\"])\n","    model.load_state_dict(torch.load(inference_parameters[\"load_ckpt_path\"], map_location=torch.device(config_dict[\"device\"])))\n","    model.to(config_dict[\"device\"])\n","    torch.compile(model)\n","\n","    # load parameters into the model\n","    block_size = model.get_block_size()\n","    assert (block_size == dataset.block_size), \"Warning: model block size and dataset block size are different\"\n","    molecules_list, molecules_set = [], set()\n","    completions = []\n","    pbar = tqdm()\n","    while True:\n","        pbar.update()\n","        pbar.set_description(f\"generated {len(molecules_set)} unique molecules\")\n","        # create an input tensor by converting 'context' to a tensor of token indices, repeat this batch times along the batch dimension\n","        x = (torch.tensor([dataset.stoi[s] for s in regex.findall(inference_parameters[\"generation_context\"])], dtype=torch.long,)[None, ...]\n","            .repeat(inference_parameters[\"batch_size\"], 1).to(config_dict[\"device\"]))\n","        y = sample(model, x, block_size, temperature=config_dict[\"inference_temp\"])\n","        for gen_mol in y:\n","            completion = \"\".join([dataset.itos[int(i)] for i in gen_mol])  # convert generated molecule from list of integers to list of strings and concatenate to one string\n","            completions.append(completion)\n","            if completion[0] == '!' and completion[1] == '~':\n","                completion = '!' + completion[2:]\n","            if \"~\" not in completion: continue\n","            mol_string = completion[1 : completion.index(\"~\")]\n","            mol = get_mol(mol_string)  # convert the string representation of the molecule to an rdkit Mol object\n","            if mol is not None:\n","                molecules_list.append(Chem.MolToSmiles(mol))\n","                molecules_set.add(Chem.MolToSmiles(mol))\n","        if len(molecules_set) >= inference_parameters[\"gen_size\"]:\n","            break\n","    pbar.close()\n","\n","    completions_df = pd.DataFrame({\"smiles\": completions})\n","    completions_df.to_csv(config_dict[\"generation_path\"]+ f\"_temp{config_dict['inference_temp']}_completions.csv\")\n","    molecules_df = pd.DataFrame({\"smiles\": list(molecules_set)})\n","    molecules_df.to_csv(config_dict[\"generation_path\"]+ f\"_temp{config_dict['inference_temp']}_processed.csv\")\n","    characterize_generated_molecules(config_dict, molecules_list)\n","\n","def characterize_generated_molecules(config_dict, molecules_list=None):\n","    completions = pd.read_csv(config_dict[\"generation_path\"]+ f\"_temp{config_dict['inference_temp']}_completions.csv\")['smiles']\n","    molecules_set = set(pd.read_csv(config_dict[\"generation_path\"]+ f\"_temp{config_dict['inference_temp']}_processed.csv\")['smiles'])\n","    if molecules_list is None:\n","        molecules_list = []\n","        for completion in tqdm(completions, total=len(completions)):\n","            if completion[0] == '!' and completion[1] == '~':\n","                completion = '!' + completion[2:]\n","            if \"~\" not in completion: continue\n","            mol_string = completion[1 : completion.index(\"~\")]\n","            mol = get_mol(mol_string)  # convert the string representation of the molecule to an rdkit Mol object\n","            if mol is not None:\n","                molecules_list.append(Chem.MolToSmiles(mol))\n","\n","    assert molecules_set == set(molecules_list), \"Warning: set(molecules_list) and molecules_set are different\"\n","    train_data = set(pd.read_csv(config_dict[\"train_path\"])[config_dict[\"smiles_key\"]])\n","    scored_sets = {i: set(pd.read_csv(path)['smiles']) for i, path in enumerate(config_dict['diffdock_scored_path_list'])}\n","    al_sets = {i: set(pd.read_csv(path)['smiles']) for i, path in enumerate(config_dict['al_trainsets_path_list'])}\n","\n","    multiplier = 100\n","    metrics = {\n","        \"generated\": len(completions), \"valid\": len(molecules_list), \"unique\": len(molecules_set),\n","        \"validity\": np.round(multiplier*len(molecules_list)/len(completions), 3),\n","        \"% unique (rel. to generated)\": np.round(multiplier*len(molecules_set)/len(completions), 3),\n","        \"% unique (rel. to valid)\": np.round(multiplier*len(molecules_set)/len(molecules_list), 3),\n","        \"% novelty (rel. to train set)\": check_novelty(molecules_set, (train_data,), multiplier=multiplier),\n","        \"% novelty (rel. to train+AL sets)\": check_novelty(molecules_set, (train_data, *list(al_sets.values())), multiplier=multiplier),\n","    }\n","    for al_round, al_set in al_sets.items():\n","        metrics[f\"% repetitions (from AL{al_round} training set)\"] = check_novelty(molecules_set, (al_set,), subtracted=False, multiplier=multiplier, show_work=True)\n","    for score_round, score_set in scored_sets.items():\n","        metrics[f\"% repetitions (from scored from round {score_round})\"] = check_novelty(molecules_set, (score_set,), subtracted=False, multiplier=multiplier, show_work=True)\n","    for al_round, al_set in al_sets.items():\n","        metrics[f\"% fraction of AL{al_round} training set in generated\"] = check_novelty(molecules_set, (al_set,), subtracted=False, multiplier=multiplier, denominator=len(al_set), show_work=True)\n","    for score_round, score_set in scored_sets.items():\n","        metrics[f\"% fraction of scored from round {score_round} in generated\"] = check_novelty(molecules_set, (score_set,), subtracted=False, multiplier=multiplier, denominator=len(score_set), show_work=True)\n","    dump_dic_to_text(metrics, config_dict[\"generation_path\"]+ f\"_temp{config_dict['inference_temp']}_metrics.txt\")\n","    export_metrics_to_workbook(metrics, config_dict[\"generation_path\"].split('/')[-1])"]},{"cell_type":"markdown","metadata":{"id":"lo6dY-UstJbu"},"source":["### Runs"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"uO6ReKqdBt85"},"outputs":[{"name":"stderr","output_type":"stream","text":["generated 48 unique molecules: : 7it [00:16,  2.33s/it]\n"]}],"source":["# run function to generate SMILES strings\n","generate_SMILES(config_dict=CONFIG_DICT, inference_parameters=inference_parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["characterize_generated_molecules({\n","    \"train_path\": f\"{BASE_PATH}1. Pretraining/datasets/moses_and_binding_no_rare_tokens_train.csv.gz\",\n","    \"generation_path\": f\"{BASE_PATH}2. Generation/smiles/model1_softdiv_al2\", \"inference_temp\": 1.0, \"smiles_key\": \"smiles\",\n","    # \"diffdock_scored_path_list\": [\n","    #                               f'{BASE_PATH}5. Scoring/scored_dataframes/model1_baseline.csv',\n","    #                               f'{BASE_PATH}5. Scoring/scored_dataframes/model1_softsub_al1.csv',\n","    #                               # f'{BASE_PATH}5. Scoring/scored_dataframes/model1_softsub_al2.csv'\n","    #                              ],\n","    \"diffdock_scored_path_list\": [\n","                                  f'{BASE_PATH}5. Scoring/scored_dataframes/model1_baseline.csv',\n","                                  f'{BASE_PATH}5. Scoring/scored_dataframes/model1_softdiv_al1.csv',\n","                                  # f'{BASE_PATH}5. Scoring/scored_dataframes/model1_softdiv_al2.csv'\n","                                 ],\n","    # \"al_trainsets_path_list\": [\n","    #                            f'{BASE_PATH}6. ActiveLearning/training_sets/model1_baseline_threshold11_softmax_sub.csv',\n","    #                            f'{BASE_PATH}6. ActiveLearning/training_sets/model1_softsub_al1_threshold11_softmax_sub.csv',\n","    #                           #  f'{BASE_PATH}6. ActiveLearning/training_sets/model1_softsub_al2_threshold11_softmax_sub.csv'\n","    #                           ],\n","    \"al_trainsets_path_list\": [\n","                               f'{BASE_PATH}6. ActiveLearning/training_sets/model1_baseline_threshold11_softmax_divf0.25.csv',\n","                               f'{BASE_PATH}6. ActiveLearning/training_sets/model1_softdiv_al1_threshold11_softmax_divf0.25.csv',\n","                            #    f'{BASE_PATH}6. ActiveLearning/training_sets/model1_softdiv_al2_threshold11_softmax_divf0.25.csv',\n","                              ],\n","    })"]},{"cell_type":"markdown","metadata":{"id":"RmPym8y6p18t"},"source":["# Sampling molecules for DiffDock calculations"]},{"cell_type":"code","execution_count":132,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":533,"status":"ok","timestamp":1690216909185,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"NHrQY-oT0gAd","outputId":"70aef996-3fd6-4e3d-c3c4-b74e8a9e4849"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated molecules will be read from\n"," 2. Generation/smiles/model1_softsub_al2_temp1.0_processed.csv\n","... and descriptors will be saved to\n"," 3. Sampling/descriptors/model1_softsub_al2_temp1.0.pkl\n","... pca weights will be read from to\n"," 3. Sampling/pca_weights/scaler_pca_moses+bindingdb.pkl\n","... kmeans objects will be saved to\n"," 3. Sampling/kmeans_objects/model1_softsub_al2_k100means.pkl\n","... clusterings will be saved to\n"," 3. Sampling/clusterings/model1_softsub_al2_cluster_to_mols.pkl\n","... samples for diffdock will be saved to\n"," 4. DiffDock/sampled_mols/model1_softsub_al2_samples.csv\n"]}],"source":["# @title Run this cell and make sure all the paths are correct!\n","PCA_FNAME = \"scaler_pca_moses+bindingdb.pkl\" # @param {type: \"string\"}\n","N_CLUSTERS = 100 #@param {type:\"integer\"}\n","SAMPLES_PER_CLUSTER = 10 #@param {type:\"integer\"}\n","sampling_config = {\n","    \"path_to_completions\": CONFIG_DICT[\"generation_path\"]+f\"_temp{CONFIG_DICT['inference_temp']}_completions.csv\",\n","    \"path_to_predicted\": CONFIG_DICT[\"generation_path\"]+f\"_temp{CONFIG_DICT['inference_temp']}_processed.csv\",\n","    \"path_to_descriptors\": SAMPLING_PATH + \"descriptors/\" + CONFIG_DICT[\"generation_path\"].split('/')[-1] +f\"_temp{CONFIG_DICT['inference_temp']}.pkl\",\n","    \"path_to_pca\": f\"{SAMPLING_PATH}pca_weights/{PCA_FNAME}\",\n","    \"kmeans_save_path\": f\"{SAMPLING_PATH}kmeans_objects/{CURRENT_CYCLE_PREFIX}_k{N_CLUSTERS}means.pkl\",\n","    \"clusters_save_path\": f\"{SAMPLING_PATH}clusterings/{CURRENT_CYCLE_PREFIX}_cluster_to_mols.pkl\",\n","    \"samples_save_path\": f\"{SAMPLING_PATH}clusterings/{CURRENT_CYCLE_PREFIX}_cluster_to_samples.pkl\",\n","    \"diffdock_save_path\": f\"{DIFFDOCK_PATH}sampled_mols/{CURRENT_CYCLE_PREFIX}_samples.csv\",\n","    \"n_clusters\": N_CLUSTERS,\n","}\n","print(\"Generated molecules will be read from\\n\", '/'.join(sampling_config['path_to_predicted'].split('/')[6:]))\n","print(\"... and descriptors will be saved to\\n\", '/'.join(sampling_config['path_to_descriptors'].split('/')[6:]))\n","print(\"... pca weights will be read from to\\n\", '/'.join(sampling_config['path_to_pca'].split('/')[6:]))\n","print(\"... kmeans objects will be saved to\\n\", '/'.join(sampling_config['kmeans_save_path'].split('/')[6:]))\n","print(\"... clusterings will be saved to\\n\", '/'.join(sampling_config['clusters_save_path'].split('/')[6:]))\n","print(\"... samples for diffdock will be saved to\\n\", '/'.join(sampling_config['diffdock_save_path'].split('/')[6:]))"]},{"cell_type":"markdown","metadata":{"id":"Mdcw_UmAV8FK"},"source":["## Code"]},{"cell_type":"markdown","metadata":{"id":"7iB0S7xSK7-i"},"source":["### Imports & Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6379,"status":"ok","timestamp":1690054886913,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"g5iCVkclJXWf","outputId":"6abdc31a-8b6a-4b1d-d896-4b4b433ce7f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n"]}],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","!pip install rdkit\n","\n","import rdkit.Chem\n","import rdkit.Chem.Descriptors\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import pickle\n","from sklearn.cluster import KMeans\n","np.random.seed(RANDOM_SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hPBcXwkBIZIL"},"outputs":[],"source":["def descriptors_for_gpt_predictions(config):\n","    gpt_mols = pd.read_csv(config['path_to_predicted'])\n","    smiles_set = set(gpt_mols['smiles'].to_list())\n","    for scored_file in config['diffdock_scored_path_list']:\n","        for smile in pd.read_csv(scored_file)['smiles'].values:\n","            assert isinstance(smile, str), f\"{smile} is not a string\"\n","            smiles_set.add(smile)\n","    keySet = None\n","    keyToData = {}\n","    pbar = tqdm(smiles_set, total=len(smiles_set))\n","    for smile in pbar:\n","        mol = rdkit.Chem.MolFromSmiles(smile)\n","        if not mol: continue\n","        mol_data = rdkit.Chem.Descriptors.CalcMolDescriptors(mol)\n","        if keySet is None:\n","            keySet = set(mol_data.keys())\n","        for key in keySet:\n","            keyToData.setdefault(key, []).append(mol_data[key])\n","        keyToData.setdefault('smiles', []).append(smile)\n","    gpt_df = pd.DataFrame(keyToData)\n","    gpt_df.to_pickle(config['path_to_descriptors'])\n","    return gpt_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2Te1IDeJnao"},"outputs":[],"source":["def project_into_pca_space(config):\n","    scaler, pca = pickle.load(open(config[\"path_to_pca\"], 'rb'))\n","    gptMols = pd.read_pickle(config[\"path_to_descriptors\"])\n","    return pca.transform(scaler.transform(gptMols[scaler.get_feature_names_out()]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XGAgRrOKPo3"},"outputs":[],"source":["def _cluster_mols_experimental_loss(mols, n_clusters, n_iter):\n","    min_loss, best_kmeans = float('inf'), None\n","    for _ in range(n_iter):\n","        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n","        if kmeans.inertia_ < min_loss:\n","            min_loss = kmeans.inertia_\n","            best_kmeans = kmeans\n","    return best_kmeans\n","\n","def _cluster_mols_experimental_variance(mols, n_clusters, n_iter):\n","    max_variance, best_kmeans = float('-inf'), None\n","    for _ in range(n_iter):\n","        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n","        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n","        if (variance:=np.var(counts)) > max_variance:\n","            max_variance = variance\n","            best_kmeans = kmeans\n","    return best_kmeans\n","\n","def _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile):\n","    inertias = []\n","    variances = []\n","    km_objs = []\n","    for _ in range(n_iter):\n","        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n","        inertias.append(kmeans.inertia_)\n","        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n","        variances.append(np.var(counts))\n","        km_objs.append(kmeans)\n","    loss_var_kmeans_triples = sorted(zip(inertias, variances, km_objs), key=lambda x: x[0])\n","    lowest_n = loss_var_kmeans_triples[:int(len(loss_var_kmeans_triples) * mixed_objective_loss_quantile)]\n","    sorted_by_variance = sorted(lowest_n, key=lambda x: x[1])\n","    return sorted_by_variance[0][2]\n","\n","def _cluster_mols_experimental(mols, n_clusters, save_path, n_iter=1, objective='loss', mixed_objective_loss_quantile=0.1):\n","    if n_iter == 1:\n","        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n","    elif objective == 'loss':\n","        kmeans = _cluster_mols_experimental_loss(mols, n_clusters, n_iter)\n","    elif objective == 'variance':\n","        kmeans = _cluster_mols_experimental_variance(mols, n_clusters, n_iter)\n","    elif objective == 'mixed':\n","        kmeans = _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile)\n","    else:\n","        raise ValueError(f'Unknown objective {objective}')\n","\n","    pickle.dump(kmeans, open(save_path, 'wb'))\n","    return kmeans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l24k__P4KPhk"},"outputs":[],"source":["def cluster_and_sample(mols, config, n_clusters, n_samples, ensure_correctness=False, path_to_pca=None, load_kmeans=False):\n","    \"\"\"\n","        Clusters a given list of molecules, samples from each cluster, and saves the resulting data to specified files.\n","\n","        This function performs K-Means clustering on the input list of molecules and then samples a specified number of molecules\n","        from each cluster. The function ensures that the number of samples requested from each cluster doesn't exceed the total number\n","        of available molecules. The clustered data and sampled data are saved to specified file paths using pickle.\n","\n","        Parameters\n","        ----------\n","        mols : array-like or sparse matrix, shape (n_samples, n_features)\n","            The input samples where n_samples is the number of samples and n_features is the number of features.\n","\n","        n_clusters : int\n","            The number of clusters to form as well as the number of centroids to generate.\n","\n","        n_samples : int\n","            The number of samples to draw from each cluster.\n","\n","        ensure_correctness : bool, optional (default=False)\n","            If True, performs additional correctness checks, such as comparing SMILES string derived features to features in mols array.\n","            This requires 'path_to_pca' to be set.\n","\n","        path_to_pca : str, optional (default=None)\n","            If ensure_correctness is True, this should be the path to a PCA model used to transform the molecules' descriptors.\n","\n","        Returns\n","        -------\n","        cluster_to_samples : dict\n","            A dictionary where the keys are cluster labels and the values are lists of sampled SMILES strings from each cluster.\n","\n","        Raises\n","        ------\n","        AssertionError\n","            If the number of requested samples exceeds the total number of molecules provided.\n","            If ensure_correctness is True but path_to_pca is None.\n","            If the number of labels returned by the KMeans algorithm differs from the number of molecules.\n","            If features calculated from a smile string differ from features in the mols array.\n","            If the total number of sampled molecules doesn't equal to n_clusters * n_samples.\n","\n","    \"\"\"\n","    assert n_clusters * n_samples <= len(mols), f\"{n_clusters=} * {n_samples=} = {n_clusters*n_samples} requested but only {len(mols)} molecules provided\"\n","    if ensure_correctness:\n","        assert path_to_pca is not None, \"path_to_pca must be provided to ensure correctness\"\n","        scaler, pca = pickle.load(open(path_to_pca, 'rb'))\n","    if load_kmeans:\n","        kmeans = pickle.load(open(config['kmeans_save_path'], 'rb'))\n","    else:\n","        kmeans = _cluster_mols_experimental(mols=mols, n_iter=100, n_clusters=n_clusters, save_path=config[\"kmeans_save_path\"], objective='mixed', mixed_objective_loss_quantile=0.05)\n","    mols_smiles = pd.read_pickle(config[\"path_to_descriptors\"])['smiles']\n","    assert len(kmeans.labels_) == len(mols_smiles), \"Number of labels differs from number of molecules\"\n","    scored_smiles = set()\n","    for scored_file in config['diffdock_scored_path_list']:\n","        for smile in pd.read_csv(scored_file)['smiles'].values:\n","            scored_smiles.add(smile)\n","    cluster_to_mols = {}\n","    for mol, label, smile in zip(mols, kmeans.labels_, mols_smiles):\n","        if smile in scored_smiles: continue\n","        cluster_to_mols.setdefault(label, []).append(smile)\n","        if ensure_correctness: # recalculate descriptors from a smile string and compare to the descriptors in the array\n","            smile_features = pca.transform(scaler.transform(pd.DataFrame({k: [v] for k, v in rdkit.Chem.Descriptors.CalcMolDescriptors(rdkit.Chem.MolFromSmiles(smile)).items()})[scaler.get_feature_names_out()]))\n","            assert np.allclose(smile_features[0], mol), \"Features calculated from a smile string differ from features in the array\"\n","    # return cluster_to_mols\n","    # What happens below is sampling from each cluster. All the extra code is to ensure that the number of samples requested from each cluster\n","    # doesn't exceed the total number of available molecules. This is done by calculating the average number of molecules per cluster and then\n","    # calculating the number of extra molecules that need to be sampled from each cluster. The extra molecules are then distributed among the\n","    # clusters uniformly. If the number of extra molecules is greater than the number of molecules in a cluster, all\n","    # molecules from that cluster are sampled.\n","    avg_len = np.mean([len(v) for v in cluster_to_mols.values()])\n","    cluster_to_samples = {}\n","    extra_mols = (100 - len(cluster_to_mols))*10\n","    left_to_sample = n_clusters*n_samples\n","    cluster_to_len = {cluster:len(mols) for cluster, mols in cluster_to_mols.items()}\n","    for i, (cluster, _) in enumerate(sorted(cluster_to_len.items(), key=lambda x: x[1], reverse=False)):\n","        smiles = cluster_to_mols[cluster]\n","    # for i, (cluster, smiles) in enumerate(cluster_to_mols.items()):\n","        if extra_mols > 0:\n","            cur_extra = int(1+extra_mols/(len(cluster_to_mols) - i) * len(smiles)/avg_len)\n","            cur_samples = n_samples + cur_extra\n","            extra_mols -= cur_extra\n","        else:\n","            cur_samples = n_samples\n","        if cur_samples > left_to_sample:\n","            cur_samples = left_to_sample\n","        # print(f\"{cur_samples=}, {left_to_sample=}\")\n","        if len(smiles) > cur_samples:\n","            cluster_to_samples[cluster] = np.random.choice(smiles, cur_samples, replace=False)\n","            left_to_sample -= cur_samples\n","        else:\n","            cluster_to_samples[cluster] = smiles\n","            left_to_sample -= len(smiles)\n","            extra_mols += cur_samples - len(smiles)\n","\n","    assert (n_sampled:=sum(len(vals) for vals in cluster_to_samples.values())) == n_clusters*n_samples, f\"Sampled {n_sampled} but were requested {n_clusters*n_samples}\"\n","    pickle.dump(cluster_to_mols, open(config[\"clusters_save_path\"], 'wb'))\n","    pickle.dump(cluster_to_samples, open(config[\"samples_save_path\"], 'wb'))\n","    keyToData = {}\n","    for cluster, mols in cluster_to_samples.items():\n","        for mol in mols:\n","            keyToData.setdefault('smiles', []).append(mol)\n","            keyToData.setdefault('cluster_id', []).append(cluster)\n","    pd.DataFrame(keyToData).to_csv(config[\"diffdock_save_path\"])\n","    return cluster_to_samples"]},{"cell_type":"markdown","metadata":{"id":"5PpgMw_bK7-k"},"source":["### Runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjuPYQTsK7-k"},"outputs":[],"source":["descriptors_for_gpt_predictions(sampling_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljwR2w2CK7-k"},"outputs":[],"source":["pca_transformed = project_into_pca_space(sampling_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":698,"status":"ok","timestamp":1690059028600,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"8oEobsQ_K7-l","outputId":"9e325b25-dab2-451d-bbdd-5b6df004e11a"},"outputs":[],"source":["cluster_and_sample(mols=pca_transformed, config=sampling_config, n_clusters=N_CLUSTERS, n_samples=SAMPLES_PER_CLUSTER, ensure_correctness=False, load_kmeans=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690058801994,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"nYRjUHcXWBKO","outputId":"21e24b86-5c2e-4a04-ab48-5e696d7f64df"},"outputs":[{"data":{"text/plain":["(1, 4489, 935.3636363636364, array([ 273. ,  616. , 1504.5]))"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["sizes = np.array([len(elt) for elt in o.values()])\n","sizes.min(), sizes.max(), sizes.mean(), np.percentile(sizes, [25, 50, 75])"]},{"cell_type":"markdown","metadata":{"id":"jzLWORD_OwCw"},"source":["# Docking pose generation (DiffDock)"]},{"cell_type":"code","execution_count":136,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1690216910605,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"cIF0b2xeTlBM","outputId":"cee33bf0-5dda-4aac-f74d-9458f9770b87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Molecules will be read from\n"," 4. DiffDock/sampled_mols/model1_softsub_al2_samples.csv\n","... and best poses will be written to\n"," 4. DiffDock/poses/model1_softsub_al2/\n","... already scored molecules will be read from\n"," ['5. Scoring/scored_dataframes/model1_baseline.csv', '5. Scoring/scored_dataframes/model1_softsub_al1.csv', '5. Scoring/scored_dataframes/model1_softdiv_al1.csv']\n"]}],"source":["# @title Run this cell to ensure proper paths are selected!\n","DIFFDOCK_RESULTS_PATH = f\"{DIFFDOCK_PATH}poses/{CURRENT_CYCLE_PREFIX}/\"\n","\n","if not os.path.exists(DIFFDOCK_RESULTS_PATH):\n","    os.mkdir(DIFFDOCK_RESULTS_PATH)\n","print(\"Molecules will be read from\\n\", \"/\".join(sampling_config[\"diffdock_save_path\"].split('/')[6:]))\n","print(\"... and best poses will be written to\\n\", \"/\".join(DIFFDOCK_RESULTS_PATH.split('/')[6:]))\n","print(\"... already scored molecules will be read from\\n\", ['/'.join(i.split('/')[6:]) for i in CONFIG_DICT['diffdock_scored_path_list']])\n"]},{"cell_type":"markdown","metadata":{"id":"B-JpaIyJVwYw"},"source":["## Code"]},{"cell_type":"markdown","metadata":{"id":"zBgupbjAPYu7"},"source":["### Set up notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118231,"status":"ok","timestamp":1690059168291,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"GMwALd1zQBQ3","outputId":"da16f2cd-513c-44d4-872f-a1b98ec6659b"},"outputs":[],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","try:\n","    import biopandas\n","except:\n","    !pip install pyg==0.7.1 --quiet\n","    !pip install pyyaml==6.0 --quiet\n","    !pip install scipy==1.7.3 --quiet\n","    !pip install networkx==2.6.3 --quiet\n","    !pip install biopython==1.79 --quiet\n","    !pip install rdkit-pypi==2022.03.5 --quiet\n","    !pip install e3nn==0.5.0 --quiet\n","    !pip install spyrmsd==0.5.2 --quiet\n","    !pip install pandas==1.5.3 --quiet\n","    !pip install biopandas==0.4.1 --quiet\n","\n","import torch\n","print(torch.__version__)\n","\n","try:\n","    import torch_geometric\n","except ModuleNotFoundError:\n","    !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n","    !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet\n","    !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet\n","    !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet\n","    !pip install git+https://github.com/pyg-team/pytorch_geometric.git  --quiet # @ 15573f4674b2a37b1b9adc967df69ef6eee573ea\n","\n","from rdkit import Chem\n","import shutil\n","import os\n","import pandas as pd\n","\n","if not os.path.exists(\"/content/DiffDock\"):\n","    os.chdir('/content')\n","    !git clone https://github.com/gcorso/DiffDock.git\n","    os.chdir('/content/DiffDock')\n","    !git checkout a6c5275 # remove/update for more up to date code\n","\n","# clone ESM repository\n","if not os.path.exists(\"/content/DiffDock/esm\"):\n","    os.chdir('/content/DiffDock')\n","    !git clone https://github.com/facebookresearch/esm\n","    os.chdir('/content/DiffDock/esm')\n","    !git checkout ca8a710\n","    !sudo pip install -e .\n","    os.chdir('/content/DiffDock')\n"]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":["def load_scored_mols(scored_path_list):\n","    scored_mols = {}\n","    for scored_path in scored_path_list:\n","        # iterate through every row in the scored dataframe\n","        for i, row in pd.read_csv(scored_path).iterrows():\n","            if row['smiles'] in scored_mols:\n","                assert scored_mols[row['smiles']] == row['score'], f\"{row['smiles']} scored {row['score']} but was already scored {scored_mols[row['smiles']]}\"\n","            scored_mols[row['smiles']] = row['score']\n","    return scored_mols"]},{"cell_type":"markdown","metadata":{"id":"42dkxj7rRd1z"},"source":["### Run DiffDock to get top poses"]},{"cell_type":"code","execution_count":123,"metadata":{"id":"PeGCZ7P_leGr"},"outputs":[],"source":["from tqdm import tqdm\n","\n","def count_new_samples(sampled_path, scored_set):\n","    sampled_mols = set(pd.read_csv(sampling_config['diffdock_save_path'])['smiles'])\n","    repeated = len(sampled_mols & scored_set)\n","    print(f\"scored directory contains {len(scored_set)} unique molecules\")\n","    print(f\"{repeated} out of {len(sampled_mols)} sampled molecules were already scored\")\n","\n","def load_scored_mols(scored_path_list):\n","    scored_mols = set()\n","    for scored_path in scored_path_list:\n","        for smile in pd.read_csv(scored_path)['smiles'].values:\n","            scored_mols.add(smile)\n","    return scored_mols\n","\n","def get_top_poses(ligands_csv, protein_pdb_path, scored_set):\n","    data = pd.read_csv(ligands_csv)\n","    ligand_files = []\n","\n","    os.environ['HOME'] = 'esm/model_weights'\n","    os.environ['PYTHONPATH'] = f'{os.environ.get(\"PYTHONPATH\", \"\")}:/content/DiffDock/esm'\n","    pbar = tqdm(range(len(data)), total=len(data))\n","    for i in pbar:  # change 1 to len(data) for processing all ligands\n","        # print(str((i / len(data)) * 100)[:5], ' %')\n","        smiles = data['smiles'][i]\n","        if smiles in scored_set: continue\n","        rdkit_mol = Chem.MolFromSmiles(smiles)\n","\n","        if rdkit_mol is not None:\n","            with open('/content/input_protein_ligand.csv', 'w') as out:\n","                out.write('protein_path,ligand\\n')\n","                out.write(f'{protein_pdb_path},{smiles}\\n')\n","\n","            # Clear out old results if running multiple times\n","            shutil.rmtree('/content/DiffDock/results', ignore_errors=True)\n","\n","            # ESM Embedding Preparation\n","            os.chdir('/content/DiffDock')\n","            !python /content/DiffDock/datasets/esm_embedding_preparation.py --protein_ligand_csv /content/input_protein_ligand.csv --out_file /content/DiffDock/data/prepared_for_esm.fasta\n","\n","            # ESM Extraction\n","            !python /content/DiffDock/esm/scripts/extract.py esm2_t33_650M_UR50D /content/DiffDock/data/prepared_for_esm.fasta /content/DiffDock/data/esm2_output --repr_layers 33 --include per_tok --truncation_seq_length 30000\n","\n","            # Inference\n","            !python /content/DiffDock/inference.py --protein_ligand_csv /content/input_protein_ligand.csv --out_dir /content/DiffDock/results/user_predictions_small --inference_steps 20 --samples_per_complex 10 --batch_size 6\n","\n","            # Move results\n","            for root, dirs, files in os.walk('/content/DiffDock/results/user_predictions_small'):\n","                for file in files:\n","                    if file.startswith('rank1_confidence'):\n","                        shutil.move(os.path.join(root, file), os.path.join(DIFFDOCK_RESULTS_PATH, f'complex{i}.sdf'))\n","                        ligand_files.append(f'{DIFFDOCK_RESULTS_PATH}complex{i}.sdf')\n","    return ligand_files"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["scored directory contains 2000 unique molecules\n","0 out of 1000 sampled molecules were already scored\n"]}],"source":["count_new_samples(sampling_config['diffdock_save_path'], load_scored_mols(CONFIG_DICT['diffdock_scored_path_list']).keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1BiYuB92mc-EV0kmzILyP-46RuNGIu-RG"},"id":"3cfcaA3OR0Wy","outputId":"62508f20-e739-4636-a9e4-ea5db4b61fdf"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["# get top DiffDock poses\n","top_diffdock_poses = get_top_poses(sampling_config['diffdock_save_path'], PROTEIN_PATH, load_scored_mols(CONFIG_DICT['diffdock_scored_path_list']))"]},{"cell_type":"markdown","metadata":{"id":"veWeIgUNtYxK"},"source":["# Scoring poses with prolif"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1690216925344,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"-DA_sA4OmymV","outputId":"9e103668-300a-4713-e712-e2c83d87d827"},"outputs":[{"name":"stdout","output_type":"stream","text":["Molecules will be read from\n"," 4. DiffDock/sampled_mols/model1_softsub_al2_samples.csv\n","... and best poses will be read from\n"," 4. DiffDock/poses/model1_softsub_al2/\n","... scored molecules will be saved to\n"," 5. Scoring/scored_dataframes/model1_softsub_al2.csv\n","... good ligands will be selected based on threshold: 11\n","... and saved to\n"," 5. Scoring/scored_dataframes/model1_softsub_al2_threshold11.csv\n"]}],"source":["# @title Run this cell to ensure the paths are correct!\n","GOOD_LIGAND_SELECTION_MODE = \"Threshold\" #@param [\"Threshold\", \"Percentile\"]\n","# @markdown only one of the variables below is relevant (depending on selection above)\n","GOOD_LIGANDS_THRESHOLD = 11 #@param {type:\"integer\"}\n","GOOD_LIGANDS_PERCENTILE = 50 #@param {type:\"integer\"}\n","\n","scoring_config = {}\n","if GOOD_LIGAND_SELECTION_MODE == \"Threshold\":\n","    GOOD_LIGANDS_PERCENTILE = None\n","    scoring_config[\"path_to_good_ligands\"] = f\"{SCORING_PATH}scored_dataframes/{CURRENT_CYCLE_PREFIX}_threshold{GOOD_LIGANDS_THRESHOLD}.csv\"\n","    scoring_config[\"suffix\"] = f\"_threshold{GOOD_LIGANDS_THRESHOLD}\"\n","elif GOOD_LIGAND_SELECTION_MODE == \"Percentile\":\n","    GOOD_LIGANDS_THRESHOLD = None\n","    scoring_config[\"path_to_good_ligands\"] = f\"{SCORING_PATH}scored_dataframes/{CURRENT_CYCLE_PREFIX}_percentile{GOOD_LIGANDS_PERCENTILE}.csv\"\n","    scoring_config[\"al_suffix\"] = f\"_percentile{GOOD_LIGANDS_PERCENTILE}\"\n","else:\n","    raise KeyError(\"Only threshold and percentile based approaches are supported\")\n","\n","scoring_config[\"path_to_scored\"] = f\"{SCORING_PATH}scored_dataframes/{CURRENT_CYCLE_PREFIX}.csv\"\n","print(\"Molecules will be read from\\n\", \"/\".join(sampling_config[\"diffdock_save_path\"].split('/')[6:]))\n","print(\"... and best poses will be read from\\n\", \"/\".join(DIFFDOCK_RESULTS_PATH.split('/')[6:]))\n","print(\"... scored molecules will be saved to\\n\", \"/\".join(scoring_config[\"path_to_scored\"].split('/')[6:]))\n","print(\"... good ligands will be selected based on\", f\"{GOOD_LIGANDS_PERCENTILE}th percentile\" if GOOD_LIGANDS_PERCENTILE is not None else f\"threshold: {GOOD_LIGANDS_THRESHOLD}\")\n","print(\"... and saved to\\n\", \"/\".join(scoring_config[\"path_to_good_ligands\"].split('/')[6:]))"]},{"cell_type":"markdown","metadata":{"id":"s1evrZcDXi2u"},"source":["## Code"]},{"cell_type":"markdown","metadata":{"id":"Tamyd6Hstkpl"},"source":["### Set up notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11049,"status":"ok","timestamp":1690216811834,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"gjg__oTktYHh","outputId":"b21553f1-1cd3-455e-d6ff-5539a80ffd30"},"outputs":[],"source":["!pip install -q condacolab\n","import condacolab\n","condacolab.install()\n","import condacolab\n","\n","!mamba install pymol-open-source --yes\n","\n","!pip install prolif\n","!pip install rdkit\n","\n","from pymol import cmd\n","import prolif\n","from prolif.plotting.network import LigNetwork\n","from rdkit import Chem\n","from IPython.display import Image\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from scipy.stats import gaussian_kde\n","from tqdm import tqdm\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"nTDMdNdimsus"},"source":["### Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1690216928627,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"G_HXZNoHAJ-Y","outputId":"53226a38-dee0-470e-8cae-4939cfd8645c"},"outputs":[{"name":"stdout","output_type":"stream","text":["11.0\n"]}],"source":["interaction_scores = {\n","    'Hydrophobic': 2.5,\n","    'HBDonor': 3.5,\n","    'HBAcceptor': 3.5,\n","    'Anionic': 7.5,\n","    'Cationic': 7.5,\n","    'CationPi': 2.5,\n","    'PiCation': 2.5,\n","    'VdWContact': 1.0,\n","    'XBAcceptor': 3.0,\n","    'XBDonor': 3.0,\n","    'FaceToFace': 3.0,\n","    'EdgeToFace': 1.0,\n","    'MetalDonor': 3.0,\n","    'MetalAcceptor': 3.0,\n","}\n","# interaction_scores = {\n","#     'Hydrophobic': 1,\n","#     'HBDonor': 1,\n","#     'HBAcceptor': 1,\n","#     'Anionic': 1,\n","#     'Cationic': 1,\n","#     'CationPi': 1,\n","#     'PiCation': 1,\n","#     'VdWContact': 1,\n","#     'XBAcceptor': 1,\n","#     'XBDonor': 1,\n","#     'FaceToFace': 1,\n","#     'EdgeToFace': 1,\n","#     'MetalDonor': 1,\n","#     'MetalAcceptor': 1}\n","print(interaction_scores['HBDonor']*2 + interaction_scores['VdWContact']*3 + interaction_scores['EdgeToFace']*1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhu7U2OSps1z"},"outputs":[],"source":["def get_contacts(protein, ligand):\n","  cmd.delete('all')\n","  cmd.load(protein, 'protein')\n","  cmd.h_add('protein')\n","  cmd.remove('sol')\n","  cmd.save('/content/protein.pdb')\n","\n","  prot = prolif.Molecule(Chem.MolFromPDBFile('/content/protein.pdb', removeHs=False))\n","  lig = Chem.SDMolSupplier(ligand, removeHs=False)\n","  lig = prolif.Molecule.from_rdkit(lig[0])\n","\n","  fp = prolif.Fingerprint(interactions=list(interaction_scores.keys()))\n","  fp.run_from_iterable([lig], prot, progress=False)\n","  try:\n","    df = fp.to_dataframe(return_atoms=True)\n","    df_stacked = df.stack(level=[0, 1, 2])\n","    df_reset = df_stacked.to_frame().reset_index()\n","    df_reset.columns = ['Frame', 'ligand', 'protein', 'interaction', 'value']\n","    df_reset['score'] = df_reset['interaction'].apply(lambda x: interaction_scores[x])\n","    return df_reset['score'].sum()\n","  except:\n","    print('Complex has no meaningful protein-ligand connections')\n","    return int(0)\n","\n","def score_ligands(ligand_poses_dir, protein_path):\n","    ligand_list = [ligand_poses_dir + lig for lig in os.listdir(ligand_poses_dir) if lig.endswith('.sdf')]\n","    # count = 0\n","    ligand_scores = {}\n","    pbar = tqdm(ligand_list, total=len(ligand_list))\n","    # for lig in ligand_list:\n","    for lig in pbar:\n","        if lig.split('/')[-1].split('.')[0] not in ligand_scores:\n","            score = get_contacts(PROTEIN_PATH, lig)\n","            ligand_scores[lig.split('/')[-1].split('.')[0]] = score\n","            # count += 1\n","            # if int(count%(len(ligand_list)/100)) == 0:\n","                # print(int(count/len(ligand_list)*100), '% done')\n","    return ligand_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJIQQXxkm3Pu"},"outputs":[],"source":["def plot_ligand_scores(ligand_scores):\n","\n","    data = list(ligand_scores.values())\n","\n","    plt.figure(figsize=(8, 6), dpi=80)\n","\n","    plt.hist(data, bins=50, density=True, color='gray', alpha=0.7, edgecolor='black')\n","\n","    smoothed_data = np.linspace(min(data), max(data), 1000)\n","    kde = gaussian_kde(data)\n","    smoothed_line = kde(smoothed_data)\n","\n","    plt.plot(smoothed_data, smoothed_line, linewidth=2.5, color='black')\n","\n","    plt.xlabel(\"Values\", fontsize=18)\n","    plt.ylabel(\"Density\", fontsize=18)\n","\n","    plt.xticks(fontsize=16)\n","    plt.yticks(fontsize=16)\n","\n","    plt.title(\"Density Plot of Scores\", fontsize=20)\n","\n","    plt.grid(linestyle='--', linewidth=0.5, alpha=0.7)\n","\n","    plt.gca().spines['top'].set_visible(False)\n","    plt.gca().spines['right'].set_visible(False)\n","\n","    plt.tick_params(axis='both', which='both', direction='in', length=4)\n","\n","    plt.savefig(SCORING_PATH + 'plots/' + CURRENT_CYCLE_PREFIX + '_scores.png')\n","\n","    plt.show()\n","\n","    print(f'There are {list(ligand_scores.values()).count(0)} ligands with 0 connections ({np.round(list(ligand_scores.values()).count(0) / len(ligand_scores)*100, 1)}%)')\n","\n","    for i in range(int(np.max(list(ligand_scores.values()))) + 1):\n","        print(f'There are {len([value for value in list(ligand_scores.values()) if value > i])} ligands with scores greater than {i} ({np.round(len([value for value in list(ligand_scores.values()) if value > i]) / len(ligand_scores)*100, 1)}%)')\n","\n","\n","\n","    print(f'The mean score for all ligands is {np.round(np.mean(list(ligand_scores.values())), 1)}')\n","    print(f'The lowest score for all ligands is {np.min(list(ligand_scores.values()))}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYpuxAweb5Jg"},"outputs":[],"source":["def parse_and_prepare_diffdock_data(ligand_scores, config, diffdock_samples_path, lower_percentile=None, threshold=None, scored_db = None):\n","    if scored_db is None: scored_db = {}\n","    diffdock_samples = pd.read_csv(diffdock_samples_path)\n","    if lower_percentile is not None:\n","        threshold = np.percentile(list(ligand_scores.values()), lower_percentile)\n","    all_ligands = {int(complex_name[7:]): score for complex_name, score in ligand_scores.items()} # complex names are complex000\n","    getter = lambda x: scored_db[x] if x in scored_db else all_ligands.get(x, 0)\n","    diffdock_samples['score'] = [getter(complex_number) for complex_number in diffdock_samples.index]\n","    diffdock_samples.to_csv(config[\"path_to_scored\"])\n","    good_ligands = diffdock_samples.loc[diffdock_samples['score'] >= threshold]\n","    good_ligands.to_csv(config[\"path_to_good_ligands\"])\n","    return diffdock_samples"]},{"cell_type":"markdown","metadata":{"id":"KhUk_1SLmzvh"},"source":["### Runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":428350,"status":"ok","timestamp":1690217366102,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"LOZj7YU9Fdye","outputId":"3e507340-e9c4-4038-b7b7-f937e7fc08f4"},"outputs":[{"name":"stderr","output_type":"stream","text":["  7%|▋         | 68/989 [00:40<06:17,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["Complex has no meaningful protein-ligand connections\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 165/989 [01:20<05:06,  2.69it/s]"]},{"name":"stdout","output_type":"stream","text":["Complex has no meaningful protein-ligand connections\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▊      | 381/989 [02:53<03:58,  2.55it/s]"]},{"name":"stdout","output_type":"stream","text":["Complex has no meaningful protein-ligand connections\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 989/989 [07:07<00:00,  2.31it/s]\n"]}],"source":["ligand_scores = score_ligands(DIFFDOCK_RESULTS_PATH, PROTEIN_PATH)\n","# ligand_scores_unweighted = score_ligands(ligand_poses_dir, protein_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1438,"status":"ok","timestamp":1690217466957,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"bKVQurmgFu41","outputId":"44badb34-fcca-458d-a226-1fc325807fbd"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["There are 3 ligands with 0 connections (0.3%)\n","There are 986 ligands with scores greater than 0 (99.7%)\n","There are 982 ligands with scores greater than 1 (99.3%)\n","There are 974 ligands with scores greater than 2 (98.5%)\n","There are 959 ligands with scores greater than 3 (97.0%)\n","There are 937 ligands with scores greater than 4 (94.7%)\n","There are 908 ligands with scores greater than 5 (91.8%)\n","There are 857 ligands with scores greater than 6 (86.7%)\n","There are 783 ligands with scores greater than 7 (79.2%)\n","There are 705 ligands with scores greater than 8 (71.3%)\n","There are 630 ligands with scores greater than 9 (63.7%)\n","There are 561 ligands with scores greater than 10 (56.7%)\n","There are 486 ligands with scores greater than 11 (49.1%)\n","There are 416 ligands with scores greater than 12 (42.1%)\n","There are 364 ligands with scores greater than 13 (36.8%)\n","There are 306 ligands with scores greater than 14 (30.9%)\n","There are 259 ligands with scores greater than 15 (26.2%)\n","There are 216 ligands with scores greater than 16 (21.8%)\n","There are 179 ligands with scores greater than 17 (18.1%)\n","There are 153 ligands with scores greater than 18 (15.5%)\n","There are 132 ligands with scores greater than 19 (13.3%)\n","There are 113 ligands with scores greater than 20 (11.4%)\n","There are 96 ligands with scores greater than 21 (9.7%)\n","There are 86 ligands with scores greater than 22 (8.7%)\n","There are 76 ligands with scores greater than 23 (7.7%)\n","There are 66 ligands with scores greater than 24 (6.7%)\n","There are 56 ligands with scores greater than 25 (5.7%)\n","There are 47 ligands with scores greater than 26 (4.8%)\n","There are 42 ligands with scores greater than 27 (4.2%)\n","There are 39 ligands with scores greater than 28 (3.9%)\n","There are 30 ligands with scores greater than 29 (3.0%)\n","There are 24 ligands with scores greater than 30 (2.4%)\n","There are 15 ligands with scores greater than 31 (1.5%)\n","There are 14 ligands with scores greater than 32 (1.4%)\n","There are 13 ligands with scores greater than 33 (1.3%)\n","There are 10 ligands with scores greater than 34 (1.0%)\n","There are 10 ligands with scores greater than 35 (1.0%)\n","There are 7 ligands with scores greater than 36 (0.7%)\n","There are 5 ligands with scores greater than 37 (0.5%)\n","There are 4 ligands with scores greater than 38 (0.4%)\n","There are 4 ligands with scores greater than 39 (0.4%)\n","There are 3 ligands with scores greater than 40 (0.3%)\n","There are 3 ligands with scores greater than 41 (0.3%)\n","There are 2 ligands with scores greater than 42 (0.2%)\n","There are 2 ligands with scores greater than 43 (0.2%)\n","There are 1 ligands with scores greater than 44 (0.1%)\n","There are 1 ligands with scores greater than 45 (0.1%)\n","There are 0 ligands with scores greater than 46 (0.0%)\n","The mean score for all ligands is 12.4\n","The lowest score for all ligands is 0.0\n"]}],"source":["plot_ligand_scores(ligand_scores)\n","# plot_ligand_scores(ligand_scores_unweighted, save_name='good_ligands_iter1_plot_unweighted_connections.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":1511,"status":"ok","timestamp":1690217482061,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"4828Gu_zFwUu","outputId":"ebbe028f-b897-4034-e034-807379ab70d4"},"outputs":[{"data":{"text/html":["\n","\n","  <div id=\"df-cf2846dc-0ef1-4782-9c23-fb4e6daef5ff\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>smiles</th>\n","      <th>cluster_id</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>CN1c2ccc(NS(=O)(=O)c3ccccc3[Na+])cc2N=C(c2ccc(...</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>N#[N+]c1ccc2nc(-c3ccc(C(=O)N4CCN(C(=O)c5ccc([N...</td>\n","      <td>22</td>\n","      <td>11.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>N#[N+]c1ccc2nc(-c3ccc(OCCCN4CCCCC4)cc3)[nH]c2c1</td>\n","      <td>22</td>\n","      <td>15.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>N#Cc1ccc(Oc2ccc(NC(=O)C3CCCO3)c(Cl)c2)c([N+]#N)c1</td>\n","      <td>22</td>\n","      <td>5.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>N#Cc1ccc(C2CCc3nc4ccc(-c5ccc([N+]#N)cc5)cn4c32...</td>\n","      <td>22</td>\n","      <td>15.5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>995</td>\n","      <td>Cn1nc2n(c1=O)CCN(Cc1nc(-c3cccc(Cl)c3)no1)C2</td>\n","      <td>83</td>\n","      <td>12.0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>996</td>\n","      <td>CC(C)C(NC(=O)Cn1cnc2c(nnn2C)c1=O)c1ccc(F)cc1</td>\n","      <td>83</td>\n","      <td>6.5</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>997</td>\n","      <td>O=C(NCc1ccccn1)c1ccc2ncc(-c3cc4ccccc4o3)n2n1</td>\n","      <td>83</td>\n","      <td>11.5</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>998</td>\n","      <td>NC(c1cccs1)c1nc2c(s1)CN(C(=O)c1cccnc1)CC2</td>\n","      <td>83</td>\n","      <td>9.0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>999</td>\n","      <td>Cc1ccsc1-c1cccc(Nc2cc(=O)n(C)c(=O)n2C)c1</td>\n","      <td>83</td>\n","      <td>9.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf2846dc-0ef1-4782-9c23-fb4e6daef5ff')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-6a36316a-d87d-455a-86e9-8b8ab1bc0095\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a36316a-d87d-455a-86e9-8b8ab1bc0095')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-6a36316a-d87d-455a-86e9-8b8ab1bc0095 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cf2846dc-0ef1-4782-9c23-fb4e6daef5ff button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cf2846dc-0ef1-4782-9c23-fb4e6daef5ff');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"],"text/plain":["     Unnamed: 0                                             smiles  \\\n","0             0  CN1c2ccc(NS(=O)(=O)c3ccccc3[Na+])cc2N=C(c2ccc(...   \n","1             1  N#[N+]c1ccc2nc(-c3ccc(C(=O)N4CCN(C(=O)c5ccc([N...   \n","2             2    N#[N+]c1ccc2nc(-c3ccc(OCCCN4CCCCC4)cc3)[nH]c2c1   \n","3             3  N#Cc1ccc(Oc2ccc(NC(=O)C3CCCO3)c(Cl)c2)c([N+]#N)c1   \n","4             4  N#Cc1ccc(C2CCc3nc4ccc(-c5ccc([N+]#N)cc5)cn4c32...   \n","..          ...                                                ...   \n","995         995        Cn1nc2n(c1=O)CCN(Cc1nc(-c3cccc(Cl)c3)no1)C2   \n","996         996       CC(C)C(NC(=O)Cn1cnc2c(nnn2C)c1=O)c1ccc(F)cc1   \n","997         997       O=C(NCc1ccccn1)c1ccc2ncc(-c3cc4ccccc4o3)n2n1   \n","998         998          NC(c1cccs1)c1nc2c(s1)CN(C(=O)c1cccnc1)CC2   \n","999         999           Cc1ccsc1-c1cccc(Nc2cc(=O)n(C)c(=O)n2C)c1   \n","\n","     cluster_id  score  \n","0             4    0.0  \n","1            22   11.5  \n","2            22   15.5  \n","3            22    5.5  \n","4            22   15.5  \n","..          ...    ...  \n","995          83   12.0  \n","996          83    6.5  \n","997          83   11.5  \n","998          83    9.0  \n","999          83    9.0  \n","\n","[1000 rows x 4 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["parse_and_prepare_diffdock_data(ligand_scores, config=scoring_config, diffdock_samples_path=sampling_config[\"diffdock_save_path\"],\n","                                threshold=GOOD_LIGANDS_THRESHOLD, lower_percentile=GOOD_LIGANDS_PERCENTILE)"]},{"cell_type":"markdown","metadata":{"id":"oUjA5PGgZbQd"},"source":["# Prepare training dataset for next round of active learning"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["there are 5304 scored, 5304 diffused, and 1450 overlapping molecules\n"]}],"source":["def sandbox():\n","    scored = set(pd.read_csv(f\"{AL_PATH}training_sets/model1_baseline_threshold11_softmax_sub.csv\")['smiles'])\n","    diffused = set(pd.read_csv(f\"{AL_PATH}training_sets/model1_baseline_threshold11_softmax_sub_noscore.csv\")['smiles'])\n","    overlap = scored & diffused\n","    print(f\"there are {len(scored)} scored, {len(diffused)} diffused, and {len(overlap)} overlapping molecules\")\n","\n","sandbox()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1690220226477,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"lhWjqnBYaJSQ","outputId":"25e12d76-21d2-4582-c230-8b17b66c01d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Will read scored molecules from\n"," 5. Scoring/scored_dataframes/model1_softsub_al2.csv\n","... and good ligands from\n"," 5. Scoring/scored_dataframes/model1_softsub_al2_threshold11.csv\n","... and clusterings from\n"," 3. Sampling/clusterings/model1_softsub_al2_cluster_to_mols.pkl\n","... and will save the AL training set to\n"," 6. ActiveLearning/training_sets/model1_softsub_al2_threshold11_softmax_sub.csv\n","... and will save the no score samplings to\n"," 6. ActiveLearning/training_sets/model1_softsub_al2_threshold11_softmax_sub_noscore.csv\n"]}],"source":["# @title Run this cell to ensure paths are correct\n","PROBABILITY_CALCULATION = \"softmax\" #@param [\"softmax\", \"linear\"]\n","SOFTMAX_NORMALIZATION_TYPE = \"subtract\" #@param [\"divide\", \"subtract\"]\n","# @markdown the following variable is only relevant when softmax is selected\n","SOFTMAX_DIV_FACTOR = 0.25 #@param\n","N_SAMPLES = 5_000 # @param\n","if PROBABILITY_CALCULATION == 'softmax':\n","    al_suffix = f\"_{PROBABILITY_CALCULATION}\"\n","    if SOFTMAX_NORMALIZATION_TYPE == 'divide':\n","        al_suffix += f'_divf{SOFTMAX_DIV_FACTOR}'\n","    else:\n","        al_suffix += '_sub'\n","elif PROBABILITY_CALCULATION == 'linear':\n","    al_suffix = f\"_{PROBABILITY_CALCULATION}\"\n","else:\n","    raise KeyError(\"Only softmax and linear probabilities are supported\")\n","\n","\n","AL_TRAIN_PATH = f\"{AL_PATH}training_sets/{CURRENT_CYCLE_PREFIX}{scoring_config['suffix']}{al_suffix}.csv\"\n","AL_DIFFUSION_PATH = f\"{AL_PATH}training_sets/{CURRENT_CYCLE_PREFIX}{scoring_config['suffix']}{al_suffix}_noscore.csv\"\n","\n","print(\"Will read scored molecules from\\n\", '/'.join(scoring_config['path_to_scored'].split('/')[6:]))\n","print(\"... and good ligands from\\n\", '/'.join(scoring_config['path_to_good_ligands'].split('/')[6:]))\n","print(\"... and clusterings from\\n\", '/'.join(sampling_config['clusters_save_path'].split('/')[6:]))\n","print(\"... and will save the AL training set to\\n\", '/'.join(AL_TRAIN_PATH.split('/')[6:]))\n","print(\"... and will save the no score samplings to\\n\", '/'.join(AL_DIFFUSION_PATH.split('/')[6:]))"]},{"cell_type":"markdown","metadata":{"id":"TQ9BaRa8GyK7"},"source":["Here, we need to map the good data back to the original clusters and create a training set for active learning. THen, perform active learning, generate molecules, and repeat process. Finally, generate 2 million molecules to train BERT"]},{"cell_type":"markdown","metadata":{"id":"svfAh4qsC17K"},"source":["## Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRsKbVS6akBs"},"outputs":[],"source":["import numpy as np\n","np.random.seed(RANDOM_SEED)\n","import pandas as pd\n","\n","def compute_cluster_scores():\n","    good_data = pd.read_csv(scoring_config['path_to_scored'])\n","    cluster_to_scores = {}\n","    for index, row in good_data.iterrows():\n","        cluster_to_scores.setdefault(row['cluster_id'], []).append(row['score'])\n","    cluster_to_score = {cluster_id: np.mean(scores) for cluster_id, scores in cluster_to_scores.items()}\n","    return cluster_to_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqjFiWRObkRf"},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","def plot_probability_distribution(cluster_scores, args_list, bin_size=0.001):\n","    fig = go.Figure()\n","    fig.add_trace(go.Histogram(x=[1/100 for _ in range(100)], name='uniform', opacity=0.75, histnorm='probability', xbins=dict(start=-0.1, end=1, size=bin_size)))\n","    keyToFunc = {\"linear\": _preprocess_scores_linearly, \"softmax\": _preprocess_scores_softmax}\n","    for (func, param_switch, param_value) in args_list:\n","        suffix, extras = \"\", {}\n","        if func == \"softmax\":\n","            suffix = f\" divf {param_value}\" if param_switch else \"\"\n","            extras = dict(divide=param_switch, divide_factor=param_value)\n","        fig.add_trace(go.Histogram(x=list(keyToFunc[func](cluster_scores, **extras).values()), name=f\"{func}{suffix}\", opacity=0.75, histnorm='probability', xbins=dict(start=-0.1, end=1, size=bin_size)))\n","    fig.update_layout(barmode='overlay', title=\"Probabilities of sampling from clusters based on mean scores\", xaxis_title=\"probability\", yaxis_title=\"rel. frequency\")\n","    return fig\n","\n","def plot_mean_cluster_scores(cluster_scores):\n","    fig = go.Figure()\n","    fig.add_trace(go.Histogram(x=list(cluster_scores.values()), opacity=0.75, histnorm='probability'))\n","    fig.update_layout(barmode='overlay', title=\"Distribution of cluster scores\", xaxis_title=\"Mean cluster score\", yaxis_title=\"rel. frequency\")\n","    return fig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84XMCXxAQyfa"},"outputs":[],"source":["import pickle\n","\n","def _preprocess_scores_linearly(scores, do_negation=False, **kwargs):\n","    \"\"\"\n","        Preprocesses a dictionary of scores by negating and normalizing them.\n","\n","        The function negates all scores and optionally removes positive scores. If the minimum value among the negated scores\n","        is less than zero, it shifts all values by subtracting the minimum value and adding 'lowest_score'. The final step is\n","        to normalize the scores so that their total sum equals to 1.\n","\n","        Parameters\n","        ----------\n","        scores : dict\n","            A dictionary of scores where the keys are identifiers and the values are their corresponding scores.\n","\n","        remove_positives : bool, optional (default=False)\n","            If True, all positive scores are removed after negation.\n","\n","        lowest_score : int, optional (default=1)\n","            This value is added to all scores if the minimum score is less than zero.\n","\n","        Returns\n","        -------\n","        normalized : dict\n","            The normalized dictionary of scores.\n","\n","    \"\"\"\n","    sign = -1 if do_negation else 1\n","    negated = {k: sign*v for k, v in scores.items()}\n","    total = sum(negated.values())\n","    normalized = {k: v / total for k, v in negated.items()}\n","    return normalized\n","\n","def _preprocess_scores_softmax(scores, do_negation=False, divide=True, divide_factor=None):\n","    sign = -1 if do_negation else 1\n","    negated = {k: sign*v for k, v in scores.items()}\n","    max_value = max(negated.values())\n","    if divide:\n","        assert divide_factor is not None, 'You have to specify a value p in (0, 1). Softmax is computed as e^[x/(p*max)]'\n","        exponentiate = {k: np.exp(v / (divide_factor*max_value)) for k, v in negated.items()}\n","    else:\n","        exponentiate = {k: np.exp(v - max_value) for k, v in negated.items()}\n","    total = sum(exponentiate.values())\n","    softmax = {k: v / total for k, v in exponentiate.items()}\n","    return softmax\n","\n","def balance_cluster_to_n(cluster_to_n, cluster_to_len):\n","    \"\"\"\n","        Balances the target number of samples for each cluster to ensure it doesn't exceed the actual size of the cluster.\n","\n","        The function first calculates the surplus (i.e., the excess of the target number over the actual size) for each cluster.\n","        Then, it distributes the total surplus proportionally among the clusters that have a deficit (i.e., the target number is less than the actual size).\n","        If after this distribution, there's still a deficit (i.e., the sum of target numbers is less than the sum of actual sizes), the function\n","        increases the target number of the largest clusters one by one until the sum of target numbers equals to the sum of actual sizes.\n","\n","        Parameters\n","        ----------\n","        cluster_to_n : dict\n","            A dictionary mapping cluster identifiers to their target number of samples.\n","\n","        cluster_to_len : dict\n","            A dictionary mapping cluster identifiers to the actual size of each cluster.\n","\n","        Returns\n","        -------\n","        balanced : dict\n","            A dictionary mapping cluster identifiers to their balanced target number of samples.\n","\n","        Raises\n","        ------\n","        AssertionError\n","            If the sum of target numbers before and after balancing don't match.\n","\n","    \"\"\"\n","\n","    surplus = {key: cluster_to_n[key] - cluster_to_len[key] for key in cluster_to_n if cluster_to_n[key] > cluster_to_len[key]}\n","    balanced = {k:v for k, v in cluster_to_n.items()}\n","    n_to_cluster = {v: k for k, v in cluster_to_n.items()}\n","\n","    for key in surplus:\n","        balanced[key] = cluster_to_len[key]\n","\n","    total_surplus = sum(surplus.values())\n","    initial_n_sum = sum(n for key, n in cluster_to_n.items() if key not in surplus)\n","\n","    for key in balanced:\n","        if key in surplus: continue\n","        surplus_to_add = total_surplus * cluster_to_n[key] / initial_n_sum\n","        new_n = int(cluster_to_n[key] + surplus_to_add)\n","        balanced[key] = min(new_n, cluster_to_len[key])\n","\n","    deficit = sum(cluster_to_n.values()) - sum(balanced.values())\n","    while deficit > 0:\n","        for initial_n in sorted(n_to_cluster, reverse=True):\n","            if deficit == 0:\n","                break\n","            if (cluster:=n_to_cluster[initial_n]) in surplus: continue\n","            if balanced[cluster] < cluster_to_len[cluster]:\n","                balanced[cluster] += 1\n","                deficit -= 1\n","\n","    assert sum(cluster_to_n.values()) == sum(balanced.values()), f\"Before balancing had {sum(cluster_to_n.values())}, post balancing = {sum(balanced.values())}\"\n","    return balanced\n","\n","def sample_clusters_for_active_learning(cluster_to_scores, n_samples, config, probability_type='softmax', divide=True, divide_factor=None):\n","    \"\"\"\n","        Sample molecules from clusters for active learning purposes, considering previously docked molecules and balancing the sampling among clusters.\n","\n","        This function uses either softmax or uniform probabilities to determine how many molecules to sample from each cluster. The function then samples\n","        the required number of new molecules (i.e., those not present in docked_mols) from each cluster. The sampling is balanced to ensure the target number\n","        doesn't exceed the actual size of the cluster.\n","\n","        Parameters\n","        ----------\n","        cluster_to_scores : dict\n","            A dictionary mapping cluster identifiers to their scores.\n","\n","        n_samples : int\n","            The total number of molecules to sample.\n","\n","        path_to_clusters : str\n","            The path to a pickle file storing a dictionary that maps each cluster to a list of molecules.\n","\n","        probability_type : str, optional (default='softmax')\n","            The type of probability distribution used to determine the number of samples per cluster.\n","            Options are 'softmax' and 'uniform'.\n","\n","        remove_positives : bool, optional (default=False)\n","            Only used when probability_type is 'uniform'. If True, positive scores are removed after negation.\n","\n","        lowest_score : int, optional (default=1)\n","            Only used when probability_type is 'uniform'. This value is added to all scores if the minimum score is less than zero.\n","\n","        Returns\n","        -------\n","        training : list\n","            A list of randomly sampled molecules for active learning.\n","\n","        Raises\n","        ------\n","        KeyError\n","            If an unsupported probability_type is provided.\n","        AssertionError\n","            If the number of sampled molecules doesn't equal to n_samples.\n","\n","    \"\"\"\n","    if probability_type == 'softmax':\n","        probability_function = lambda x: _preprocess_scores_softmax(x, divide=divide, divide_factor=divide_factor)\n","    elif probability_type == 'linear':\n","        probability_function = lambda x: _preprocess_scores_linearly(x)\n","    elif probability_type == 'no_score':\n","        probability_function = lambda x: x\n","    else:\n","        raise KeyError(\"Only uniform and softmax probabilities are supported\")\n","    cluster_to_mols = pickle.load(open(config['clusters_save_path'], 'rb'))\n","    cluster_to_samples = pickle.load(open(config['samples_save_path'], 'rb'))\n","    docked_mols = {smile for smiles in cluster_to_samples.values() for smile in smiles}\n","    cluster_to_new_mols = {k: [smile for smile in set(v) if smile not in docked_mols] for k, v in cluster_to_mols.items()}\n","\n","    probabilities = probability_function(cluster_to_scores)\n","    # print(f\"Probabilities\\n\", probabilities)\n","    cluster_to_n = {k: int(v * n_samples) for k, v in probabilities.items()}\n","    max_cluster_id, max_prob = None, 0\n","    for cluster, prob in probabilities.items():\n","        if prob > max_prob:\n","            max_cluster_id, max_prob = cluster, prob\n","    cluster_to_n[max_cluster_id] += n_samples - sum(cluster_to_n.values())\n","\n","    cluster_to_len = {k: len(v) for k, v in cluster_to_new_mols.items()}\n","    balanced = balance_cluster_to_n(cluster_to_n, cluster_to_len)\n","\n","    training = []\n","    np.random.seed(RANDOM_SEED)\n","    for i, (cluster, n) in enumerate(balanced.items()):\n","        training.extend(np.random.choice(cluster_to_new_mols[cluster], n, replace=False))\n","    assert len(training) == n_samples, f\"{len(training)=} != {n_samples=}\"\n","    return training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFY3C94tXbki"},"outputs":[],"source":["def combine_sampled_and_good_ligands(sampled, good_ligands, good_ligand_multiplier:int, save_path):\n","    assert isinstance(good_ligand_multiplier, int), \"A multiplier should be an integer\"\n","    keyToData = {}\n","    print(f\"Sampled {len(sampled)} molecules, of which {len(set(sampled))} are unique\")\n","    for mol in sampled:\n","        keyToData.setdefault('smiles', []).append(mol)\n","    for mol in good_ligands:\n","        for _ in range(good_ligand_multiplier):\n","            keyToData.setdefault('smiles', []).append(mol)\n","    combined = pd.DataFrame(keyToData)\n","    print(\"Combined dataframe has shape\", combined.shape)\n","    combined.to_csv(save_path)\n","    return combined"]},{"cell_type":"markdown","metadata":{"id":"xGA3B_vOC0Dn"},"source":["## Runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":1057,"status":"ok","timestamp":1690220232034,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"lDhJii45al0b","outputId":"775e4409-fc2b-4a52-ac71-ef06ca752ea6"},"outputs":[{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"f2e5e741-7519-47db-a237-b0cec9fc36a2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f2e5e741-7519-47db-a237-b0cec9fc36a2\")) {                    Plotly.newPlot(                        \"f2e5e741-7519-47db-a237-b0cec9fc36a2\",                        [{\"histnorm\":\"probability\",\"opacity\":0.75,\"x\":[0.0,12.0,12.5,14.11111111111111,13.727272727272727,10.636363636363637,14.5,17.045454545454547,10.181818181818182,13.0,12.454545454545455,9.454545454545455,10.045454545454545,13.454545454545455,11.363636363636363,11.818181818181818,14.272727272727273,10.181818181818182,11.045454545454545,11.363636363636363,24.818181818181817,14.227272727272727,7.7727272727272725,11.954545454545455,15.909090909090908,13.181818181818182,12.409090909090908,10.0,10.818181818181818,8.590909090909092,13.636363636363637,16.681818181818183,19.40909090909091,8.909090909090908,11.636363636363637,9.45,9.0,11.6,12.45,11.8,17.65,13.45,13.05,22.75,16.15,17.55,10.2,22.4,11.55,10.65,14.35,12.75,10.05,14.2,9.9,12.8,9.45,8.25,8.65,10.65,9.4,21.65,16.9,16.5,10.05,9.35,12.6,9.75,12.25,11.6,9.55,12.0,20.6,15.95,12.55,8.4,12.3,12.05,8.95,12.2,12.5,8.05,16.6,8.7,9.45,8.0,7.8,11.2,10.45,5.55,9.65,9.5,20.35,8.5,11.1,13.35,9.55,7.45,9.7],\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"barmode\":\"overlay\",\"title\":{\"text\":\"Distribution of cluster scores\"},\"xaxis\":{\"title\":{\"text\":\"Mean cluster score\"}},\"yaxis\":{\"title\":{\"text\":\"rel. frequency\"}}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('f2e5e741-7519-47db-a237-b0cec9fc36a2');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["cluster_to_score = compute_cluster_scores()\n","plot_mean_cluster_scores(cluster_to_score).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":702,"status":"ok","timestamp":1690220234793,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"q9yKQZOjCzj8","outputId":"4792f2d0-2818-4b21-b8ac-ddb7cf87fece"},"outputs":[{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"b0d3237c-1af6-46e3-85eb-352a0f150b15\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b0d3237c-1af6-46e3-85eb-352a0f150b15\")) {                    Plotly.newPlot(                        \"b0d3237c-1af6-46e3-85eb-352a0f150b15\",                        [{\"histnorm\":\"probability\",\"name\":\"uniform\",\"opacity\":0.75,\"x\":[0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01],\"xbins\":{\"end\":1,\"size\":0.001,\"start\":-0.1},\"type\":\"histogram\"},{\"histnorm\":\"probability\",\"name\":\"linear\",\"opacity\":0.75,\"x\":[0.0,0.009950982198798513,0.010365606457081785,0.011701617955994548,0.01138332054559527,0.008820188767116864,0.01202410349021487,0.014134917896020617,0.008443257623222981,0.010780230715365057,0.010327913342692396,0.007840167792992769,0.008330178280054815,0.01115716185925894,0.009423278597347077,0.00980020974124096,0.01183563791826793,0.008443257623222981,0.00915942679662136,0.009423278597347077,0.020580440456606015,0.01179794480387854,0.0064455225605854006,0.009913289084409126,0.013192590036285907,0.01093100317292261,0.010290220228303008,0.008292485165665428,0.008970961224674417,0.007123998619594391,0.011307934316816494,0.013833372980905511,0.01609495984426881,0.007387850420320108,0.009649437283683407,0.00783639848155383,0.007463236649098885,0.009619282792171896,0.010324144031253457,0.009785132495485205,0.014636236317399479,0.01115339254782,0.010821693141193384,0.01886540375188885,0.013392363542549664,0.014553311465742827,0.008458334868978736,0.018575166771090557,0.009577820366343569,0.008831496701433681,0.011899716212729889,0.010572918586223421,0.008333947591493756,0.011775328935244907,0.008209560314008773,0.010614381012051748,0.00783639848155383,0.006841300261673978,0.007172999668300596,0.008831496701433681,0.007794936055725503,0.017953230383665652,0.014014299929974572,0.013682600523347957,0.008333947591493756,0.007753473629897175,0.010448531308738438,0.008085173036523792,0.010158294327940149,0.009619282792171896,0.007919323333210484,0.009950982198798513,0.01708251944127078,0.013226513839236356,0.010407068882910113,0.00696568753915896,0.010199756753768477,0.009992444624626841,0.007421774223270557,0.01011683190211182,0.010365606457081785,0.00667545055836067,0.013765525375004611,0.007214462094128921,0.00783639848155383,0.006633988132532342,0.006468138429219034,0.009287583385545278,0.008665646998120372,0.004602329266944313,0.008002248184867139,0.007877860907382156,0.01687520731212915,0.0070486123908156135,0.009204658533888625,0.011070467696163346,0.007919323333210484,0.006177901448420744,0.008043710610695464],\"xbins\":{\"end\":1,\"size\":0.001,\"start\":-0.1},\"type\":\"histogram\"},{\"histnorm\":\"probability\",\"name\":\"softmax\",\"opacity\":0.75,\"x\":[1.2891579410413389e-11,2.098166318003357e-06,3.4592914379587035e-06,1.7325421824116924e-05,1.1802796662106352e-05,5.36562309860962e-07,2.5560898497627332e-05,0.000325876451640049,3.405756390994728e-07,5.703407375313346e-06,3.305571029792699e-06,1.6457469085511953e-07,2.9716085924945855e-07,8.985473662966147e-06,1.1103776081734131e-06,1.7493522902423561e-06,2.036445652665428e-05,3.405756390994728e-07,8.077669638270792e-07,1.1103776081734131e-06,0.773936880544693,1.9459521910563217e-05,3.061672208585997e-08,2.0049301774271347e-06,0.00010460116512924908,6.840644574270698e-06,3.1586814898291864e-06,2.8395593292450504e-07,6.435507429463407e-07,6.938898596697151e-08,1.0777142086303215e-05,0.00022653103676575093,0.0034639148586608957,9.538391605735218e-08,1.4585275767310648e-06,1.6382832165720725e-07,1.0446154992158247e-07,1.4064429428744373e-06,3.2905798037097026e-06,1.7178332896217457e-06,0.0005964907780875523,8.944723285518415e-06,5.995827324524767e-06,0.09783755510872288,0.00013309508284173704,0.0005397271755270024,3.4682455966993434e-07,0.06894495981231519,1.337849911143543e-06,5.439291831576208e-07,2.2000469221499115e-05,4.44181813006926e-06,2.9851466492802134e-07,1.8935979344038263e-05,2.569339531834013e-07,4.669555015500815e-06,1.6382832165720725e-07,4.934414223044132e-08,7.361281006329612e-08,5.439291831576208e-07,1.558383201269033e-07,0.03256729299541112,0.00028176229258702266,0.00018887091293804057,2.9851466492802134e-07,1.4823799556947215e-07,3.823108294380046e-06,2.2114510291937582e-07,2.694098880754444e-06,1.4064429428744373e-06,1.810582966526885e-07,2.098166318003357e-06,0.011396525205457697,0.00010896903740599183,3.6366531026670416e-06,5.7329714121381614e-08,2.832228284116146e-06,2.2057416055066358e-06,9.936690001435943e-08,2.562706127888066e-06,3.4592914379587035e-06,4.03995667283163e-08,0.00020873464024952028,7.738701954256111e-08,1.6382832165720725e-07,3.842925660905448e-08,3.146321420375821e-08,9.427668982140925e-07,4.453315497477299e-07,3.316198379299951e-09,2.0010036393686465e-07,1.7222797932601634e-07,0.008875622754303458,6.335913278854161e-08,8.5305076598981e-07,8.09352032271461e-06,1.810582966526885e-07,2.2171752313657034e-08,2.1035972898114887e-07],\"xbins\":{\"end\":1,\"size\":0.001,\"start\":-0.1},\"type\":\"histogram\"},{\"histnorm\":\"probability\",\"name\":\"softmax divf 1\",\"opacity\":0.75,\"x\":[0.0061073119489139975,0.009904638839099726,0.010106206470507915,0.01078402963334313,0.01061852679793136,0.009375108658760224,0.010954341018370093,0.012137501052095295,0.009204966209031757,0.010311876170723613,0.010087713874628552,0.008939137557360833,0.009154528164567645,0.010502478486163081,0.00965390205219833,0.009832342527404858,0.010854484456698609,0.009204966209031757,0.009530924081932877,0.00965390205219833,0.016601395091463717,0.010834622642562675,0.008353440974016281,0.009886515076843627,0.01159428581410065,0.010387698449261979,0.010069255116975562,0.009137776974068257,0.00944404297180697,0.008633418800321176,0.010579702246023762,0.011960958717630412,0.013350288772843477,0.008744816216918765,0.009760573923662759,0.008937500502598013,0.008776907249656834,0.009746283188408348,0.010085866477165601,0.009825141983949853,0.012436788498004697,0.010500555131441568,0.010332671953925199,0.015274020828955858,0.01170737908723478,0.012386777717978432,0.009211712244793712,0.015060129722549245,0.009726667585289311,0.00938026123277662,0.010888333138063715,0.010208523383900504,0.00915620497162671,0.010822723003295403,0.009101032170194449,0.01022911073731565,0.008937500502598013,0.008515638664179828,0.00865399904994439,0.00938026123277662,0.008919512674895276,0.014611823881119226,0.012066573563936958,0.011873652838671393,0.00915620497162671,0.008901561049925661,0.010147009597859026,0.009046191825061204,0.010004915048302691,0.009746283188408348,0.008973585058387799,0.009904638839099726,0.014006525995247529,0.011613413034095264,0.010126587483164742,0.008567262659381862,0.010025091788291628,0.009924613353783359,0.008759242635783655,0.009984778916504204,0.010106206470507915,0.008447290236297474,0.011921591911587925,0.008671451420889292,0.008937500502598013,0.008430289017524803,0.008362625625069531,0.009590459332415724,0.009304973150030356,0.007637826759206445,0.009009815303139075,0.008955524606043297,0.013866142861145405,0.008601852404837915,0.009551894203475785,0.010458330327753989,0.008973585058387799,0.008245518854858804,0.009027985242598043],\"xbins\":{\"end\":1,\"size\":0.001,\"start\":-0.1},\"type\":\"histogram\"},{\"histnorm\":\"probability\",\"name\":\"softmax divf 0.5\",\"opacity\":0.75,\"x\":[0.0035939199133295254,0.009452473674131191,0.009841119864954654,0.011205476610137753,0.010864174260907515,0.008468779164219582,0.011562205973251964,0.014194717357341253,0.008164180265084975,0.010245745561973937,0.009805137748386447,0.007699445140450299,0.008074955144240567,0.01062800596260928,0.008979950952393003,0.009314985589744093,0.01135237138677856,0.008164180265084975,0.008752622719136613,0.008979950952393003,0.026555675854655847,0.011310863676663238,0.006723555662530498,0.009417912565815489,0.012952577288072021,0.010396971553346844,0.009769287193340756,0.008045430661125813,0.008593777341958889,0.007181808251637906,0.01078487407223498,0.013784790531944592,0.017173131988364437,0.00736833837338775,0.009179497296522792,0.007696625347352192,0.007422517088808917,0.009152637045645706,0.009801546780564151,0.009301347253105,0.014903376932252504,0.010624113633056525,0.010287112069000317,0.022478894303085115,0.013206494367536662,0.014783759082831256,0.008176151201746898,0.021853732042238085,0.009115832486492557,0.008478090630202563,0.011423284371529677,0.010041394832452945,0.008077913545066335,0.011286031933241228,0.007980856228248864,0.010081936287184453,0.007696625347352192,0.006987191376142309,0.007216088894350142,0.008478090630202563,0.007665675698473564,0.020572025127739223,0.014029303742989831,0.013584288048758502,0.008077913545066335,0.007634850504238689,0.00992074599979269,0.007884965069337795,0.009644839368670343,0.009152637045645706,0.007758900020978368,0.009452473674131191,0.018902926753321288,0.012995348555670456,0.009880852723005708,0.007072164470214365,0.009683779757449194,0.009490637399385979,0.007392669683312781,0.009606055566876768,0.009841119864954654,0.006875479959416411,0.013694200778883971,0.0072452233668143995,0.007696625347352192,0.006847832298134679,0.00673834895835698,0.008862311367085167,0.00834254267331328,0.00562092506081875,0.007821678569328883,0.00772769995335181,0.018525909107435214,0.007129386527198696,0.00879118046625302,0.01053884211167113,0.007758900020978368,0.006550948216470644,0.007853258069282215],\"xbins\":{\"end\":1,\"size\":0.001,\"start\":-0.1},\"type\":\"histogram\"},{\"histnorm\":\"probability\",\"name\":\"softmax divf 0.25\",\"opacity\":0.75,\"x\":[0.0011284810568870877,0.007806357494438639,0.008461483777808887,0.010970291049660763,0.010312190545377702,0.006266125652048931,0.011679893603062302,0.017603984672895687,0.005823480878571866,0.009171589665619274,0.008399721397722234,0.005179362891794289,0.00569688850843533,0.009868725333206155,0.007045396061212105,0.007580919025672534,0.011259799671633172,0.005823480878571866,0.006693201510099171,0.007045396061212105,0.061612978053482384,0.011177611719108475,0.0039496216273584095,0.007749377036716993,0.014657836324325432,0.009444330889126116,0.008338409835919242,0.005655305565938642,0.006452465298641925,0.004506351813558625,0.010162197710401727,0.016601901212537407,0.025766563402196744,0.004743474893110089,0.007361990956056844,0.0051755698762979635,0.004813488020893137,0.007318969948345591,0.00839357000920361,0.007558736396239706,0.019405590132549225,0.009861498145441972,0.009245798520446297,0.044147627913015866,0.015238161918552748,0.019095332999259414,0.005840571052321382,0.041726188000426684,0.007260226252257146,0.006279912501613254,0.011400908484589621,0.008809384608039154,0.0057010635822745005,0.011128587185067205,0.005564888377861324,0.008880662801605092,0.0051755698762979635,0.004265429420801915,0.004549474469915371,0.006279912501613254,0.005134029590432124,0.03697528884301821,0.01719609038603992,0.016122457734496987,0.0057010635822745005,0.005092822716227425,0.008598964265862331,0.005431965844818915,0.008127322271386152,0.007318969948345591,0.0052596614955969625,0.007806357494438639,0.031218755473680377,0.014754800710525401,0.008529947047992297,0.004369806307425272,0.008193081785337218,0.007869520029086717,0.004774853885294773,0.008062090558058638,0.008461483777808887,0.004130128120775434,0.016384412187895898,0.0045862850232421025,0.0051755698762979635,0.004096978785165984,0.003967020813499521,0.006862012294844044,0.006080710910941737,0.0027604067044030155,0.005345119418627789,0.0052174462715140575,0.029985862209134104,0.0044408060421785795,0.006752302244871033,0.009703832229401847,0.0052596614955969625,0.0037494348867809623,0.005388367667342724],\"xbins\":{\"end\":1,\"size\":0.001,\"start\":-0.1},\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"barmode\":\"overlay\",\"title\":{\"text\":\"Probabilities of sampling from clusters based on mean scores\"},\"xaxis\":{\"title\":{\"text\":\"probability\"}},\"yaxis\":{\"title\":{\"text\":\"rel. frequency\"}}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('b0d3237c-1af6-46e3-85eb-352a0f150b15');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_probability_distribution(cluster_to_score, [\n","    (\"linear\", None, None), (\"softmax\", False, np.nan), (\"softmax\", True, 1), (\"softmax\", True, 0.5), (\"softmax\", True, 0.25)\n","], bin_size=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2505,"status":"ok","timestamp":1690221187401,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"__L_nvs-CyTl","outputId":"e50c8318-f423-4205-93e4-edf2ea9b78ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 527 good ligands\n","Sampled 5000 molecules, of which 5000 are unique\n","Combined dataframe has shape (10270, 1)\n","Sampled 5000 molecules, of which 5000 are unique\n","Combined dataframe has shape (10270, 1)\n"]}],"source":["good_ligands = pd.read_csv(scoring_config['path_to_good_ligands'])\n","print(f\"There are {len(good_ligands)} good ligands\")\n","gl_mult = int(np.ceil(N_SAMPLES/len(good_ligands)))\n","\n","training_sampled = sample_clusters_for_active_learning(cluster_to_score, n_samples=N_SAMPLES, config=sampling_config, probability_type=PROBABILITY_CALCULATION, divide=True if SOFTMAX_NORMALIZATION_TYPE == 'divide' else False, divide_factor=SOFTMAX_DIV_FACTOR)\n","df1 = combine_sampled_and_good_ligands(training_sampled, good_ligands['smiles'].values, good_ligand_multiplier=gl_mult,\n","                                 save_path=AL_TRAIN_PATH)\n","\n","diffusion_sampled = sample_clusters_for_active_learning({cluster_id: 1/100 for cluster_id in cluster_to_score.keys()}, n_samples=N_SAMPLES, config=sampling_config, probability_type='no_score', divide=True if SOFTMAX_NORMALIZATION_TYPE == 'divide' else False, divide_factor=SOFTMAX_DIV_FACTOR)\n","df2 = combine_sampled_and_good_ligands(diffusion_sampled, good_ligands['smiles'].values, good_ligand_multiplier=gl_mult,\n","                                 save_path=AL_DIFFUSION_PATH)"]},{"cell_type":"markdown","metadata":{"id":"0iAJS9sVH69y"},"source":["# Active Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1690045605006,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"i6CPhht2dnuM","outputId":"ee439955-9afa-4e70-919c-095fded1d5a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated molecules from previous round will be read from\n"," 2. Generation/smiles/model1_softsub_al1_temp1.0_processed.csv\n","... active learning will be performed on\n"," 6. ActiveLearning/training_sets/model1_softsub_al1_threshold11_softmax_sub_translated.csv\n","... model weights will be LOADED from\n"," 6. ActiveLearning/model_weights/model1_softsub_al1.pt\n","... model weights will be SAVED to\n"," 6. ActiveLearning/model_weights/model1_softsub_al2.pt\n","... dataset descriptors will be saved to\n"," 6. ActiveLearning/dataset_descriptors/model1_softsub_al1_threshold11_softmax_sub.yaml\n"]}],"source":["# @title Run this cell to ensure proper variables are set\n","assert MODE == 'Active Learning', \"Please go to the beginning of the notebook and set the MODE to Active Learning\"\n","# @markdown if true, weights will be loaded from 1. Pretraining/model_weights\n","# @markdown otherwise from 6. Active Learning/model_weights\n","LOAD_FROM_BASELINE = False #@param {type:\"boolean\"}\n","LOAD_CKPT_FNAME = \"model1_softsub_al1.pt\" #@param [\"GPT_pretrain_07_14_23:39_1end_ignore_moses+bindingdb.pt\", \"model1_softsub_al1.pt\"]\n","if LOAD_FROM_BASELINE:\n","    CONFIG_DICT['load_ckpt_path'] = f\"{PRETRAINING_PATH}model_weights/{LOAD_CKPT_FNAME}\"\n","else:\n","    CONFIG_DICT['load_ckpt_path'] = f\"{AL_PATH}model_weights/{LOAD_CKPT_FNAME}\"\n","\n","CONFIG_DICT[\"desc_path\"] = f\"{AL_PATH}dataset_descriptors/{AL_TRAIN_PATH.split('/')[-1][:-4]}.yaml\"\n","CONFIG_DICT['al_path'] = AL_TRAIN_PATH.split('.csv')[0]+'_translated.csv'\n","\n","print(\"Generated molecules from previous round will be read from\\n\", '/'.join(sampling_config['path_to_predicted'].split('/')[6:]))\n","print(\"... active learning will be performed on\\n\", '/'.join(CONFIG_DICT['al_path'].split('/')[6:]))\n","print('... model weights will be LOADED from\\n', '/'.join(CONFIG_DICT['load_ckpt_path'].split('/')[6:]))\n","print('... model weights will be SAVED to\\n', '/'.join(CONFIG_DICT['save_ckpt_path'].split('/')[6:]))\n","print('... dataset descriptors will be saved to\\n', '/'.join(CONFIG_DICT['desc_path'].split('/')[6:]))"]},{"cell_type":"markdown","metadata":{"id":"NtOQLpdXWE9w"},"source":["\n","### Translation Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsaWjfTyQ5ij"},"outputs":[],"source":["from tqdm import tqdm\n","from rdkit import Chem\n","\n","def get_mol(smile_string):\n","    mol = Chem.MolFromSmiles(smile_string)\n","    if mol is None:\n","        return None\n","    try:\n","        Chem.SanitizeMol(mol)\n","    except ValueError:\n","        return None\n","    return mol\n","\n","def fill_translation_table():\n","    smile_df = pd.read_csv(sampling_config['path_to_completions'])\n","    rdkit_to_predicted = {}\n","    pbar = tqdm(smile_df['smiles'], total=len(smile_df))\n","    for completion in pbar:\n","        if '~' not in completion: continue\n","        mol_string = completion[1:completion.index('~')]\n","        mol = get_mol(mol_string)\n","        if mol is None: continue\n","        canonic_smile = Chem.MolToSmiles(mol)\n","        rdkit_to_predicted[canonic_smile] = mol_string\n","    return rdkit_to_predicted\n","\n","def translate_dataset_for_al():\n","    rdkit_to_predicted = fill_translation_table()\n","    sampled = pd.read_csv(AL_TRAIN_PATH)\n","    translated = []\n","    for mol in sampled['smiles'].values:\n","        mol = rdkit_to_predicted[mol]\n","        translated.append(mol)\n","    pd.DataFrame({\"smiles\": translated}).to_csv(AL_TRAIN_PATH.split('.csv')[0]+'_translated.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"MB99Ldh4WGAs"},"source":["### Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TedCQM1XqhSP"},"outputs":[],"source":["translate_dataset_for_al()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":532,"status":"ok","timestamp":1690045755515,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"_JWTjsqFKQl5","outputId":"0161b517-7bbc-4ddb-f6b5-3f8db7357df7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading AL dataset from 6. ActiveLearning/training_sets/model1_softsub_al1_threshold11_softmax_sub_translated.csv\n","45 131\n"]}],"source":["al_train_dataset = load_data(CONFIG_DICT, mode='al', forced_block_size=131)[0]\n","print(al_train_dataset.vocab_size, al_train_dataset.block_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":812},"executionInfo":{"elapsed":82983,"status":"ok","timestamp":1690045840007,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"Nuh0rVnAKXlQ","outputId":"9f0db7fd-30d2-4616-d30c-196589ab1394"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_EXECUTABLE=python3\n"]},{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20230722_170957-k52umkdj</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/generative_ml/BetaPipeline/runs/k52umkdj' target=\"_blank\">model1_softsub_al2</a></strong> to <a href='https://wandb.ai/generative_ml/BetaPipeline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/generative_ml/BetaPipeline' target=\"_blank\">https://wandb.ai/generative_ml/BetaPipeline</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/generative_ml/BetaPipeline/runs/k52umkdj' target=\"_blank\">https://wandb.ai/generative_ml/BetaPipeline/runs/k52umkdj</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch=0\n"]},{"name":"stderr","output_type":"stream","text":["epoch 1 iter 20: train loss 0.38240. lr 2.926585e-05: 100%|██████████| 21/21 [00:05<00:00,  3.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=1\n"]},{"name":"stderr","output_type":"stream","text":["epoch 2 iter 20: train loss 0.34122. lr 2.713525e-05: 100%|██████████| 21/21 [00:02<00:00,  8.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=2\n"]},{"name":"stderr","output_type":"stream","text":["epoch 3 iter 20: train loss 0.31251. lr 2.381678e-05: 100%|██████████| 21/21 [00:02<00:00,  8.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=3\n"]},{"name":"stderr","output_type":"stream","text":["epoch 4 iter 20: train loss 0.33369. lr 1.963525e-05: 100%|██████████| 21/21 [00:02<00:00,  8.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=4\n"]},{"name":"stderr","output_type":"stream","text":["epoch 5 iter 20: train loss 0.31251. lr 1.500000e-05: 100%|██████████| 21/21 [00:02<00:00,  7.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=5\n"]},{"name":"stderr","output_type":"stream","text":["epoch 6 iter 20: train loss 0.32591. lr 1.036475e-05: 100%|██████████| 21/21 [00:02<00:00,  7.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=6\n"]},{"name":"stderr","output_type":"stream","text":["epoch 7 iter 20: train loss 0.33063. lr 6.183221e-06: 100%|██████████| 21/21 [00:02<00:00,  7.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=7\n"]},{"name":"stderr","output_type":"stream","text":["epoch 8 iter 20: train loss 0.30867. lr 3.000000e-06: 100%|██████████| 21/21 [00:02<00:00,  8.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=8\n"]},{"name":"stderr","output_type":"stream","text":["epoch 9 iter 20: train loss 0.27576. lr 3.000000e-06: 100%|██████████| 21/21 [00:02<00:00,  8.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=9\n"]},{"name":"stderr","output_type":"stream","text":["epoch 10 iter 20: train loss 0.28812. lr 3.000000e-06: 100%|██████████| 21/21 [00:02<00:00,  7.96it/s]\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_train_loss</td><td>█▆▄▃▃▂▂▁▁▁</td></tr><tr><td>learning_rate</td><td>██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_train_loss</td><td>▇█▇▆▆▅▆▅▅▅▅▄▄▃▃▃▃▃▃▂▃▃▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_train_loss</td><td>0.29657</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>step_train_loss</td><td>0.28812</td></tr><tr><td>train_step</td><td>209</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">model1_softsub_al2</strong> at: <a href='https://wandb.ai/generative_ml/BetaPipeline/runs/k52umkdj' target=\"_blank\">https://wandb.ai/generative_ml/BetaPipeline/runs/k52umkdj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230722_170957-k52umkdj/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["model, trainer, wandb = train_GPT(\n","                train_dataset = al_train_dataset,\n","                config_dict = CONFIG_DICT,\n","                load_ckpt=True\n","        )\n","wandb.finish()\n","#GK wandb API Key: c99c9a01523f93287716691fa3360b1f4566e115\n","#RB wandb API Key: 4d3d628c6b5a4b3554c7a89ea50df8a4a6be0f85\n","#AM wandb API key: 5be14d5930441de4707f6a58e4f7c2e229dab1d1"]},{"cell_type":"markdown","metadata":{"id":"4CXn_Jqt_76M"},"source":["#BERT"]},{"cell_type":"markdown","metadata":{"id":"EJqiexX6AAsv"},"source":["##Set up notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A23L1dGiAO0V"},"outputs":[],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# install necessary packages\n","!pip install rdkit\n","!pip install pandas==1.5.3\n","!pip install molsets\n","!pip install wandb\n","\n","# clone Sophia optimizer GitHub repository\n","!git clone https://github.com/Liuhong99/Sophia.git\n","\n","# import necessary packages\n","import numpy as np\n","import h5py\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, pairwise_distances\n","import os\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","import pkg_resources\n","pkg_resources.require(\"pandas==1.5.3\")\n","import pandas as pd\n","import re\n","import math\n","import random\n","import logging\n","import wandb\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","from tqdm import tqdm\n","import torch.optim as optim\n","from torch.cuda.amp import GradScaler\n","from rdkit import Chem\n","from rdkit.Chem import QED, Crippen\n","from rdkit.Contrib.SA_Score import sascorer\n","from rdkit.Chem.rdMolDescriptors import CalcTPSA\n","from rdkit.Chem.Fingerprints import FingerprintMols\n","from rdkit.DataStructs.cDataStructs import TanimotoSimilarity\n","from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmiles\n","import moses\n","from moses.utils import get_mol\n","from Sophia.sophia import SophiaG\n","import yaml\n","\n","# set random seed for reproducibility\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)"]},{"cell_type":"markdown","metadata":{"id":"0gbEP-i0Av6M"},"source":["##Utils & Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FK7qHr5Axl3"},"outputs":[],"source":["@torch.no_grad()\n","def sample(model, x, steps, temperature=1.0, prop=None, scaffold=None):\n","    block_size = model.get_block_size() # define size of context window used for input conditioning\n","    model.eval()\n","    for k in range(steps):\n","        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # limit conditioning input to the most recent block_size elements\n","        logits, _= model(x_cond, prop = prop, scaffold = scaffold) # give input to model and get logits (unnormalized scores or probabilities)\n","        logits = logits[:, -1, :] / temperature # extract the logits for the next token in the sequence\n","        probs = F.softmax(logits, dim=-1)\n","        ix = torch.multinomial(probs, 1)\n","        x = torch.cat((x, ix), dim=1) # concatenate the chosen token index with the existing sequence\n","    return x\n","\n","\n","\n","@torch.no_grad()\n","def fill_mask(model, x, temperature=1.0, prop=None, scaffold=None):\n","    block_size = model.get_block_size()  # Get model's maximum context length\n","    model.eval()\n","\n","    # Check if mask token is present\n","    mask_index = model.config.mask_index  # Assuming 'mask_index' is available in your model's config\n","    while mask_index in x[0]:  # If mask token is present\n","        # Find all mask positions\n","        mask_positions = (x[0] == mask_index).nonzero(as_tuple=True)\n","\n","        for pos in mask_positions:  # Iterate over each mask position\n","            # Limit conditioning input to the most recent block_size elements\n","            x_cond = x if x.size(1) <= block_size else x[:, -block_size:]\n","            # Get token probabilities\n","            logits, _ = model(x_cond, prop=prop, scaffold=scaffold)\n","\n","            # Extract the logits for the masked token position, apply temperature and softmax to get probabilities\n","            logits = logits[0, pos, :] / temperature\n","            probs = F.softmax(logits, dim=-1)\n","            ix = torch.multinomial(probs, 1)  # Sample token from probability distribution\n","\n","            x[0, pos] = ix  # Replace mask token with sampled token\n","\n","    return x  # Return unmasked tensor\n","\n","def check_novelty(gen_smiles, train_smiles):\n","    if len(gen_smiles) == 0:\n","        novel_ratio = 0\n","    else:\n","        duplicates = [1 for mol in gen_smiles if mol in train_smiles]\n","        novel = len(gen_smiles) - sum(duplicates)\n","        novel_ratio = novel*100/len(gen_smiles)\n","    return novel_ratio\n","\n","def canonic_smiles(smiles_or_mol):\n","    mol = get_mol(smiles_or_mol)\n","    if mol is None:\n","        return None\n","    return Chem.MolToSmiles(mol)\n","\n","class SMILESDataset(Dataset):\n","\n","    def __init__(self, data=None, content=None, block_size=None, prop=None, scaffold=None, scaffold_maxlen=None, len_data=None, mask_prob=0.15):\n","        if content is None:\n","            self.desc_only = True\n","            return\n","        self.desc_only = False\n","        self.chars = sorted(list(set(content)))\n","        data_size, vocab_size = len(content), len(self.chars)\n","        self.vocab_size = vocab_size\n","\n","        self.stoi = {ch:i for i,ch in enumerate(self.chars)}\n","        self.itos = {i:ch for i,ch in enumerate(self.chars)}\n","        self.max_len = block_size\n","        self.data = data\n","        self.prop = prop\n","        self.sca = scaffold\n","        self.scaf_max_len = scaffold_maxlen\n","        self.len_data = len_data\n","        self.mask_prob = mask_prob\n","\n","        # add X token to vocabulary\n","        self.stoi['X'] = len(self.stoi)\n","        self.itos[len(self.itos)] = 'X'\n","        self.vocab_size = len(self.stoi)\n","\n","    def export_desc_attributes(self, export_path):\n","        attr_dict = {\n","            \"desc_only\": self.desc_only,\n","            \"vocab_size\": self.vocab_size,\n","            \"max_len\": self.max_len,\n","            \"stoi\": self.stoi,\n","            \"itos\": self.itos,\n","            \"scaf_max_len\": self.scaf_max_len,\n","            \"len_data\": self.len_data\n","        }\n","        with open(export_path, 'w') as f:\n","            yaml.dump(attr_dict, f)\n","\n","    def load_desc_attributes(self, load_path):\n","        with open(load_path, 'r') as f:\n","            attr_dict = yaml.load(f, Loader=yaml.SafeLoader)\n","        self.__dict__.update(attr_dict)\n","\n","    def __len__(self):\n","        assert not self.desc_only, \"Dataset is not initialized\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        assert not self.desc_only, \"Dataset is not initialized\"\n","        smiles, prop, scaffold = self.data[idx].strip(), self.prop[idx], self.sca[idx]\n","        if scaffold:\n","          scaffold=scaffold.strip()\n","        pattern =  \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]|X)\"\n","        regex = re.compile(pattern)\n","\n","        #tokenize input string\n","        tokens_arr=regex.findall(smiles)\n","\n","        #Save ground truth encodings before token masking\n","        smiles_true = smiles+str('<')*(self.max_len - len(tokens_arr))\n","        if len(tokens_arr) > self.max_len:\n","            smiles_true = smiles[:self.max_len]\n","        dix_true=[self.stoi[s] for s in regex.findall(smiles_true)]\n","\n","\n","\n","        # randomly replace tokens\n","        mask_idx = [] #True indicates that the corresponding position will be ignored for computing loss\n","        for s in range(len(tokens_arr)):\n","          if random.random() < .15:\n","            mask_idx.append(False)\n","            num = random.random()\n","            if num >= .2: # 80%\n","                tokens_arr[s]='X'\n","            elif num >= 0.1: # 10%\n","                tokens_arr[s] = self.chars[int(random.random()*len(self.chars))]\n","          else:\n","            mask_idx.append(True)\n","\n","        #pad the mask appropriately\n","        while len(mask_idx)<self.max_len:\n","            mask_idx.append(True)\n","\n","        assert len(mask_idx)==self.max_len\n","        smiles=''.join(tokens_arr)\n","\n","        smiles += str('<')*(self.max_len - len(regex.findall(smiles)))\n","        if len(regex.findall(smiles)) > self.max_len:\n","            smiles = smiles[:self.max_len]\n","\n","        assert len(''.join(regex.findall(smiles)))==len(smiles)\n","        smiles=regex.findall(smiles)\n","\n","\n","        dix =  [self.stoi[s] for s in smiles]\n","        if scaffold:\n","          scaffold += str('<')*(self.scaf_max_len - len(regex.findall(scaffold)))\n","          if len(regex.findall(scaffold)) > self.scaf_max_len:\n","              scaffold = scaffold[:self.scaf_max_len]\n","          scaffold=regex.findall(scaffold)\n","          sca_dix = [self.stoi[s] for s in scaffold]\n","          sca_tensor = torch.tensor(sca_dix, dtype=torch.long)\n","        else:\n","          sca_tensor=torch.tensor(scaffold,dtype=torch.bool)\n","        x = torch.tensor(dix, dtype=torch.long)\n","        y = torch.tensor(dix_true, dtype=torch.long)\n","        prop = torch.tensor([prop], dtype=torch.float)\n","        return x, y, prop, sca_tensor, torch.tensor(mask_idx)"]},{"cell_type":"markdown","metadata":{"id":"VXLdZWy9A5Tk"},"source":["##Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLo8qZ7yA4SF"},"outputs":[],"source":["class GPTConfig:\n","    def __init__(self, vocab_size=None, block_size=None, mask_index=None, **kwargs):\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        self.mask_index = mask_index\n","        for k,v in kwargs.items():\n","            setattr(self, k, v)\n","\n","    def export_attributes(self, export_path):\n","        with open(export_path, 'w') as f:\n","            yaml.dump(vars(self), f)\n","\n","    def load_attributes(self, load_path):\n","        with open(load_path, 'r') as f:\n","            config_dict = yaml.load(f, Loader=yaml.SafeLoader)\n","        self.__dict__.update(config_dict)\n","\n","class SelfAttention(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embed % config.n_head == 0\n","        self.config = config\n","\n","        self.query = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n","        self.key = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n","        self.value = nn.Linear(config.n_embed, config.n_embed, bias=config.att_bias)\n","\n","        self.attn_drop = nn.Dropout(config.att_drop_rate)\n","        self.resid_drop = nn.Dropout(config.att_drop_rate)\n","\n","        self.proj = nn.Linear(config.n_embed, config.n_embed)\n","        self.n_head = config.n_head\n","\n","        num = int(bool(config.num_props)) + int(config.scaffold_maxlen)\n","        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size + num, config.block_size + num))\n","                                .view(1, 1, config.block_size + num, config.block_size + num))\n","\n","    def forward(self, x, layer_past=None):\n","        B, T, C = x.size()\n","        # apply attention functions to get tensors with dimensions (B, n_head, T, head_size)\n","        q = self.query(x).view(B, T, self.n_head, C // self.n_head)\n","        k = self.key(x).view(B, T, self.n_head, C // self.n_head)\n","        v = self.value(x).view(B, T, self.n_head, C // self.n_head)\n","        if self.config.do_flash:\n","            q = q.transpose(1, 2)\n","            k = k.transpose(1, 2)\n","            v = v.transpose(1, 2)\n","            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.config.att_drop_rate if self.training else 0, is_causal=self.config.is_causal)\n","            y = y.transpose(1, 2)\n","        else:\n","            # (B h T s) @ (B h s T) -> (B h T T)\n","            att = torch.einsum('bths,bihs->bhti', q, k) / math.sqrt(k.size(-1))\n","            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            # (B h T T) @ (B h T s) -> (B h T s)\n","            y = torch.einsum('bhtq,bqhs->bths', att, v)\n","            self.att_weights = att\n","        self.attended = y\n","        y = y.contiguous().view(B, T, C)\n","        y = self.resid_drop(self.proj(y))\n","        self.out = y\n","        return y\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(config.n_embed)\n","        self.ln2 = nn.LayerNorm(config.n_embed)\n","        self.attn = SelfAttention(config)\n","        self.mlp = nn.Sequential(nn.Linear(config.n_embed, config.ff_mult*config.n_embed), nn.GELU() if config.doGELU else nn.ReLU(),\n","            nn.Linear(config.ff_mult*config.n_embed, config.n_embed), nn.Dropout(config.att_drop_rate))\n","\n","    def forward(self, x):\n","        y = self.attn(self.ln1(x))\n","        x = x + y # perform a residual connection by summing input and attention output\n","        x = x + self.mlp(self.ln2(x)) # apply layer normalization and then MLP, create a residual connection with input\n","        return x\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embed)\n","        self.type_emb = nn.Embedding(2, config.n_embed)\n","        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embed))\n","        # if conditioning on at least 1 property:\n","        if config.num_props:\n","            # initialize property linear layer, map property vector to embedding dimension\n","            self.prop_nn = nn.Linear(config.num_props, config.n_embed)\n","\n","        self.drop = nn.Dropout(config.gpt_drop_rate)\n","        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n","\n","        self.ln_f = nn.LayerNorm(config.n_embed)\n","        self.head = nn.Linear(config.n_embed, config.vocab_size, bias=config.gpt_bias)\n","        self.block_size = config.block_size # define the context size\n","        self.apply(self._init_weights) # initialize weights and apply to all relevant modules in the model\n","\n","    def get_block_size(self):\n","        return self.block_size\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            module.weight.data.normal_(mean=0.0, std=0.02)\n","            if isinstance(module, nn.Linear) and module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def configure_optimizers(self, train_config):\n","        decay, no_decay = set(), set()\n","        no_decay = set()\n","\n","        whitelist_weight_modules = (torch.nn.Linear)\n","        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n","        # for named module of the model:\n","        for mn, m in self.named_modules():\n","            # for named parameter of each module:\n","            for pn, p in m.named_parameters():\n","                # construct full parameter name by concatenating module name and parameter name, separated by a dot\n","                fpn = '%s.%s' % (mn, pn) if mn else pn\n","                if pn.endswith('bias') or ('bias' in pn):\n","                    no_decay.add(fpn)\n","                elif (pn.endswith('weight') or ('weight' in pn)) and isinstance(m, whitelist_weight_modules):\n","                    decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n","                    no_decay.add(fpn)\n","        no_decay.add('pos_emb')\n","        param_dict = {pn:p for pn, p in self.named_parameters()}\n","        assert len(decay & no_decay) == 0\n","        # assert that all parameters from both sets have been correctly separated\n","        assert len(param_dict.keys() - (decay | no_decay)) == 0\n","        optim_groups = [{\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n","                        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}]\n","        optimizer = SophiaG(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, rho=train_config.rho, weight_decay=train_config.weight_decay)\n","        return optimizer\n","\n","    def forward(self, idx, targets=None, prop=None, scaffold=None):\n","        b, t = idx.size()\n","        assert t <= self.block_size\n","        if self.config.num_props:\n","            assert prop.size(-1) == self.config.num_props, f\"number of properties {prop.size(-1)=} doesn't match the expected size {self.config.num_props=}\"\n","        token_embeddings = self.tok_emb(idx)\n","        position_embeddings = self.pos_emb[:, :t, :]\n","        type_embeddings = self.type_emb(torch.ones((b,t), dtype=torch.long, device=idx.device))\n","        x = self.drop(token_embeddings + position_embeddings + type_embeddings)\n","        # Condition on properties\n","        if self.config.num_props:\n","            type_embd = self.type_emb(torch.zeros((b, 1), dtype=torch.long, device=idx.device))\n","            if prop.ndim == 2:\n","                p = self.prop_nn(prop.unsqueeze(1))\n","            else:\n","                p = self.prop_nn(prop)\n","            p += type_embd\n","            x = torch.cat([p, x], 1)\n","        # Condition on scaffold\n","        if self.config.scaffold:\n","            type_embd = self.type_emb(torch.zeros((b, 1), dtype = torch.long, device = idx.device))\n","            scaffold_embeds = self.tok_emb(scaffold)\n","            scaffold_embeds += type_embd\n","            x = torch.cat([scaffold_embeds, x], 1)\n","\n","        # Transformer blocks\n","        for layer in self.blocks:\n","            x = layer(x)\n","        x = self.ln_f(x)\n","        logits = self.head(x)\n","\n","        if self.config.num_props and self.config.scaffold:\n","            num = int(bool(self.config.num_props)) + int(self.config.scaffold_maxlen)\n","        elif self.config.num_props:\n","            num = int(bool(self.config.num_props))\n","        elif self.config.scaffold:\n","            num = int(self.config.scaffold_maxlen)\n","        else:\n","            num = 0\n","        # Slice the logits tensor along the second dimension to exclude the first num elements\n","        logits = logits[:, num:, :]\n","\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"6Jia_2uzBQOp"},"source":["##Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"It9Ei0OYBSey"},"outputs":[],"source":["class TrainerConfig:\n","    epochs = 10\n","    batch_size = 64\n","    learning_rate = 3e-4\n","    betas = (0.965, 0.99) #(0.9, 0.95)\n","    rho = 0.04 # For SophiaG\n","    weight_decay = 0.1\n","\n","    lr_decay = False\n","    warmup_tokens = 375e6 # number of warm-up tokens for learning rate decay\n","    final_tokens = 260e9 # number of tokens at which the learning rate decays to 10% of the original\n","    num_workers = 0 # number of worker processes to use for loading data\n","\n","    def __init__(self, **kwargs):\n","        for k,v in kwargs.items():\n","            setattr(self, k, v)\n","\n","def loss_function(logits, y, mask_idx):\n","    loss=nn.CrossEntropyLoss(ignore_index=-1)\n","    y[mask_idx]=-1\n","    return loss(logits.view(-1,logits.size(-1)),y.view(-1))\n","\n","class Trainer:\n","\n","    def __init__(self, model, train_dataset, test_dataset, config, stoi, itos):\n","        self.model = model\n","        self.train_dataset = train_dataset\n","        self.test_dataset = test_dataset\n","        self.config = config\n","        self.stoi = stoi\n","        self.itos = itos\n","        self.model = self.model.to(config.device)\n","\n","    def train(self, wandb):\n","        model, config = self.model, self.config\n","        optimizer = model.configure_optimizers(config)\n","        scaler = GradScaler() # define variable used for gradient scaling in mixed-precision training\n","        self.tokens = 0 # initialize a counter used for learning rate decay\n","\n","        def run_epoch(split):\n","            is_train = split == 'train'\n","            model.train(is_train)\n","            data = self.train_dataset if is_train else self.test_dataset\n","            loader = DataLoader(data, shuffle=True, pin_memory=True, batch_size=config.batch_size, num_workers=config.num_workers)\n","            losses = []\n","            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n","            # for batch index, batch in progress bar:\n","            for it, (x, y, p, scaffold, mask_idx) in pbar:\n","                # move the input data tensor, target data tensor, property tensor, and scaffold tensor to GPU\n","                x, y, p, scaffold , mask_idx = x.to(config.device), y.to(config.device), p.to(config.device), scaffold.to(config.device), mask_idx.to(config.device)\n","                # allow model to use lower-precision computations for improved memory usage\n","                if config.device == 'cuda':\n","                    with torch.cuda.amp.autocast():\n","                        with torch.set_grad_enabled(is_train):\n","                            logits= model(x, y, p, scaffold)\n","                            loss = loss_function(logits, y, mask_idx)\n","                            loss = loss.mean()\n","                            losses.append(loss.item())\n","                else:\n","                    with torch.cpu.amp.autocast():\n","                        with torch.set_grad_enabled(is_train):\n","                            logits = model(x, y, p, scaffold)\n","                            loss = loss_function(x, y, mask_idx)\n","                            loss = loss.mean()\n","                            losses.append(loss.item())\n","\n","                if is_train:\n","                    model.zero_grad()\n","                    scaler.scale(loss).backward()\n","                    scaler.unscale_(optimizer) # unscale the gradients of the optimizer's parameters to their original values\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients of model parameters to prevent them from exploding, setting maximum gradient norm to be 1.0\n","                    scaler.step(optimizer) # update the optimizer's parameters based on calculated gradients\n","                    scaler.update() # update the scale factor of the gradient scaler\n","                    if config.lr_decay:\n","                        self.tokens += (y >= 0).sum() # increment the number of processed tokens by the count of valid tokens (not padding or special tokens)\n","                        if self.tokens < config.warmup_tokens:\n","                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens)) # perform a linear warm-up\n","                        else:\n","                            # calculate the progress of training in terms of the number of tokens processed\n","                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n","                            # calculate the scaling factor for the learning rate (between 0.1 and 1.0)\n","                            # to gradually reduce learning rate as training progresses\n","                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n","                        lr = config.learning_rate * lr_mult # multiply the base learning rate by the scaling factor to obtain the updated learning rate\n","                        for param_group in optimizer.param_groups:\n","                            param_group['lr'] = lr\n","                    else:\n","                        lr = config.learning_rate\n","                    # log training progress using Weights & Biases\n","                    if wandb is not None:\n","                        wandb.log({'step_train_loss': loss, 'train_step': it + epoch*len(loader), 'learning_rate': lr})\n","                    # update the description of the progress bar with epoch, iteration, and training loss\n","                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n","            return float(np.mean(losses))\n","\n","\n","        # initialize best loss as infinity\n","        best_loss = float('inf')\n","        for epoch in range(config.epochs):\n","            print(f'{epoch=}')\n","            train_loss = run_epoch('train')\n","            if self.test_dataset is not None:\n","                test_loss = run_epoch('test')\n","            if wandb is not None:\n","                wandb.log({'epoch_valid_loss': test_loss, 'epoch_train_loss': train_loss, 'epoch': epoch + 1})\n","            good_model = self.test_dataset is None or test_loss < best_loss\n","            if good_model:\n","                best_loss = test_loss\n","                torch.save(self.model.state_dict(), self.config.ckpt_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gthVEc2ZBX2u"},"outputs":[],"source":["def load_data(train_config_dict):\n","    if (cut:=train_config_dict[\"slice_data\"]):\n","        train_data = pd.read_csv(train_config_dict[\"train_path\"])[:cut]\n","        val_data = pd.read_csv(train_config_dict[\"val_path\"])[:cut]\n","    else:\n","        train_data = pd.read_csv(train_config_dict[\"train_path\"])\n","        val_data = pd.read_csv(train_config_dict[\"val_path\"])\n","\n","    smiles = train_data['smiles']\n","    vsmiles = val_data['smiles']\n","\n","    prop = train_data[train_config_dict[\"props\"]].values.tolist()\n","    vprop = val_data[train_config_dict[\"props\"]].values.tolist()\n","\n","    scaffold = train_data['scaffold_smiles']\n","    vscaffold = val_data['scaffold_smiles']\n","\n","    # define a regular expression that matches molecular tokens in SMILES strings\n","    pattern = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]|X)\"\n","    # compile pattern into a regular expression object that can be used for matching operations\n","    regex = re.compile(pattern)\n","\n","    context = {'<'}\n","\n","    max_len, scaffold_max_len = 0, 0\n","    for iterator in (smiles.values, vsmiles.values):\n","        for i in iterator:\n","            chars = regex.findall(i.strip())\n","            max_len = max(max_len, len(chars))\n","            for char in chars:\n","                context.add(char)\n","    for iterator in (scaffold.values, vscaffold.values):\n","        for i in iterator:\n","            chars = regex.findall(i.strip())\n","            scaffold_max_len = max(scaffold_max_len, len(chars))\n","            for char in chars:\n","                context.add(char)\n","    print(max_len)\n","    context = sorted(list(context))\n","\n","    smiles = [i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in smiles]\n","    vsmiles = [i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in vsmiles]\n","    scaffold = [i + str('<')*(scaffold_max_len - len(regex.findall(i.strip()))) for i in scaffold]\n","    vscaffold = [i + str('<')*(scaffold_max_len - len(regex.findall(i.strip()))) for i in vscaffold]\n","\n","    # if not conditioning on scaffolds: define 'scaffold' as a list of length SMILES string filled with 'False' values\n","    scaffold=[False]*len(smiles) if not train_config_dict[\"use_scaf\"] else scaffold\n","    train_dataset = SMILESDataset(smiles, context, max_len, prop=prop, scaffold=scaffold, scaffold_maxlen=scaffold_max_len, len_data=len(train_data), mask_prob=0.15)\n","    valid_dataset = SMILESDataset(vsmiles, context, max_len, prop=vprop, scaffold=vscaffold, scaffold_maxlen=scaffold_max_len, len_data=len(val_data), mask_prob=0.15)\n","    train_dataset.export_desc_attributes(train_config_dict[\"desc_path\"])\n","    return train_dataset, valid_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWAIfqxj9to1"},"outputs":[],"source":["def pretrain_BERT(train_dataset, valid_dataset, model_config_dict, train_config_dict):\n","  \"\"\"\n","  OUTPUTS:\n","  1) checkpoint of trained model parameters\n","  2) Weights & Biases logged run\n","  \"\"\"\n","\n","  mask_index = train_dataset.stoi['X']\n","  mconf = GPTConfig(train_dataset.vocab_size, train_dataset.max_len, mask_index=mask_index, num_props=len(train_config_dict[\"props\"]), scaffold=train_config_dict[\"use_scaf\"], scaffold_maxlen=train_dataset.scaf_max_len, **model_config_dict)\n","  model = GPT(mconf).to(model_config_dict[\"device\"])\n","\n","  if train_config_dict['load_cpt'] != None:\n","    model.load_state_dict(torch.load(train_config_dict['load_cpt']))\n","  torch.compile(model)\n","\n","  tconf = TrainerConfig(warmup_tokens=0.1*train_dataset.len_data*train_dataset.max_len, final_tokens=train_config_dict[\"epochs\"]*train_dataset.len_data*train_dataset.max_len, block_size=train_dataset.max_len, **train_config_dict)\n","  trainer = Trainer(model, train_dataset, valid_dataset, tconf, train_dataset.stoi, train_dataset.itos)\n","\n","  %env WANDB_EXECUTABLE=python3\n","  wandb.init(project=\"mol_transformer\", name=train_config_dict[\"wandb_runname\"])\n","  trainer.train(wandb=wandb)\n","  return model, tconf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eApgFW7e9vd6"},"outputs":[],"source":["DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n","BASE = '/content/drive/MyDrive/Generative_ML/'\n","\n","model_config_dict = {\n","    \"device\": DEVICE,\n","    \"att_bias\": False,\n","    \"gpt_bias\": True,\n","    \"att_drop_rate\": 0.1,\n","    \"gpt_drop_rate\": 0.1,\n","    \"n_layer\": 8,\n","    \"n_head\": 8,\n","    \"n_embed\": 256,\n","    \"ff_mult\": 4, # multiplier for FF inside multihead,\n","    \"doGELU\": True, # else ReLU\n","    \"attention_times\": [],\n","    \"do_flash\": True,\n","    \"is_causal\": False\n","}\n","\n","train_config_dict = {\n","    \"desc_path\": BASE + 'checkpoints/descriptors_10k.yaml',\n","    \"train_path\": BASE + 'data/MOSES_processed_train.csv',\n","    \"val_path\": BASE + 'data/MOSES_processed_val.csv',\n","    \"slice_data\": False,\n","    \"ckpt_path\": BASE + 'checkpoints/7-03_all_mask.pt',\n","    \"wandb_runname\": \"7_03_all_mask\",\n","    \"use_scaf\": False,\n","    \"props\": [],\n","    \"device\": DEVICE,\n","    \"epochs\": 10,\n","    \"batch_size\": 512,\n","    \"lr_decay\": True,\n","    \"num_workers\": 0,\n","    \"load_cpt\": None\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xk6icEUs9xFh"},"outputs":[],"source":["train_dataset, val_dataset = load_data(train_config_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gg3xxdXyRLZ3"},"outputs":[],"source":["# define function to train BERT model\n","model, tconf = pretrain_BERT(\n","                train_dataset = train_dataset,\n","                valid_dataset = val_dataset,\n","                model_config_dict = model_config_dict,\n","                train_config_dict = train_config_dict\n","        )\n","\n","#GK wandb API Key: c99c9a01523f93287716691fa3360b1f4566e115\n","#RB wandb API Key: 4d3d628c6b5a4b3554c7a89ea50df8a4a6be0f85"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjaK3DzoSLB5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ywY37VNWSLS9"},"source":["##Generation & Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChKsmt1qSLS9"},"outputs":[],"source":["def generate_SMILES(model_config_dict, inference_config_dict):\n","  props = inference_config_dict[\"props\"]\n","  scaffold = inference_config_dict[\"scaffold\"]\n","\n","  # define a regular expression that matches molecular tokens in SMILES strings\n","  pattern = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]|X)\"\n","  regex = re.compile(pattern)\n","\n","  dataset = SMILESDataset()\n","  dataset.load_desc_attributes(inference_config_dict['desc_path'])\n","  use_scaf = False if scaffold is None else True\n","\n","  mconf = GPTConfig(dataset.vocab_size, dataset.max_len, num_props=len(props), scaffold=use_scaf, scaffold_maxlen=dataset.scaf_max_len, **model_config_dict)\n","  model = GPT(mconf).to(model_config_dict['device'])\n","  torch.compile(model)\n","\n","  # load parameters into the model\n","  model.load_state_dict(torch.load(inference_config_dict[\"model_params\"], map_location=torch.device(model_config_dict['device'])))\n","  block_size = model.get_block_size() #inference_config_dict[\"block_size\"]\n","  assert block_size == dataset.max_len, \"Warning: model block size and dataset block size are different\"\n","  # calculate number of generation iterations from total number of SMILES to generate and batch size\n","  gen_iter = math.ceil(inference_config_dict[\"gen_size\"] / inference_config_dict[\"batch_size\"])\n","  stoi = dataset.stoi # define dictionary to map strings to integers\n","  itos = dataset.itos # define dictionary to map integers to strings\n","  # is a scaffold is defined for conditioning:\n","  if scaffold is not None:\n","      # pad '<' to end of scaffold string to achieve maximum scaffold length\n","      scaffold += str('<')*(dataset.scaf_max_len - len(regex.findall(scaffold)))\n","      # convert the scaffold SMILES to a tensor of integers and repeat along the batch dimension, move to GPU\n","      scaffold=torch.tensor([stoi[s] for s in regex.findall(scaffold)])[None,...].repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n","\n","  if props is None:\n","    p = None\n","  elif len(props) == 1:\n","    # create a tensor for conditioning with a single property value\n","    p = torch.tensor([[props[0]]]).repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n","  else:\n","    # create a tensor for conditioning with multiple property values\n","    p = torch.tensor([props]).repeat(inference_config_dict[\"batch_size\"], 1).unsqueeze(1).to(model_config_dict['device'])\n","\n","  molecules = []\n","  for i in tqdm(range(gen_iter)):\n","          # create an input tensor by converting 'context' to a tensor of token indices,\n","          # repeat this batch times along the batch dimension\n","          x = torch.tensor([stoi[s] for s in regex.findall(inference_config_dict[\"context\"])], dtype=torch.long)[None,...].repeat(inference_config_dict[\"batch_size\"], 1).to(model_config_dict['device'])\n","\n","          if ['X'] in inference_config_dict['context']:\n","            # call sample function to generate molecules conditioned on the input\n","            y = fill_mask(model, x, block_size, temperature=inference_config_dict[\"temp\"], prop=p, scaffold=scaffold)\n","          else:\n","            y = sample(model, x, block_size, temperature=inference_config_dict[\"temp\"], prop=p, scaffold=scaffold)\n","\n","          # for each generated molecule:\n","          for gen_mol in y:\n","                  # convert generated molecule from list of integers to list of strings and concatenate to one string\n","                  completion = ''.join([itos[int(i)] for i in gen_mol])\n","                  # remove padding tokens\n","                  completion = completion.replace('<', '')\n","                  # convert the string representation of the molecule to an rdkit Mol object\n","                  mol = get_mol(completion)\n","                  # if an rdkit Mol object was created:\n","                  if mol:\n","                          # append the Mol object to the list\n","                          molecules.append(mol)\n","  # create dataframe where first column contains rdkit Mols and second column contins SMILES\n","  results = pd.DataFrame([{'molecule' : i, 'smiles': Chem.MolToSmiles(i)} for i in molecules])\n","  # iterate over each SMILES and ensure that equivalent molecules have same SMILES\n","  canon_smiles = [canonic_smiles(s) for s in results['smiles']]\n","  # create set of unique SMILES strings\n","  unique_smiles = list(set(canon_smiles))\n","  data = pd.read_csv(inference_config_dict[\"train_data\"]) # load training data\n","  novel_ratio = check_novelty(unique_smiles, set(data['smiles'])) # calculate novelty ratio from generated SMILES and training SMILES\n","  results['qed'] = results['molecule'].apply(lambda x: QED.qed(x)) # quantitative estimate of drug-likeliness (QED)\n","  results['sas'] = results['molecule'].apply(lambda x: sascorer.calculateScore(x)) #synthetic accessibility score (SAS)\n","  results['logp'] = results['molecule'].apply(lambda x: Crippen.MolLogP(x)) #(measure of hydrophobicity)\n","  results['tpsa'] = results['molecule'].apply(lambda x: CalcTPSA(x)) #topological polar surface area (TPSA)\n","  results['validity'] = np.round(len(results)/(inference_config_dict[\"batch_size\"]*gen_iter), 3)\n","  results['unique'] = np.round(len(unique_smiles)/len(results), 3)\n","  results['novelty'] = np.round(novel_ratio/100, 3)\n","  # save the dataframe as a csv file\n","  results.to_csv(inference_config_dict[\"save_path\"], index = False)\n","  # print all evaluation metrics using function from moses package\n","  print(moses.get_all_metrics(list(results['smiles'].values), device=model_config_dict['device']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6N-mabNHSLS-"},"outputs":[],"source":["inference_config_dict = {\n","    # \"model_params\": train_config_dict[\"ckpt_path\"],\n","    \"model_params\": BASE + 'checkpoints/test_6_21.pt',\n","    \"train_data\": train_config_dict[\"train_path\"],\n","    # \"desc_path\": model_config_dict[\"desc_path\"],\n","    \"desc_path\": BASE + 'checkpoints/descriptors_200k.yaml',\n","    \"save_path\": BASE + 'data/test_06_21_t.csv',\n","    \"batch_size\": 1,\n","    \"gen_size\": 10,\n","    \"temp\": 1,\n","    \"context\": \"C1CCXCCC1\",\n","    \"scaffold\": None,\n","    \"props\": []\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5DqRGAxSLS-"},"outputs":[],"source":["# run function to generate SMILES strings\n","generate_SMILES(\n","              model_config_dict = model_config_dict,\n","              inference_config_dict = inference_config_dict\n","              )"]},{"cell_type":"markdown","metadata":{"id":"efDKGmL7-lKW"},"source":["We present GPT-based pipeline for molecular generation that can identify regions of PCA chemical space of interest"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jzRj6bGQpUaM","78g7xtxZK7-e","iTFNFFNGpUaR","zWbFS84KpUaS","lo6dY-UstJbu","Mdcw_UmAV8FK","7iB0S7xSK7-i","B-JpaIyJVwYw","zBgupbjAPYu7","j8-Pj-x1K7-l","42dkxj7rRd1z","Tamyd6Hstkpl","nTDMdNdimsus","KhUk_1SLmzvh","svfAh4qsC17K","xGA3B_vOC0Dn","NtOQLpdXWE9w","4CXn_Jqt_76M","EJqiexX6AAsv","0gbEP-i0Av6M","VXLdZWy9A5Tk","6Jia_2uzBQOp"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
