{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22548,"status":"ok","timestamp":1692042174420,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"qhUCZq9nvmGP","outputId":"d2e18ac9-803d-40b4-be07-bef1059deb8f"},"outputs":[],"source":["# mount google drive\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6040,"status":"ok","timestamp":1692042180454,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"McHGEQv_u-8r"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from rdkit import Chem\n","from rdkit.Chem import Descriptors\n","from rdkit.Chem import rdMolDescriptors\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import pickle\n","from pprint import pprint as pp\n","from tqdm import tqdm\n","import itertools\n","import umap\n","from sklearn.manifold import TSNE\n","import scipy\n","\n","\n","# BASE_PATH = '/content/drive/MyDrive/Generative_ML/current_data/' #@param {type:\"string\"}\n","BASE_PATH = \"/Users/morgunov/batista/Summer/pipeline/\"\n","\n","PRETRAINING_PATH = BASE_PATH + \"1. Pretraining/\"\n","GENERATION_PATH = BASE_PATH + \"2. Generation/\"\n","SAMPLING_PATH = BASE_PATH + \"3. Sampling/\"\n","DIFFDOCK_PATH = BASE_PATH + \"4. DiffDock/\"\n","SCORING_PATH = BASE_PATH + \"5. Scoring/\"\n","AL_PATH = BASE_PATH + \"6. ActiveLearning/\"\n","PICKLES = BASE_PATH + \"Archive/pickle/\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tqjuAeINwMDW"},"source":["# Processing dataset descriptors"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AJJ6x8zuwPM6"},"source":["## Convert each dictionary to a dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7sdUnA39c2X"},"outputs":[],"source":["def split_the_dictionary(fname):\n","    with open(PICKLES + f\"{fname}.pkl\", \"rb\") as f:\n","        smiles_to_descriptors = pickle.load(f)\n","    smiles = list(smiles_to_descriptors.keys())\n","    half_index = int(len(smiles) // 2)\n","    pt1 = {}\n","    pt2 = {}\n","    for i, smile in enumerate(smiles):\n","        if i < half_index:\n","            pt1[smile] = smiles_to_descriptors[smile]\n","        else:\n","            pt2[smile] = smiles_to_descriptors[smile]\n","    print(len(pt1), len(pt2), len(pt1) + len(pt2), len(smiles_to_descriptors))\n","    pickle.dump(pt1, open(PICKLES + f\"{fname}_subpt1.pkl\", \"wb\"))\n","    pickle.dump(pt2, open(PICKLES + f\"{fname}_subpt2.pkl\", \"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KC_EdZfO9gFI"},"outputs":[],"source":["split_the_dictionary(\"smile_to_descriptors_pt1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8fjkgHHwPeo"},"outputs":[],"source":["def pickle_to_csv(fname):\n","    with open(PICKLES + f\"{fname}.pkl\", \"rb\") as f:\n","        smiles_to_descriptors = pickle.load(f)\n","    keyToData = {}\n","    keys = pickle.load(open(PICKLES + \"descriptors_list.pkl\", \"rb\"))\n","    pbar = tqdm(\n","        smiles_to_descriptors.items(), total=len(smiles_to_descriptors), desc=fname\n","    )\n","    for smile, descriptors in pbar:\n","        keyToData.setdefault(\"smile\", []).append(smile)\n","        for key in keys:\n","            keyToData.setdefault(key, []).append(descriptors[key])\n","    df = pd.DataFrame(keyToData)\n","    df.to_pickle(PICKLES + \"_\".join(fname.split(\"_\")[2:]) + \".pkl\")\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zx1dTYunJJVb"},"outputs":[],"source":["pickle_to_csv(\"smile_to_descriptors_pt1_subpt1\")  # done\n","pickle_to_csv(\"smile_to_descriptors_pt1_subpt2\")  # done\n","pickle_to_csv(\"smile_to_descriptors_pt2_subpt1\")  # done\n","pickle_to_csv(\"smile_to_descriptors_pt2_subpt2\")\n","pickle_to_csv(\"smile_to_descriptors_pt3\")  # done"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"giWJddP_21Qw"},"source":["## Combine dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wt-JolBT27FL"},"outputs":[],"source":["def merge_parts():\n","    pt1_sbpt1 = pd.read_pickle(PICKLES + \"descriptors_pt1_subpt1.pkl\")\n","    pt1_sbpt2 = pd.read_pickle(PICKLES + \"descriptors_pt1_subpt2.pkl\")\n","    pt2_sbpt1 = pd.read_pickle(PICKLES + \"descriptors_pt2_subpt1.pkl\")\n","    pt2_sbpt2 = pd.read_pickle(PICKLES + \"descriptors_pt2_subpt2.pkl\")\n","    pt3 = pd.read_pickle(PICKLES + \"descriptors_pt3.pkl\")\n","    print(pt1_sbpt1.shape, pt1_sbpt2.shape, pt2_sbpt1.shape, pt2_sbpt2.shape, pt3.shape)\n","    merged_df = pd.concat([pt1_sbpt1, pt1_sbpt2, pt2_sbpt1, pt2_sbpt2, pt3])\n","    print(merged_df.shape)\n","    merged_df.to_pickle(PICKLES + \"descriptors_combined.pkl\")\n","\n","\n","merge_parts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkqyAb_SjrHu"},"outputs":[],"source":["def analyze_training_sets():\n","    def combine_sets(train_fname, valid_fname):\n","        train_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{train_fname}.csv.gz\")\n","        valid_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{valid_fname}.csv.gz\")\n","        combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n","        return combined_df\n","\n","    train_100 = combine_sets(\n","        \"combined_processed_freq100_block133_train\",\n","        \"combined_processed_freq100_block133_val\",\n","    )\n","    train_1000 = combine_sets(\n","        \"combined_processed_freq1000_block133_train\",\n","        \"combined_processed_freq1000_block133_val\",\n","    )\n","    print(f\"{train_100.shape=}, {train_1000.shape=}\")\n","    in100 = set(train_100[\"smiles\"]) - set(train_1000[\"smiles\"])\n","    in1000 = set(train_1000[\"smiles\"]) - set(train_100[\"smiles\"])\n","    print(f\"{len(in100)=}, {len(in1000)=}\")\n","\n","\n","analyze_training_sets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNsSvD1kRwqQ"},"outputs":[],"source":["def fill_descriptors_for_training_set(\n","    all_descriptors_path, train_fname, valid_fname, extra_mols_path=None\n","):\n","    descriptors_wsmiles = pd.read_pickle(PICKLES + \"descriptors_combined.pkl\")\n","    descriptors_wsmiles.rename(columns={\"smile\": \"smiles\"}, inplace=True)\n","    print(f\"Descriptors df has shape {descriptors_wsmiles.shape}\")\n","    train_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{train_fname}.csv.gz\")\n","    valid_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{valid_fname}.csv.gz\")\n","    combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n","    # assert len(set(descriptors_wsmiles['smiles'])-set(descriptors_wsmiles['smiles'])) == 0, \"Combined df\"\n","    print(f\"Training/validation df has shape {combined_df.shape}\")\n","    merged_df = combined_df.merge(descriptors_wsmiles, on=\"smiles\", how=\"inner\")\n","    extra_mols = set(combined_df[\"smiles\"]) - set(merged_df[\"smiles\"])\n","    if len(extra_mols) != 0:\n","        print(f\"Extra molecules in the training set detected\")\n","        if extra_mols_path is None:\n","            pd.DataFrame({\"smiles\": list(extra_mols)}).to_csv(\n","                f\"{PRETRAINING_PATH}descriptors/{train_fname[:-6]}_extra_mols.csv\"\n","            )\n","            print(\n","                f\"Saved extra mols to {PRETRAINING_PATH}descriptors/{train_fname[:-6]}_extra_mols.csv, abort execution\"\n","            )\n","            return\n","        else:\n","            extra_descriptors = pd.read_pickle(extra_mols_path)\n","            assert (\n","                len(set(extra_descriptors[\"smiles\"]) - extra_mols) == 0\n","            ), \"Extra descriptors do not cover all extra_mols\"\n","            extra_descriptors = extra_descriptors[\n","                extra_descriptors[\"smiles\"].isin(extra_mols)\n","            ]\n","\n","    final_df = pd.concat([merged_df, extra_descriptors])\n","    print(f\"Final descriptors df has shape {final_df.shape}\")\n","    assert (\n","        final_df.shape[0] == combined_df.shape[0]\n","    ), \"Number of molecules in training/valid set and descriptors df do not match\"\n","    final_df.to_pickle(f\"{PRETRAINING_PATH}descriptors/{train_fname[:-6]}.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDbUmjrUb8qC"},"outputs":[],"source":["fill_descriptors_for_training_set(\n","    f\"{PICKLES}descriptors_combined.pkl\",\n","    \"combined_processed_freq1000_block133_train\",\n","    \"combined_processed_freq1000_block133_val\",\n","    f\"{PRETRAINING_PATH}descriptors/combined_processed_freq1000_block133_extra_mols.pkl\",\n",")\n","\n","# For 100\n","# Descriptors df has shape (5770637, 210)\n","# Training/validation df has shape (5560874, 2)\n","# Extra molecules in the training set detected\n","# Final descriptors df has shape (5560874, 211)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MHqNMSWvHLmu"},"source":["# Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK5Pz7eT3j_H"},"outputs":[],"source":["def fit_pca(dataframe, n=3, sigma=None, whiten=False):\n","    scaler = StandardScaler()\n","    scaled_data = scaler.fit_transform(dataframe)\n","    if sigma is not None:\n","        scaled_data = scaled_data[(scaled_data <= sigma).all(axis=1)]\n","    pca = PCA(n_components=n, whiten=whiten)\n","    pca.fit(scaled_data)\n","    return scaler, pca\n","\n","\n","def pca_transform(pca, dataframe, n):\n","    assert (\n","        pca.n_components_ >= n\n","    ), f\"PCA was fitted on {pca.n_components_} components, but {n} were requested.\"\n","    transformed = pca.transform(dataframe)\n","    return [transformed[:, i] for i in range(n)]\n","\n","\n","def plot_pca(scaler, pca, datapoints, yscale=1.05):\n","    fig = go.Figure()\n","    minX, minY, maxX, maxY = float(\"inf\"), float(\"inf\"), float(\"-inf\"), float(\"-inf\")\n","    traces = []\n","    for data, color, label in datapoints:\n","        transformed = pca.transform(\n","            scaler.transform(data[scaler.get_feature_names_out()])\n","        )\n","        xarr = transformed[:, 0]\n","        yarr = transformed[:, 1]\n","        minX = min(minX, min(xarr))\n","        minY = min(minY, min(yarr))\n","        maxX = max(maxX, max(xarr))\n","        maxY = max(maxY, max(yarr))\n","        fig.add_trace(\n","            go.Scatter(\n","                x=xarr,\n","                y=yarr,\n","                mode=\"markers\",\n","                name=label,\n","                marker=dict(\n","                    size=5,\n","                    color=color,\n","                    showscale=True if isinstance(color, (list, np.ndarray)) else False,\n","                    colorscale=\"Viridis\",\n","                    opacity=0.5,\n","                ),\n","            )\n","        )\n","\n","    fig.update_layout(\n","        xaxis=dict(\n","            title=\"PCA Component 1\",\n","            autorange=False,\n","            range=[yscale * minX, yscale * maxX],\n","        ),\n","        yaxis=dict(\n","            title=\"PCA Component 2\",\n","            autorange=False,\n","            range=[yscale * minY, yscale * maxY],\n","        ),\n","    )\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBQQugKc3p4t"},"outputs":[],"source":["def get_data_boundaries(data_list):\n","    combined = np.vstack(\n","        data_list\n","    )  # Combine both datasets to get overall min and max values\n","\n","    min_val = (\n","        np.floor(np.min(combined, axis=0) / 10.0) * 10\n","    )  # Round to the nearest number divisible by 10\n","    max_val = np.ceil(np.max(combined, axis=0) / 10.0) * 10\n","    return min_val, max_val\n","\n","\n","def discretize_data(data, boundaries, bin_size):\n","    # Use 2D histogram to discretize data\n","    bins = [np.arange(boundaries[0][i], boundaries[1][i], bin_size) for i in range(2)]\n","    hist_data, xedges, yedges = np.histogram2d(data[:, 0], data[:, 1], bins=bins)\n","\n","    # Compute bin centers\n","    xcenters = (xedges[:-1] + xedges[1:]) / 2\n","    ycenters = (yedges[:-1] + yedges[1:]) / 2\n","\n","    return hist_data.T, xcenters, ycenters\n","\n","\n","def plot_heatmap(\n","    args_list,\n","    difference=False,\n","    bin_size=10,\n","    width=1280,\n","    height=720,\n","    all_differences=False,\n","):\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    fig = go.Figure()\n","\n","    if difference:\n","        if all_differences:\n","            traces = []\n","            for i, ((data_before, name_before), (data_after, name_after)) in enumerate(\n","                itertools.combinations(args_list, 2)\n","            ):\n","                hist_before, xcenters, ycenters = discretize_data(\n","                    data_before, boundaries, bin_size\n","                )\n","                hist_after, _, _ = discretize_data(data_after, boundaries, bin_size)\n","                diff = hist_after - hist_before\n","                label = f\"|{name_after}|<br>-|{name_before}|\"\n","                traces.append(\n","                    go.Heatmap(\n","                        x=xcenters,\n","                        y=ycenters,\n","                        z=diff,\n","                        zmid=0,\n","                        zmax=110,\n","                        zmin=-110,\n","                        colorscale=\"RdBu\",\n","                        name=label,\n","                        showlegend=True,\n","                        visible=True if i == 0 else \"legendonly\",\n","                    )\n","                )\n","            for trace in traces:\n","                fig.add_trace(trace)\n","        else:\n","            assert (\n","                len(data_list) == 2\n","            ), f\"To plot a difference, please provide only 2 data sources\"\n","            hist_before, xcenters, ycenters = discretize_data(\n","                data_list[0], boundaries, bin_size\n","            )\n","            hist_after, _, _ = discretize_data(data_list[1], boundaries, bin_size)\n","            diff = hist_after - hist_before\n","            label = f\"|{name_list[1]}|<br>-|{name_list[0]}|\"\n","            fig.add_trace(\n","                go.Heatmap(\n","                    x=xcenters,\n","                    y=ycenters,\n","                    z=diff,\n","                    zmid=0,\n","                    colorscale=\"RdBu\",\n","                    name=label,\n","                    showlegend=True,\n","                )\n","            )\n","    else:\n","        traces = []\n","        zmax = max(\n","            [discretize_data(data, boundaries, bin_size)[0].max() for data in data_list]\n","        )\n","        zmin = min(\n","            [discretize_data(data, boundaries, bin_size)[0].min() for data in data_list]\n","        )\n","        for i, (data, name) in enumerate(args_list):\n","            hist, xcenters, ycenters = discretize_data(data, boundaries, bin_size)\n","            if \"al1 good\" in name:\n","                mult = 1 / 50\n","            else:\n","                mult = 1\n","            traces.append(\n","                go.Heatmap(\n","                    x=xcenters,\n","                    y=ycenters,\n","                    z=hist,\n","                    name=name,\n","                    zmin=zmin,\n","                    zmax=mult * zmax,\n","                    showlegend=True,\n","                    visible=True if i == 0 else \"legendonly\",\n","                )\n","            )\n","        for trace in traces:\n","            fig.add_trace(trace)\n","\n","    fig.update_layout(\n","        title=f\"Difference in distribution: # of datapoints per bin ({bin_size=})\",\n","        xaxis_title=\"X\",\n","        yaxis_title=\"Y\",\n","        width=width,\n","        height=height,\n","        legend=dict(x=1.2, y=1),\n","    )\n","\n","    return fig"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K4dxpsH33a5m"},"source":["# PCA Analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EZDzTkI4U0Xw"},"source":["## preprocessing & fitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFnBoUPrPwUe"},"outputs":[],"source":["def remove_mols_with_nan_descriptors(descriptors_fname):\n","    descriptors = pd.read_pickle(\n","        f\"{PRETRAINING_PATH}descriptors/{descriptors_fname}.pkl\"\n","    ).drop(columns=[\"Unnamed: 0\"])\n","    dataset = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{descriptors_fname}.csv.gz\")\n","    assert set(descriptors[\"smiles\"]) == set(\n","        dataset[\"smiles\"]\n","    ), \"Descriptors object contains smiles different from the training dataset\"\n","    nan_smiles = set(descriptors[descriptors.isna().any(axis=1)][\"smiles\"])\n","    print(f\"There are {len(nan_smiles)=}\")\n","    descriptors_nonan = descriptors[~descriptors[\"smiles\"].isin(nan_smiles)]\n","    dataset_nonan = dataset[~dataset[\"smiles\"].isin(nan_smiles)]\n","    print(f\"Had {descriptors.shape} before\")\n","    print(f\"Now have {descriptors_nonan.shape} after\")\n","    assert set(descriptors_nonan[\"smiles\"]) == set(\n","        dataset_nonan[\"smiles\"]\n","    ), \"Descriptors object contains smiles different from the training dataset\"\n","    descriptors_nonan.to_pickle(\n","        f\"{PRETRAINING_PATH}descriptors/{descriptors_fname}_nonan.pkl\"\n","    )\n","    dataset_nonan.to_csv(f\"{PRETRAINING_PATH}datasets/{descriptors_fname}_nonan.csv.gz\")\n","\n","\n","def fit_pca_on_dataset(descriptors_fname, n_comps=100, drop_nan=False):\n","    descriptors = pd.read_pickle(\n","        f\"{PRETRAINING_PATH}descriptors/{descriptors_fname}.pkl\"\n","    ).drop(columns=[\"smiles\", \"Ipc\"])\n","    nan_cols = descriptors.columns[descriptors.isna().any()]\n","    if drop_nan:\n","        descriptors.drop(columns=nan_cols, inplace=True)\n","    scaler, pca = fit_pca(descriptors, n=n_comps)\n","    pickle.dump(\n","        (scaler, pca),\n","        open(\n","            f\"{SAMPLING_PATH}pca_weights/scaler_pca_{descriptors_fname}_{n_comps}.pkl\",\n","            \"wb\",\n","        ),\n","    )\n","    return scaler, pca\n","\n","\n","nan_rows = remove_mols_with_nan_descriptors(\"combined_processed_freq1000_block133\")\n","# scaler, pca = fit_pca_on_dataset(\"combined_processed_freq1000_block133\", n_comps=120, drop_nan=True)\n","scaler, pca = fit_pca_on_dataset(\n","    \"combined_processed_freq1000_block133_nonan\", n_comps=120, drop_nan=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1691525435350,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"NF5NwescXhJ0","outputId":"e39aabcf-1c56-4912-c41d-c5b708a8ed41"},"outputs":[],"source":["# scaler, pca = pickle.load(open(f\"{SAMPLING_PATH}pca_weights/scaler_pca_combined_processed_freq1000_block133_nonan\", 'rb'))\n","import matplotlib.pyplot as plt\n","\n","var_arr = pca.explained_variance_ratio_\n","print(sum(var_arr))\n","print(sum(var_arr[:2]))\n","cum_varr = np.cumsum(var_arr)\n","plt.plot([1 for _ in range(len(var_arr))])\n","plt.plot(cum_varr)\n","print(sum(var_arr[:100]))\n","print(np.where(cum_varr > 0.99))\n","print(np.where(cum_varr > 0.995))\n","print(np.where(cum_varr > 0.999))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8do8XB19U6Zc"},"source":["## visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["biscale = {\n","    \"blue\": (\"#03045e\", \"#023e8a\"),\n","    \"purple\": (\"#7b2cbf\", \"#c77dff\"),\n","    \"green\": (\"#008000\", \"#70e000\"),\n","    \"red\": (\"#a4133c\", \"#ff4d6d\"),\n","}\n","from IPython.display import Markdown, display\n","\n","biscale.update(\n","    {\n","        \"orange\": (\"#e85d04\", \"#faa307\"),\n","        # \"pink\": (\"#d00000\", \"#ff6f69\"),\n","        # \"yellow\": (\"#c5a880\", \"#ffdd00\"),\n","        \"dark_grey\": (\"#2e2e2e\", \"#5a5a5a\"),\n","        # \"gray\": (\"#6d6875\", \"#a5a5a5\"),\n","        \"teal\": (\"#006a71\", \"#48cae4\"),\n","        # \"violet\": (\"#3a0ca3\", \"#6f23ff\"),\n","        \"lime\": (\"#679436\", \"#aee833\"),\n","        # \"indigo\": (\"#3c096c\", \"#5e60ce\"),\n","        \"gold\": (\"#b08d57\", \"#f0e442\"),\n","        # \"cyan\": (\"#0077b6\", \"#00b4d8\"),\n","        # \"magenta\": (\"#7209b7\", \"#e43f5a\"),\n","        \"beige\": (\"#bfa58a\", \"#f5deb3\"),\n","        # \"turquoise\": (\"#00707b\", \"#00a19d\"),\n","        # \"olive\": (\"#6a994e\", \"#a7c957\"),\n","        \"maroon\": (\"#4a0000\", \"#800000\"),\n","        # \"coral\": (\"#ff4b5c\", \"#ff6b6b\")\n","    }\n",")\n","for scale, colors in biscale.items():\n","    print(scale)\n","    display(\n","        Markdown(\n","            \"<br>\".join(\n","                f'<span style=\"font-family: monospace\">{color} <span style=\"color: {color}\">████████</span></span>'\n","                for color in colors\n","            )\n","        )\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["biscale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sy449iwptvMU"},"outputs":[],"source":["scaler, pca = pickle.load(\n","    open(\n","        f\"{SAMPLING_PATH}pca_weights/scaler_pca_combined_processed_freq100_block133.pkl\",\n","        \"rb\",\n","    )\n",")\n","columns = pickle.load(\n","    open(\n","        f\"{SAMPLING_PATH}descriptors/combined_processed_freq100_block133_columnlist.pkl\",\n","        \"rb\",\n","    )\n",")\n","# descriptors = pd.read_pickle(f\"{PRETRAINING_PATH}descriptors/combined_processed_freq100_block133.pkl\")\n","gen100 = pd.read_pickle(f\"{SAMPLING_PATH}descriptors/model4_baseline_temp1.0.pkl\")\n","gen1000 = pd.read_pickle(f\"{SAMPLING_PATH}descriptors/model5_baseline_temp1.0.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0urfSS03uxoQ"},"outputs":[],"source":["sample = 5_000\n","seed = 42\n","colors = {\n","    \"gunmetal\": \"#31393C\",\n","    \"blue\": \"#4361EE\",\n","    \"plum\": \"#8E338C\",\n","    \"grape\": \"#7209B7\",\n","    \"red\": \"#D90429\",\n","    \"orange\": \"#FF7B00\",\n","    \"yellow\": \"#FFBA08\",\n","    \"mindaro\": \"#CBFF8C\",\n","}\n","\n","plot_pca(\n","    [\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gen100[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"gunmetal\"],\n","            f\"100 freq\",\n","        ),\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gen1000[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"grape\"],\n","            f\"1000 freq\",\n","        ),\n","    ],\n","    yscale=1.5,\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zFp9UAGZRVR"},"outputs":[],"source":["scaler, pca = pickle.load(\n","    open(f\"{SAMPLING_PATH}pca_weights/scaler_pca_moses+bindingdb.pkl\", \"rb\")\n",")\n","columns = pickle.load(\n","    open(f\"{SAMPLING_PATH}descriptors/descriptors_moses+bindingdb_columnlist.pkl\", \"rb\")\n",")\n","load_baseline = lambda fname: pd.read_pickle(\n","    f\"{SAMPLING_PATH}descriptors/{fname}.pkl\"\n",").drop_duplicates(subset=\"smiles\")\n","gpt_base = load_baseline(\"model1_baseline_temp1.0\")\n","gpt_sub1 = load_baseline(\"model1_softsub_al1_temp1.0\")\n","gpt_div1 = load_baseline(\"model1_softdiv_al1_temp1.0\")\n","gpt_sub2 = load_baseline(\"model1_softsub_al2_temp1.0\")\n","gpt_div2 = load_baseline(\"model1_softdiv_al2_temp1.0\")\n","\n","pre_base = \"model1_baseline_threshold11\"\n","pre_sub1 = \"model1_softsub_al1_threshold11\"\n","pre_div1 = \"model1_softdiv_al1_threshold11\"\n","pre_sub2 = \"model1_softsub_al2_threshold11\"\n","pre_div2 = \"model1_softdiv_al2_threshold11\"\n","l_base = lambda fname: gpt_base[\n","    gpt_base[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_base}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_sub1 = lambda fname: gpt_sub1[\n","    gpt_sub1[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_sub1}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_div1 = lambda fname: gpt_div1[\n","    gpt_div1[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_div1}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_sub2 = lambda fname: gpt_sub2[\n","    gpt_sub2[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_sub2}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_div2 = lambda fname: gpt_div2[\n","    gpt_div2[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_div2}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhRKcAFhlKw5"},"outputs":[],"source":["print(\n","    l_base(\"linear\").shape,\n","    l_base(\"linear_noscore\").shape,\n","    l_base(\"softmax_divf0.25\").shape,\n","    l_base(\"softmax_sub\").shape,\n",")\n","print(\n","    l_sub1(\"softmax_sub\").shape,\n","    l_sub1(\"softmax_sub_noscore\").shape,\n","    l_div1(\"softmax_divf0.25\").shape,\n","    l_div1(\"softmax_divf0.25_noscore\").shape,\n",")\n","print(\n","    l_sub2(\"softmax_sub\").shape,\n","    l_sub2(\"softmax_sub_noscore\").shape,\n","    l_div2(\"softmax_divf0.25\").shape,\n","    l_div2(\"softmax_divf0.25_noscore\").shape,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDeBBcapZq5_"},"outputs":[],"source":["sample = 5_000\n","seed = 42\n","colors = {\n","    \"gunmetal\": \"#31393C\",\n","    \"blue\": \"#4361EE\",\n","    \"plum\": \"#8E338C\",\n","    \"grape\": \"#7209B7\",\n","    \"red\": \"#D90429\",\n","    \"orange\": \"#FF7B00\",\n","    \"yellow\": \"#FFBA08\",\n","    \"mindaro\": \"#CBFF8C\",\n","}\n","\n","scatter = lambda loader, fname: pca_transform(\n","    pca, scaler.transform(loader(fname)[columns]), n=2\n",")\n","plot_pca(\n","    [\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gpt_base[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"gunmetal\"],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gpt_sub1[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"grape\"],\n","            f\"GPT Softmax Sub\",\n","        ),\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gpt_div1[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"red\"],\n","            f\"GPT Softmax Div0.25\",\n","        ),\n","        # (*scatter('linear'), colors[\"blue\"], f'AL1 Linear'),\n","        (*scatter(l_base, \"linear_noscore\"), colors[\"mindaro\"], f\"AL1 Diffusion\"),\n","        # (*scatter(l_base, 'softmax_divf0.25'), colors[\"grape\"], f'AL1 Softmax Div0.25'),\n","        # (*scatter(l_base, 'softmax_sub'), colors[\"red\"], f'AL1 Softmax Sub'),\n","    ],\n","    yscale=1.5,\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_udJYnMowyyF"},"outputs":[],"source":["# @title\n","import matplotlib.pyplot as plt\n","\n","var_arr = pca.explained_variance_ratio_\n","print(sum(var_arr))\n","print(sum(var_arr[:2]))\n","cum_varr = np.cumsum(var_arr)\n","plt.plot(var_arr)\n","plt.plot([1 for _ in range(len(var_arr))])\n","plt.plot(cum_varr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coTkRXm0VDqH"},"outputs":[],"source":["heat = lambda loader, fname: pca.transform(scaler.transform(loader(fname)[columns]))[\n","    :, :2\n","]\n","plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (heat(l_base, \"linear_noscore\"), f\"Base Diffusion\"),\n","        # (pca.transform(scaler.transform(al1_softmax_div_nosc[columns]))[:, :2], f'AL1 Diffusion Softmax Div0.25'),\n","        # (pca.transform(scaler.transform(al1_softmax_sub_nosc[columns]))[:, :2], f'AL1 Diffusion Softmax Sub'),\n","        (heat(l_base, \"linear\"), f\"AL1 Linear\"),\n","        (heat(l_base, \"softmax_divf0.25\"), f\"AL1 Softmax Div0.25\"),\n","        (heat(l_base, \"softmax_sub\"), f\"AL1 Softmax Sub\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-ABhzYUVEK6"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_sub1[columns].sample(n=5_406, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Sub1\",\n","        ),\n","        (heat(l_sub1, \"softmax_sub_noscore\"), f\"Sub1 Diffusion\"),\n","        (heat(l_sub1, \"softmax_sub\"), f\"AL2 Softmax Sub\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2Cdc-yBgYUv"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_div1[columns].sample(n=5_363, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Div1\",\n","        ),\n","        (heat(l_div1, \"softmax_divf0.25_noscore\"), f\"Div1 Diffusion\"),\n","        (heat(l_div1, \"softmax_divf0.25\"), f\"AL2 Softmax Div\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiX5wKedFQU7"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_sub2[columns].sample(n=5_406, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Sub2\",\n","        ),\n","        (heat(l_sub2, \"softmax_sub_noscore\"), f\"Sub2 Diffusion\"),\n","        (heat(l_sub2, \"softmax_sub\"), f\"AL3 Softmax Sub\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FO97WCiwzT_s"},"outputs":[],"source":["l_div2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbhu1-6pFuAI"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_div2[columns].sample(n=5_363, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Div2\",\n","        ),\n","        (heat(l_div2, \"softmax_divf0.25_noscore\"), f\"Div2 Diffusion\"),\n","        (heat(l_div2, \"softmax_divf0.25\"), f\"AL3 Softmax Div\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"B7iDVys3ArTo"},"source":["# Distribution Analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zDT4LDmYUOXA"},"source":["### Definitions"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":812,"status":"ok","timestamp":1692042181262,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"6UcS6dFJUPNi"},"outputs":[],"source":["import numpy as np\n","import plotly.graph_objects as go\n","from scipy.stats import gaussian_kde\n","import graph\n","import importlib\n","\n","importlib.reload(graph)\n","from graph import Graph\n","\n","\n","import pprint\n","\n","pp = pprint.PrettyPrinter(indent=1, compact=False, width=100)\n","\n","\n","def load_dist(fname):\n","    return pd.read_csv(f\"{SCORING_PATH}scored_dataframes/{fname}.csv\")[\n","        \"score\"\n","    ].to_numpy()\n","\n","\n","def compute_cluster_scores(fname):\n","    good_data = pd.read_csv(f\"{SCORING_PATH}scored_dataframes/{fname}.csv\")\n","    cluster_to_scores = {}\n","    for index, row in good_data.iterrows():\n","        cluster_to_scores.setdefault(row[\"cluster_id\"], []).append(row[\"score\"])\n","    cluster_to_score = {\n","        cluster_id: np.mean(scores) for cluster_id, scores in cluster_to_scores.items()\n","    }\n","    return np.array(list(cluster_to_score.values()))\n","\n","\n","# dark to light\n","# biscale = {\n","#     \"blue\": (\"#03045e\", \"#023e8a\"),\n","#     \"purple\": (\"#7b2cbf\", \"#c77dff\"),\n","#     \"green\": (\"#008000\", \"#70e000\"),\n","#     \"red\": (\"#a4133c\", \"#ff4d6d\"),\n","#     \"orange\": (\"#e85d04\", \"#faa307\"),\n","#     \"teal\": (\"#006a71\", \"#48cae4\"),\n","#     \"lime\": (\"#679436\", \"#aee833\"),\n","#     \"gold\": (\"#b08d57\", \"#f0e442\"),\n","#     \"beige\": (\"#bfa58a\", \"#f5deb3\"),\n","#     \"maroon\": (\"#4a0000\", \"#800000\"),\n","#     \"dark_grey\": (\"#000000\", \"#2e2e2e\"),\n","# }\n","biscale = {\n","    \"blue\": (lambda a: f\"rgba(3, 4, 94, {a})\", \"#023e8a\"),\n","    \"purple\": (lambda a: f\"rgba(123, 44, 191, {a})\", \"#c77dff\"),\n","    \"green\": (lambda a: f\"rgba(0, 128, 0, {a})\", \"#70e000\"),\n","    \"red\": (lambda a: f\"rgba(164, 19, 60, {a})\", \"#ff4d6d\"),\n","    \"orange\": (lambda a: f\"rgba(232, 93, 4, {a})\", \"#faa307\"),\n","    \"teal\": (lambda a: f\"rgba(0, 106, 113, {a})\", \"#48cae4\"),\n","    \"lime\": (lambda a: f\"rgba(103, 148, 54, {a})\", \"#aee833\"),\n","    \"gold\": (lambda a: f\"rgba(176, 141, 87, {a})\", \"#f0e442\"),\n","    \"beige\": (lambda a: f\"rgba(191, 165, 138, {a})\", \"#f5deb3\"),\n","    \"maroon\": (lambda a: f\"rgba(74, 0, 0, {a})\", \"#800000\"),\n","    \"dark_grey\": (lambda a: f\"rgba(0, 0, 0, {a})\", \"#2e2e2e\"),\n","}\n","\n","\n","def create_hist_trace(\n","    i,\n","    data,\n","    label,\n","    color,\n","    threshold,\n","    bin_step,\n","    trace_opacity,\n","    density_line_opacity: float = 0.8,\n","    density_fill=None,\n","    density_fill_opacity: float = 0.1,\n","):\n","    # Generate KDE for data\n","    density = gaussian_kde(data)\n","    xs = np.linspace(np.min(data), np.max(data), 200)\n","    density.covariance_factor = lambda: 0.25\n","    density._compute_covariance()\n","\n","    hist_vals, bin_edges = np.histogram(\n","        data, bins=range(0, int(np.max(data)) + 2, bin_step), density=True\n","    )\n","    hist = go.Bar(\n","        x=bin_edges[:-1],\n","        y=hist_vals,\n","        name=label,\n","        opacity=trace_opacity,\n","        marker=dict(color=biscale[color][1]),\n","        hovertemplate=[f\"[{int(i)}, {int(i + bin_step)})\" for i in bin_edges[:-1]],\n","    )\n","    density_curve = go.Scatter(\n","        x=xs,\n","        y=density(xs),\n","        mode=\"lines\",\n","        name=label,\n","        line=dict(color=biscale[color][0](density_line_opacity), width=4),\n","        fill=density_fill,\n","        fillcolor=biscale[color][0](density_fill_opacity),\n","    )\n","\n","    above_threshold_pct = np.sum(data >= threshold) / len(data) * 100\n","    q25, q50, q75 = np.percentile(data, [25, 50, 75])\n","    f = lambda x: f\"{x:0>4.1f}\" if x < 10 else f\"{x:.1f}\"\n","\n","    annotation = dict(\n","        x=0.95,\n","        y=1.0 - 0.04 * i,\n","        xref=\"paper\",\n","        yref=\"paper\",\n","        text=f\"{label}: % > threshold = {f(above_threshold_pct)}, Q25 = {f(q25)}, Q50 = {f(q50)}, Mean = {f(data.mean())}, Q75 = {f(q75)}, max = {f(data.max())}\",\n","        showarrow=False,\n","        font=dict(size=12),\n","    )\n","\n","    return [hist, density_curve], annotation, max(hist_vals), max(bin_edges[:-1])\n","\n","\n","def plot_hist_density(\n","    prefix,\n","    descriptors_type,\n","    n_clusters,\n","    n_iters,\n","    channel,\n","    colors,\n","    title_spec,\n","    threshold=11,\n","    bin_step=2,\n","    trace_opacity=0.6,\n","    show_annotation: bool = False,\n","    layout_parameters: dict = None,\n","    legend_dict: dict = None,\n","    width=1280,\n","    height=600,\n","    show_bars: bool = True,\n","    show_density: bool = True,\n","    density_line_opacity: float = 0.8,\n","    density_fill=None,\n","    density_fill_opacity: float = 0.1,\n","    threshold_shift: float = 0,\n","):\n","    fnames = [\n","        f\"{prefix}_baseline_{descriptors_type}_k{n_clusters}\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{channel}_al{i}_{descriptors_type}_k{n_clusters}\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    labels = [\n","        f\"PT\",  # f\"{descriptors_type} k{n_clusters} baseline\",\n","        *(\n","            # f\"{descriptors_type} k{n_clusters} {channel} AL{i}\"\n","            f\"AL #{i}\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    graph = Graph(BASE_PATH)\n","    traces, annotations = [], []\n","    freq = 0\n","    highest_x = 0\n","    loader = load_dist if title_spec == \"ligand\" else compute_cluster_scores\n","    for i, (fname, label, color) in enumerate(zip(fnames, labels, colors)):\n","        new_traces, annotation, max_freq, max_x = create_hist_trace(\n","            i,\n","            loader(fname),\n","            label,\n","            color,\n","            threshold,\n","            bin_step,\n","            trace_opacity,\n","            density_line_opacity,\n","            density_fill,\n","            density_fill_opacity,\n","        )\n","        if show_bars:\n","            traces.append(new_traces[0])\n","        if show_density:\n","            traces.append(new_traces[1])\n","        # traces.extend(new_traces)\n","        annotations.append(annotation)\n","        freq = max(freq, max_freq)\n","        highest_x = max(highest_x, max_x)\n","    annotations.append(\n","        dict(\n","            x=threshold + threshold_shift,\n","            y=0.8,\n","            # xref=\"paper\",\n","            yref=\"paper\",\n","            yanchor=\"bottom\",\n","            text=f\"<b>Threshold: {threshold}</b>\",\n","            showarrow=False,\n","        )\n","    )\n","    # Create figure and add traces\n","    fig = go.Figure(\n","        data=traces,\n","        layout=go.Layout(\n","            bargap=0.2,\n","            barmode=\"overlay\",\n","            shapes=[\n","                dict(\n","                    type=\"line\",\n","                    x0=threshold,\n","                    x1=threshold,\n","                    y0=0,\n","                    y1=1,\n","                    yref=\"paper\",  # refers to the entire plot for the y-dimension\n","                    line=dict(color=biscale[\"dark_grey\"][0](0.6), width=4, dash=\"dash\"),\n","                )\n","            ],\n","            annotations=annotations if show_annotation else annotations[-1:],\n","        ),\n","    )\n","    space_desc = \"196 descriptor-based\" if descriptors_type == \"mix\" else \"42 MQ-based\"\n","    # Show figure\n","    graph.update_parameters(\n","        dict(\n","            title=f\"<b>{space_desc} space, {n_clusters} clusters, {channel} selection\",\n","            xaxis_title=\"Attractive Interaction Score\",\n","            yaxis_title=\"Relative Frequency\",\n","            width=width,\n","            height=height,\n","            yrange=[0, freq * 1.05],\n","            **layout_parameters,\n","        )\n","    )\n","    graph.style_figure(fig)\n","    fig.update_layout(legend=legend_dict)\n","    graph.save_figure(\n","        figure=fig,\n","        path=f\"{BASE_PATH}/plots/{title_spec}_distribution/\",\n","        fname=f\"{descriptors_type}_k{n_clusters}_{channel}_al{n_iters}\",\n","        html=True,\n","        width=width,\n","        height=height,\n","    )\n","    with open(\n","        f\"{BASE_PATH}/plots/{title_spec}_distribution/{descriptors_type}_k{n_clusters}_{channel}_al{n_iters}.txt\",\n","        \"w\",\n","    ) as f:\n","        for i in range(len(annotations) - 1):\n","            f.write(annotations[i][\"text\"] + \"\\n\")\n","    return fig"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ffD04c6JTKco"},"source":["### Individual Scores"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["plot_hist_density(\n","    prefix=\"model7\",\n","    descriptors_type=\"mix\",\n","    n_clusters=100,\n","    n_iters=5,\n","    channel=\"softsub\",\n","    colors=[\"dark_grey\", \"teal\", \"orange\", \"purple\", \"green\", \"red\"],\n","    title_spec=calc_type,\n","    bin_step=2 if calc_type == \"ligand\" else 1,\n","    threshold=11,\n","    trace_opacity=0.7,\n","    width=1280,\n","    height=600,\n","    layout_parameters=dict(\n","        title_size=36,\n","        axis_title_size=28,\n","        tick_font_size=24,\n","        yaxis_standoff=10,\n","        title_ycoord=0.97,\n","        t_margin=60,\n","        annotation_size=24,\n","        annotation_color=biscale[\"dark_grey\"][0](0.6),\n","        # x_tickvals=[0, 10, 11, 20, 30, 40, 50, 60],\n","    ),\n","    legend_dict=dict(\n","        x=0.9,\n","        y=0.55,\n","        xanchor=\"left\",\n","        yanchor=\"middle\",\n","        font=dict(size=32),\n","        # itemsizing=\"constant\",\n","    ),\n","    show_bars=False,\n","    show_density=True,\n","    density_line_opacity=0.8,\n","    density_fill=\"tozeroy\",\n","    density_fill_opacity=0.2,\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"elapsed":4838,"status":"ok","timestamp":1692033103275,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"-ScFG6N2l2qc","outputId":"62aad8bd-2613-40f7-9ef8-514eb750685e"},"outputs":[],"source":["configs = [\n","    (\"mix\", 100, 5, \"softsub\", 5),\n","    (\"mix\", 10, 3, \"softsub\", 7),\n","    (\"mqn\", 100, 5, \"softsub\", 4),\n","    (\"mqn\", 10, 4, \"softsub\", 4),\n","]\n","# for calc_type in [\"ligand\", \"cluster\"]:\n","calc_type = \"ligand\"\n","for descriptor_type, n_clusters, n_iters, channel, threshold_shift in configs:\n","    plot_hist_density(\n","        prefix=\"model7\",\n","        descriptors_type=descriptor_type,\n","        n_clusters=n_clusters,\n","        n_iters=n_iters,\n","        channel=channel,\n","        colors=[\"dark_grey\", \"teal\", \"orange\", \"purple\", \"green\", \"red\"],\n","        title_spec=calc_type,\n","        bin_step=2 if calc_type == \"ligand\" else 1,\n","        threshold=11,\n","        trace_opacity=0.7,\n","        width=1280,\n","        height=600,\n","        threshold_shift=threshold_shift,\n","        layout_parameters=dict(\n","            title_size=36,\n","            axis_title_size=28,\n","            tick_font_size=24,\n","            yaxis_standoff=10,\n","            title_ycoord=0.97,\n","            t_margin=60,\n","            annotation_size=24,\n","            annotation_color=biscale[\"dark_grey\"][0](0.6),\n","            # x_tickvals=[0, 10, 11, 20, 30, 40, 50, 60],\n","        ),\n","        legend_dict=dict(\n","            x=0.9,\n","            y=0.55,\n","            xanchor=\"left\",\n","            yanchor=\"middle\",\n","            font=dict(size=32),\n","            # itemsizing=\"constant\",\n","        ),\n","        show_bars=False,\n","        show_density=True,\n","        density_line_opacity=0.8,\n","        density_fill=\"tozeroy\",\n","        density_fill_opacity=0.2,\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["# BindingDB Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from rdkit import Chem\n","from rdkit.Chem import Draw\n","\n","\n","def draw_molecule_from_smiles(smiles_string):\n","    # Create an RDKit molecule object from the SMILES string\n","    molecule = Chem.MolFromSmiles(smiles_string)\n","\n","    # Check if the molecule was created successfully\n","    if molecule is not None:\n","        # Draw the molecule\n","        drawing = Draw.MolToImage(molecule)\n","        # Display the drawing (you can also save it to a file if desired)\n","        return drawing\n","    else:\n","        print(\"Invalid SMILES string.\")\n","\n","\n","def load_bindingdb():\n","    columns = [\"Ligand SMILES\", \"Ki (nM)\", \"IC50 (nM)\", \"Kd (nM)\", \"Target Name\"]\n","    db = pd.read_csv(\n","        f\"{PRETRAINING_PATH}datasets/original_files/BindingDB_all.tsv\",\n","        sep=\"\\t\",\n","        usecols=columns,\n","    )\n","    print(f\"Loaded {len(db)=} entries\")\n","    db.dropna(subset=[\"Kd (nM)\"], inplace=True)\n","    print(f\"Removed entries with missing Kd, {len(db)=} entries left\")\n","    db.drop_duplicates(inplace=True)\n","    print(f\"Removed duplicates, {len(db)=} entries left\")\n","    # Loaded len(db)=2765983 entries\n","    # Removed entries with missing Kd, len(db)=105706 entries left\n","    # Removed duplicates, len(db)=86620 entries left\n","    return db[[\"Ligand SMILES\", \"Target Name\", \"Kd (nM)\"]]\n","\n","\n","def find_good_binders(df, cutoff=100):\n","    def parse_kd(value):\n","        if isinstance(value, (float, int)):\n","            return value\n","        elif isinstance(value, str):\n","            try:\n","                if \">\" in value:\n","                    return float(value[1:])\n","                elif \"<\" in value:\n","                    return (\n","                        float(value[1:]) - 1\n","                    )  # or other logic if you want to handle the '<' differently\n","                return float(value)\n","            except ValueError:\n","                return float(\"inf\")  # or another value that will be filtered out\n","        else:\n","            raise TypeError(f\"{value=} is of type {type(value)=}\")\n","\n","    # Apply the custom function to the \"Kd (nM)\" column\n","    df[\"Kd (nM)\"] = df[\"Kd (nM)\"].apply(parse_kd)\n","    good_binders = df[df[\"Kd (nM)\"] <= cutoff]\n","    return good_binders"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def analyze_repeated_targets(good_binders):\n","    # Group by \"Ligand SMILES\" and count the unique \"Target Name\" values for each group\n","    target_counts = good_binders.groupby(\"Ligand SMILES\")[\"Target Name\"].nunique()\n","\n","    # Count how many SMILES strings are associated with each number of unique targets\n","    repetition_counts = target_counts.value_counts().sort_index()\n","\n","    print(\"Number of unique targets for each SMILES string:\")\n","    print(repetition_counts)\n","\n","\n","def find_smiles_with_n_targets(good_binders, n):\n","    target_counts = good_binders.groupby(\"Ligand SMILES\")[\"Target Name\"].nunique()\n","    smiles_with_n_targets = target_counts[target_counts == n].index\n","\n","    # Get the rows from the original DataFrame that match the SMILES strings with n targets\n","    rows_with_n_targets = good_binders[\n","        good_binders[\"Ligand SMILES\"].isin(smiles_with_n_targets)\n","    ]\n","\n","    # Group by \"Ligand SMILES\" again and get the list of unique target names for each\n","    target_names_by_smiles = rows_with_n_targets.groupby(\"Ligand SMILES\")[\n","        \"Target Name\"\n","    ].unique()\n","\n","    return target_names_by_smiles\n","\n","\n","# analyze_repeated_targets(good_binders)\n","# # Find SMILES strings associated with 5 unique targets and their corresponding target names\n","# smiles_with_5_targets = find_smiles_with_n_targets(good_binders, 328)\n","# print(\"SMILES strings associated with 5 unique targets and their target names:\")\n","# for smiles, targets in smiles_with_5_targets.iteritems():\n","#     print(f\"SMILES: {smiles}\\nTargets: {', '.join(targets)}\\n\")\n","\n","\n","def calculate_descriptors_for_bindingdb(kd_cutoff, target_freq):\n","    df = pd.read_csv(\n","        f\"{BASE_PATH}analysis/no_rare_targets_Kd<{kd_cutoff}_freq>{target_freq}.csv\"\n","    )\n","    smiles = set(df[\"Ligand SMILES\"].values)\n","    calculate_descriptors(\n","        smiles, \"mix\", f\"bindingdb_mix_Kd<{kd_cutoff}_freq>{target_freq}\"\n","    )\n","    calculate_descriptors(\n","        smiles, \"mqn\", f\"bindingdb_mqn_Kd<{kd_cutoff}_freq>{target_freq}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import graph\n","import importlib\n","\n","importlib.reload(graph)\n","from graph import Graph\n","\n","\n","def plot_cumulative_histogram_of_smiles_distribution(good_binders, suffix=\"\"):\n","    graph = Graph(f\"{BASE_PATH}plots/bindingdb_analysis/\")\n","    # graph.create_folders([[\"html\", \"svg\", \"jpg\"]])\n","    smiles_counts_per_target = good_binders.groupby(\"Target Name\")[\n","        \"Ligand SMILES\"\n","    ].nunique()\n","    fig = go.Figure()\n","    fig.add_trace(\n","        go.Histogram(\n","            x=smiles_counts_per_target,\n","            cumulative=dict(enabled=True, direction=\"decreasing\"),\n","            xbins=dict(start=0, end=smiles_counts_per_target.max(), size=1),\n","            marker=dict(opacity=0.7),\n","        )\n","    )\n","    thresholds = [50, 100, 200, 300, 500]\n","    counts = {\n","        threshold: sum(smiles_counts_per_target >= threshold)\n","        for threshold in thresholds\n","    }\n","\n","    for i, (threshold, count) in enumerate(counts.items()):\n","        fig.add_annotation(\n","            x=0.95,  # Relative horizontal position in the figure\n","            y=0.95 - 0.05 * i,  # Relative vertical position, you can adjust as needed\n","            xref=\"paper\",  # Position is relative to the figure\n","            yref=\"paper\",  # Position is relative to the figure\n","            text=f\"{count} targets have {threshold} or more SMILES\",\n","            showarrow=False,\n","            align=\"right\",\n","        )\n","    graph.update_parameters(\n","        dict(\n","            title=\"Cumulative Histogram of SMILES Distribution Across Targets\",\n","            xaxis_title=\"Number of SMILES Strings or Fewer\",\n","            yaxis_title=\"Number of Targets\",\n","            showlegend=False,\n","        )\n","    )\n","    graph.style_figure(fig)\n","    graph.save_figure(\n","        fig, f\"{BASE_PATH}plots/bindingdb_analysis/\", \"smiles_distribution\" + suffix\n","    )\n","    return fig\n","\n","\n","def remove_rare_targets(kd_cutoff=100, target_freq=200):\n","    good_binders = find_good_binders(load_bindingdb(), kd_cutoff)\n","    print(f\"{len(good_binders)=}\")\n","    good_binders.drop_duplicates(subset=[\"Ligand SMILES\"], inplace=True)\n","    print(f\"{len(good_binders)=} after dropping duplicates\")\n","    smiles_counts_per_target = good_binders.groupby(\"Target Name\")[\n","        \"Ligand SMILES\"\n","    ].nunique()\n","    targets_to_keep = smiles_counts_per_target[\n","        smiles_counts_per_target >= target_freq\n","    ].index\n","    no_rare_targets = good_binders[good_binders[\"Target Name\"].isin(targets_to_keep)]\n","    print(f\"Post filtering we have {len(no_rare_targets)=}\")\n","    # no_rare_targets.drop_duplicates(subset=[\"Ligand SMILES\"], inplace=True)\n","    # print(f\"Post dropping {len(no_rare_targets)=}\")\n","    no_rare_targets.to_csv(\n","        f\"{BASE_PATH}analysis/no_rare_targets_Kd<{kd_cutoff}_freq>{target_freq}.csv\",\n","        index=False,\n","    )\n","\n","\n","def find_unique_targets(kd_cutoff, target_freq):\n","    df = pd.read_csv(\n","        f\"{BASE_PATH}analysis/no_rare_targets_Kd<{kd_cutoff}_freq>{target_freq}.csv\"\n","    )\n","    unique_targets = df[\"Target Name\"].unique()\n","    colors = [\n","        \"#e6194B\",\n","        \"#3cb44b\",\n","        \"#ffe119\",\n","        \"#4363d8\",\n","        \"#f58231\",\n","        \"#911eb4\",\n","        \"#46f0f0\",\n","        \"#f032e6\",\n","        \"#42d4f4\",\n","        \"#bfef45\",\n","        \"#fabebe\",\n","        \"#469990\",\n","        \"#e6beff\",\n","        \"#9A6324\",\n","        \"#800000\",\n","        \"#aaffc3\",\n","    ]\n","    target_to_color = {target: color for target, color in zip(unique_targets, colors)}\n","    # safe this dictionary to a yaml file\n","    with open(f\"{BASE_PATH}analysis/target_to_color_16.yaml\", \"w\") as file:\n","        yaml.dump(target_to_color, file)\n","    return target_to_color"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_targets_to_descriptors(good_binders_fname, descriptors_fname):\n","    good_binders = pd.read_csv(f\"{BASE_PATH}analysis/{good_binders_fname}.csv\")\n","    good_binders.drop_duplicates(subset=\"Ligand SMILES\", inplace=True)\n","    descriptors = pickle.load(\n","        open(f\"{SAMPLING_PATH}descriptors/{descriptors_fname}.pkl\", \"rb\")\n","    )\n","    print(f\"Loaded {descriptors.shape} descriptors\")\n","    descriptors = descriptors.merge(\n","        good_binders, left_on=\"smiles\", right_on=\"Ligand SMILES\", how=\"inner\"\n","    )\n","    print(f\"Added targets, {descriptors.shape} entries post merging\")\n","    return descriptors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for cut in [1, 10, 100, 1000, 10_000, 100_000]:\n","    plot_cumulative_histogram_of_smiles_distribution(\n","        find_good_binders(load_bindingdb(), cutoff=cut), suffix=f\"_cutoff{cut}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# remove_rare_targets(kd_cutoff=100, target_freq=100)\n","calculate_descriptors_for_bindingdb(kd_cutoff=100, target_freq=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["find_unique_targets(100, 100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import Markdown, display\n","\n","colors = [\n","    \"#e6194B\",\n","    \"#3cb44b\",\n","    \"#ffe119\",\n","    \"#4363d8\",\n","    \"#f58231\",\n","    \"#911eb4\",\n","    \"#46f0f0\",\n","    \"#f032e6\",\n","    \"#42d4f4\",\n","    \"#bfef45\",\n","    \"#fabebe\",\n","    \"#469990\",\n","    \"#e6beff\",\n","    \"#9A6324\",\n","    \"#800000\",\n","    \"#aaffc3\",\n","]\n","\n","display(\n","    Markdown(\n","        \"<br>\".join(\n","            f'<span style=\"font-family: monospace\">{color} <span style=\"color: {color}\">████████</span></span>'\n","            for color in colors[:16]\n","        )\n","    )\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# UMAP \\ t-SNE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import umap\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","import plotly.graph_objects as go\n","from typing import List, Tuple, Union\n","import pprint\n","import pickle\n","import yaml\n","\n","pp = pprint.PrettyPrinter(indent=2, compact=False, width=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Callable\n","\n","\n","def open_scored_file(fname: str) -> pd.DataFrame:\n","    return pd.read_csv(f\"{SCORING_PATH}scored_dataframes/{fname}.csv\")[\n","        [\"smiles\", \"score\"]\n","    ]\n","\n","\n","def open_training_set(fname: str) -> pd.DataFrame:\n","    return pd.read_csv(f\"{AL_PATH}training_sets/{fname}.csv\")[[\"smiles\"]]\n","\n","\n","def open_generations(fname: str) -> pd.DataFrame:\n","    return pd.read_csv(f\"{GENERATION_PATH}smiles/{fname}.csv\")[[\"smiles\"]]\n","\n","\n","def load_generations_all_iters(\n","    prefix: str,\n","    descriptors_type: str,\n","    n_clusters: int,\n","    n_iters: int,\n","    channel: str,\n","    selection: str,\n","    do_concatenation: bool = True,\n",") -> pd.DataFrame:\n","    fnames = [\n","        f\"{prefix}_baseline_temp1.0_processed\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{channel}_al{i}_temp1.0_processed\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    if do_concatenation:\n","        return pd.concat([open_generations(fname) for fname in fnames])\n","    else:\n","        return [open_generations(fname) for fname in fnames]\n","\n","\n","def load_altrain_all_iters(\n","    prefix: str,\n","    descriptors_type: str,\n","    n_clusters: int,\n","    n_iters: int,\n","    channel: str,\n","    selection: str,\n","    do_concatenation: bool = True,\n",") -> pd.DataFrame:\n","    fnames = [\n","        f\"{prefix}_baseline_{descriptors_type}_k{n_clusters}_{selection}\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{channel}_al{i}_{descriptors_type}_k{n_clusters}_{selection}\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    if do_concatenation:\n","        return pd.concat([open_training_set(fname) for fname in fnames])\n","    else:\n","        return [open_training_set(fname) for fname in fnames]\n","\n","\n","def load_scored_all_iters(\n","    prefix: str,\n","    descriptors_type: str,\n","    n_clusters: int,\n","    n_iters: int,\n","    channel: str,\n","    selection: str,\n","    do_concatenation: bool = True,\n",") -> pd.DataFrame:\n","    fnames = [\n","        f\"{prefix}_baseline_{descriptors_type}_k{n_clusters}\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{channel}_al{i}_{descriptors_type}_k{n_clusters}\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    if do_concatenation:\n","        stacked = pd.concat([open_scored_file(fname) for fname in fnames])\n","        assert (\n","            stacked.drop_duplicates().shape == stacked.shape\n","        ), \"There are duplicates in the array\"\n","        return stacked\n","    else:\n","        return [open_scored_file(fname) for fname in fnames]\n","\n","\n","def open_descriptors_file(fname: str) -> pd.DataFrame:\n","    return pickle.load(open(f\"{SAMPLING_PATH}descriptors/{fname}.pkl\", \"rb\"))\n","\n","\n","def load_descriptors(\n","    prefix: str,\n","    descriptors_type: str,\n","    n_clusters: int,\n","    n_iters: int,\n","    selection: str,\n","    do_merge: bool = True,\n",") -> pd.DataFrame:\n","    fnames = [\n","        f\"{prefix}_baseline_{descriptors_type}_temp1.0\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{selection}_al{i}_{descriptors_type}_temp1.0\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    if do_merge:\n","        merged = pd.concat([open_descriptors_file(fname) for fname in fnames])\n","        print(f\"{merged.shape[0]} descriptors loaded\")\n","        merged.drop_duplicates(subset=[\"smiles\"], inplace=True)\n","        print(f\"{merged.shape[0]} descriptors after dropping duplicates\")\n","        return merged\n","    else:\n","        return [open_descriptors_file(fname) for fname in fnames]\n","\n","\n","def get_descriptors_for_smiles(\n","    prefix: str,\n","    descriptors_type: str,\n","    n_clusters: int,\n","    n_iters: int,\n","    channel: str,\n","    smiles_loader: Callable,\n","    selection: str = \"\",\n","    override_fname: str = None,\n","    drop_duplicates: bool = True,\n","    behavior=\"merge\",\n",") -> pd.DataFrame:\n","    if behavior == \"merge\":\n","        do_merge = True\n","        do_concatenation = True\n","    elif behavior == \"list\":\n","        do_merge = False\n","        do_concatenation = False\n","    smiles = smiles_loader(\n","        prefix,\n","        descriptors_type,\n","        n_clusters,\n","        n_iters,\n","        channel,\n","        selection,\n","        do_concatenation,\n","    )\n","    if drop_duplicates:\n","        smiles.drop_duplicates(inplace=True)\n","    if override_fname is None:\n","        descriptors = load_descriptors(\n","            prefix, descriptors_type, n_clusters, n_iters, channel, do_merge\n","        )\n","    else:\n","        descriptors = open_descriptors_file(override_fname)\n","        if drop_duplicates:\n","            descriptors.drop_duplicates(inplace=True)\n","    if behavior == \"merge\":\n","        return pd.merge(smiles, descriptors, on=\"smiles\", how=\"inner\")\n","    elif behavior == \"list\":\n","        return [\n","            pd.merge(smiles_df, descriptors_df, on=\"smiles\", how=\"inner\")\n","            for smiles_df, descriptors_df in zip(smiles, descriptors)\n","        ]\n","\n","\n","def load_nona_column_names(pca_fname: str) -> np.ndarray:\n","    scaler, _ = pickle.load(open(f\"{SAMPLING_PATH}pca_weights/{pca_fname}.pkl\", \"rb\"))\n","    return scaler.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_smile(smile, desc_type):\n","    mol = Chem.MolFromSmiles(smile)\n","    if mol is None:\n","        return None\n","    if desc_type == \"mix\":\n","        return Descriptors.CalcMolDescriptors(mol, missingVal=None, silent=False)\n","    else:\n","        descriptors = rdMolDescriptors.MQNs_(mol)\n","        assert (\n","            len(descriptors) == 42\n","        ), f\"Expected 42 descriptors, got {len(descriptors)}\"\n","        return {f\"MQN{i}\": descriptor for i, descriptor in enumerate(descriptors)}\n","\n","\n","def calculate_descriptors(smiles, desc_type, fname):\n","    keyToData = {}\n","    for smile in tqdm(smiles, total=len(smiles)):\n","        data = process_smile(smile, desc_type)\n","        if data is None:\n","            continue\n","        else:\n","            keyToData.setdefault(\"smiles\", []).append(smile)\n","            for descriptor, value in data.items():\n","                keyToData.setdefault(descriptor, []).append(value)\n","    pd.DataFrame(keyToData).to_pickle(f\"{SAMPLING_PATH}descriptors/{fname}.pkl\")\n","\n","\n","def load_mqn_smiles():\n","    smiles_mqn100 = load_scored_all_iters(\"model7\", \"mqn\", 100, 5, \"softsub\")\n","    smiles_mqn10 = load_scored_all_iters(\"model7\", \"mqn\", 10, 4, \"softsub\")\n","    smiles_mqn = pd.concat([smiles_mqn100, smiles_mqn10])\n","    smiles_mqn.drop_duplicates(inplace=True)\n","    return smiles_mqn[\"smiles\"].to_list()\n","\n","\n","def load_mix_smiles():\n","    smiles_mix100 = load_scored_all_iters(\"model7\", \"mix\", 100, 5, \"softsub\")\n","    smiles_mix10 = load_scored_all_iters(\"model7\", \"mix\", 10, 3, \"softsub\")\n","    smiles_mix = pd.concat([smiles_mix100, smiles_mix10])\n","    smiles_mix.drop_duplicates(inplace=True)\n","    return smiles_mix[\"smiles\"].to_list()\n","\n","\n","# calculate_descriptors(load_mqn_smiles(), \"mix\", \"mqn100upto5_10upto4_mix_descs\")\n","# calculate_descriptors(load_mix_smiles(), \"mqn\", \"mix100upto5_10upto3_mqn_descs\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_all_scored_in_desc_space(desc, override_fname):\n","    scored_mix100 = get_descriptors_for_scored(\n","        \"model7\",\n","        \"mix\",\n","        100,\n","        5,\n","        \"softsub\",\n","        override_fname if desc == \"mqn\" else None,\n","        smile_loader=load_scored_all_iters,\n","    )\n","    scored_mix10 = get_descriptors_for_scored(\n","        \"model7\",\n","        \"mix\",\n","        10,\n","        3,\n","        \"softsub\",\n","        override_fname if desc == \"mqn\" else None,\n","        smile_loader=load_scored_all_iters,\n","    )\n","    scored_mqn100 = get_descriptors_for_scored(\n","        \"model7\",\n","        \"mqn\",\n","        100,\n","        5,\n","        \"softsub\",\n","        override_fname if desc == \"mix\" else None,\n","        smile_loader=load_scored_all_iters,\n","    )\n","    scored_mqn10 = get_descriptors_for_scored(\n","        \"model7\",\n","        \"mqn\",\n","        10,\n","        4,\n","        \"softsub\",\n","        override_fname if desc == \"mix\" else None,\n","        smile_loader=load_scored_all_iters,\n","    )\n","    scored = pd.concat([scored_mix100, scored_mix10, scored_mqn100, scored_mqn10])\n","    scored.drop_duplicates(inplace=True)\n","    print(scored.shape)\n","    return scored"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def explore_umap(\n","    data,\n","    desc_type: str,\n","    nearest_neighbors: list,\n","    min_distances: list,\n","    prefix: str,\n","    score_key: str = \"score\",\n","):\n","    reduced = reduce_scored_exploration(\n","        data=data,\n","        reduction=\"UMAP\",\n","        desc_type=desc_type,\n","        reduction_parameters=[\n","            {\"n_neighbors\": nn, \"min_dist\": dist}\n","            for nn in nearest_neighbors\n","            for dist in min_distances\n","        ],\n","        score_key=score_key,\n","    )\n","    pickle.dump(\n","        reduced,\n","        open(\n","            f\"{BASE_PATH}reducers/{prefix}_{desc_type}_umap_nn{','.join([str(elt) for elt in nearest_neighbors])}_mindist{','.join([str(elt) for elt in min_distances])}.pkl\",\n","            \"wb\",\n","        ),\n","    )\n","\n","\n","def explore_tsne(\n","    data,\n","    desc_type: str,\n","    perplexities: list,\n","    early_exaggerations: list,\n","    prefix: str,\n","    score_key: str = \"score\",\n","):\n","    reduced = reduce_scored_exploration(\n","        data=data,\n","        reduction=\"t-SNE\",\n","        desc_type=desc_type,\n","        reduction_parameters=[\n","            {\"perplexity\": iperp, \"early_exaggeration\": iee}\n","            for iee in early_exaggerations\n","            for iperp in perplexities\n","        ],\n","        score_key=score_key,\n","    )\n","    pickle.dump(\n","        reduced,\n","        open(\n","            f\"{BASE_PATH}reducers/{prefix}_{desc_type}_tsne_perp{','.join([str(elt) for elt in perplexities])}_ee{','.join([str(elt) for elt in early_exaggerations])}.pkl\",\n","            \"wb\",\n","        ),\n","    )\n","\n","\n","def explore_pca2d(\n","    data,\n","    desc_type: str,\n","    prefix: str,\n","    score_key: str = \"score\",\n","):\n","    if desc_type == \"mqn\":\n","        config = {\"refit_pca\": True, \"n_components\": 2}\n","    else:\n","        config = {\"reduce_to_2d\": True}\n","\n","    reduced = reduce_scored_exploration(\n","        data=data,\n","        reduction=\"PCA\",\n","        desc_type=desc_type,\n","        reduction_parameters=config,\n","        score_key=score_key,\n","    )\n","    pickle.dump(\n","        reduced,\n","        open(\n","            f\"{BASE_PATH}reducers/{prefix}_{desc_type}_pca.pkl\",\n","            \"wb\",\n","        ),\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_2d_scatterplot(\n","    datapoints: List[Tuple[np.ndarray, Union[str, np.ndarray], str]],\n","    reduction_type: str,\n","    figure_path: str,\n","    figure_fname: str,\n","    title: str,\n","    force_colors=None,\n","    yscale: float = 1.05,\n","    width=900,\n","    height=500,\n",") -> go.Figure:\n","    fig = go.Figure()\n","    graph = Graph(figure_path)\n","    minX, minY, maxX, maxY = float(\"inf\"), float(\"inf\"), float(\"-inf\"), float(\"-inf\")\n","    for i, (data, color, label) in enumerate(datapoints):\n","        if force_colors is not None:\n","            text = color\n","            color = force_colors\n","        xarr = data[:, 0]\n","        yarr = data[:, 1]\n","        minX = min(minX, min(xarr))\n","        minY = min(minY, min(yarr))\n","        maxX = max(maxX, max(xarr))\n","        maxY = max(maxY, max(yarr))\n","        fig.add_trace(\n","            go.Scatter(\n","                x=xarr,\n","                y=yarr,\n","                mode=\"markers\",\n","                name=label,\n","                visible=\"legendonly\" if i > 0 else True,\n","                text=text,\n","                marker=dict(\n","                    size=5,\n","                    color=color,\n","                    showscale=True\n","                    if force_colors is None and isinstance(color, (list, np.ndarray))\n","                    else False,\n","                    colorscale=\"Hot\",\n","                    opacity=0.5,\n","                ),\n","            )\n","        )\n","    graph.update_parameters(\n","        dict(\n","            title=title,\n","            xrange=[yscale * minX, yscale * maxX],\n","            yrange=[yscale * minY, yscale * maxY],\n","            xaxis_title=f\"{reduction_type} Component 1\",\n","            yaxis_title=f\"{reduction_type} Component 2\",\n","            width=width,\n","            height=height,\n","            showlegend=len(datapoints) > 1,\n","            xmirror=True,\n","            ymirror=True,\n","        )\n","    )\n","    graph.style_figure(fig)\n","    graph.save_figure(\n","        fig, figure_path, figure_fname, jpg=True, svg=True, pdf=False, html=True\n","    )\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reduce_scored_exploration(\n","    data: pd.DataFrame,\n","    reduction: str,\n","    desc_type: str,\n","    reduction_parameters: list = None,\n","    score_key: str = \"score\",\n","):\n","    assert reduction in {\"PCA\", \"UMAP\", \"t-SNE\"}\n","    if reduction_parameters is None:\n","        reduction_parameters = {}\n","    scores = data[score_key].to_numpy()\n","\n","    pca_fname = \"scaler_pca_combined_processed_freq1000_block133_120\"\n","    scaler, pca = pickle.load(open(f\"{SAMPLING_PATH}pca_weights/{pca_fname}.pkl\", \"rb\"))\n","    if desc_type == \"mix\":\n","        proper_columns = data[load_nona_column_names(pca_fname)]\n","        transformed = pca.transform(scaler.transform(proper_columns))\n","    elif desc_type == \"mqn\":\n","        proper_columns = data[[f\"MQN{i}\" for i in range(42)]]\n","        transformed = proper_columns.to_numpy()\n","    else:\n","        raise KeyError(f\"desc_type {desc_type} not supported\")\n","    datapoints = []\n","    match reduction:\n","        case \"PCA\":\n","            if reduction_parameters.get(\"refit_pca\", False):\n","                scaler = StandardScaler()\n","                pca = PCA(n_components=reduction_parameters[\"n_components\"])\n","                scaled = scaler.fit_transform(proper_columns)\n","                transformed = pca.fit_transform(scaled)\n","                suffix = \" (refitted)\"\n","            elif reduction_parameters.get(\"reduce_to_2d\", False):\n","                # print(f\"{type(transformed)=}, {transformed.shape=}\")\n","                transformed = transformed[:, :2]\n","                suffix = \" (2D)\"\n","            else:\n","                suffix = \"\"\n","            datapoints.append((transformed, scores, \"PCA Transformed\" + suffix))\n","        case \"UMAP\":\n","            for kwargs in tqdm(reduction_parameters, total=len(reduction_parameters)):\n","                reducer = umap.UMAP(\n","                    metric=\"euclidean\",\n","                    n_components=2,\n","                    random_state=42,\n","                    verbose=False,\n","                    **kwargs,\n","                )\n","                reduced = reducer.fit_transform(transformed)\n","                datapoints.append(\n","                    (\n","                        reduced,\n","                        scores,\n","                        f\"{kwargs['n_neighbors']} neighbors, {kwargs['min_dist']} min dist\",\n","                    )\n","                )\n","        case \"t-SNE\":\n","            for kwargs in tqdm(reduction_parameters, total=len(reduction_parameters)):\n","                tsne = TSNE(\n","                    n_components=2,\n","                    random_state=42,\n","                    metric=\"euclidean\",\n","                    **kwargs,\n","                )\n","                reduced = tsne.fit_transform(transformed)\n","                datapoints.append(\n","                    (\n","                        reduced,\n","                        scores,\n","                        f\"Perplexity {kwargs['perplexity']}, Early Exaggeration {kwargs['early_exaggeration']}\",\n","                    )\n","                )\n","    return datapoints"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_fname = \"scaler_pca_combined_processed_freq1000_block133_120\"\n","len(load_nona_column_names(pca_fname))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reduce_dataframe(\n","    data: pd.DataFrame,\n","    reduction: str,\n","    desc_type: str,\n","    reduction_parameters: dict = None,\n","):\n","    assert reduction in {\"PCA\", \"UMAP\", \"t-SNE\"}\n","    if reduction_parameters is None:\n","        reduction_parameters = {}\n","    # scores = data[score_key].to_numpy()\n","\n","    pca_fname = \"scaler_pca_combined_processed_freq1000_block133_120\"\n","    scaler, pca = pickle.load(open(f\"{SAMPLING_PATH}pca_weights/{pca_fname}.pkl\", \"rb\"))\n","    if desc_type == \"mix\":\n","        proper_columns = data[load_nona_column_names(pca_fname)]\n","        transformed = pca.transform(scaler.transform(proper_columns))\n","    elif desc_type == \"mqn\":\n","        proper_columns = data[[f\"MQN{i}\" for i in range(42)]]\n","        transformed = proper_columns.to_numpy()\n","    else:\n","        raise KeyError(f\"desc_type {desc_type} not supported\")\n","    datapoints = []\n","    match reduction:\n","        case \"PCA\":\n","            if reduction_parameters.get(\"refit_pca\", False):\n","                scaler = StandardScaler()\n","                pca = PCA(n_components=reduction_parameters[\"n_components\"])\n","                scaled = scaler.fit_transform(proper_columns)\n","                transformed = pca.fit_transform(scaled)\n","                suffix = \" (refitted)\"\n","            elif reduction_parameters.get(\"reduce_to_2d\", False):\n","                # print(f\"{type(transformed)=}, {transformed.shape=}\")\n","                transformed = transformed[:, :2]\n","                suffix = \" (2D)\"\n","            else:\n","                suffix = \"\"\n","            return transformed, \"PCA Transformed\" + suffix\n","        case \"UMAP\":\n","            reducer = umap.UMAP(\n","                metric=\"euclidean\",\n","                n_components=2,\n","                random_state=42,\n","                **reduction_parameters,\n","            )\n","            reduced = reducer.fit_transform(transformed)\n","            return (\n","                reduced,\n","                f\"{reduction_parameters['n_neighbors']} neighbors, {reduction_parameters['min_dist']} min dist\",\n","            )\n","        case \"t-SNE\":\n","            tsne = TSNE(\n","                n_components=2,\n","                random_state=42,\n","                metric=\"euclidean\",\n","                **reduction_parameters,\n","            )\n","            reduced = tsne.fit_transform(transformed)\n","            return (\n","                reduced,\n","                f\"Perplexity {reduction_parameters['perplexity']}, Early Exaggeration {reduction_parameters['early_exaggeration']}\",\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import graph\n","import importlib\n","\n","importlib.reload(graph)\n","from graph import Graph\n","\n","\n","def discretize_data_wscores(data, boundaries, bin_size, scores):\n","    bins = [np.arange(boundaries[0][i], boundaries[1][i], bin_size) for i in range(2)]\n","    hist_data, xedges, yedges, _ = scipy.stats.binned_statistic_2d(\n","        data[:, 0], data[:, 1], scores, statistic=\"mean\", bins=bins\n","    )\n","    hist_data = np.nan_to_num(hist_data, nan=0)\n","\n","    xcenters = (xedges[:-1] + xedges[1:]) / 2\n","    ycenters = (yedges[:-1] + yedges[1:]) / 2\n","    return hist_data.T, xcenters, ycenters\n","\n","\n","def get_data_boundaries(data_list):\n","    combined = np.vstack(\n","        data_list\n","    )  # Combine both datasets to get overall min and max values\n","\n","    min_val = (\n","        np.floor(np.min(combined, axis=0) / 10.0) * 10\n","    )  # Round to the nearest number divisible by 10\n","    max_val = np.ceil(np.max(combined, axis=0) / 10.0) * 10\n","    return min_val, max_val\n","\n","\n","def discretize_data(data, boundaries, bin_size):\n","    # Use 2D histogram to discretize data\n","    bins = [np.arange(boundaries[0][i], boundaries[1][i], bin_size) for i in range(2)]\n","    hist_data, xedges, yedges = np.histogram2d(data[:, 0], data[:, 1], bins=bins)\n","    # Compute bin centers\n","    xcenters = (xedges[:-1] + xedges[1:]) / 2\n","    ycenters = (yedges[:-1] + yedges[1:]) / 2\n","    return hist_data.T, xcenters, ycenters\n","\n","\n","def create_heatmap_traces_for_all_differences(args_list, bin_size):\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    traces = []\n","    for i, ((data_before, name_before), (data_after, name_after)) in enumerate(\n","        itertools.combinations(args_list, 2)\n","    ):\n","        hist_before, xcenters, ycenters = discretize_data(\n","            data_before, boundaries, bin_size\n","        )\n","        hist_after, _, _ = discretize_data(data_after, boundaries, bin_size)\n","        diff = hist_after - hist_before\n","        label = f\"|{name_after}|<br>-|{name_before}|\"\n","        traces.append(\n","            go.Heatmap(\n","                x=xcenters,\n","                y=ycenters,\n","                z=diff,\n","                zmid=0,\n","                zmax=110,\n","                zmin=-110,\n","                colorscale=\"RdBu\",\n","                name=label,\n","                showlegend=True,\n","                visible=True if i == 0 else \"legendonly\",\n","            )\n","        )\n","    return traces\n","\n","\n","def create_heatmap_trace_for_difference(args_list, bin_size):\n","    assert (\n","        len(data_list) == 2\n","    ), f\"To plot a difference, please provide only 2 data sources\"\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    hist_before, xcenters, ycenters = discretize_data(\n","        data_list[0], boundaries, bin_size\n","    )\n","    hist_after, _, _ = discretize_data(data_list[1], boundaries, bin_size)\n","    diff = hist_after - hist_before\n","    label = f\"|{name_list[1]}|<br>-|{name_list[0]}|\"\n","    return go.Heatmap(\n","        x=xcenters,\n","        y=ycenters,\n","        z=diff,\n","        zmid=0,\n","        colorscale=\"RdBu\",\n","        name=label,\n","        showlegend=True,\n","    )\n","\n","\n","def create_heatmap_traces(\n","    args_list,\n","    boundaries,\n","    bin_size,\n","    scores=None,\n","    force_zmax=None,\n","    showscale: bool = True,\n","):\n","    data_list, name_list = zip(*args_list)\n","    traces = []\n","    functor = (\n","        lambda x: discretize_data(*x[:-1])\n","        if scores is None\n","        else discretize_data_wscores(*x)\n","    )\n","    zmax = max(\n","        [functor([data, boundaries, bin_size, scores])[0].max() for data in data_list]\n","    )\n","    if force_zmax is not None:\n","        zmax = force_zmax\n","    zmin = min(\n","        [functor([data, boundaries, bin_size, scores])[0].min() for data in data_list]\n","    )\n","    for i, (data, name) in enumerate(args_list):\n","        hist, xcenters, ycenters = functor([data, boundaries, bin_size, scores])\n","        traces.append(\n","            go.Heatmap(\n","                x=xcenters,\n","                y=ycenters,\n","                z=hist,\n","                name=name,\n","                zmin=zmin,\n","                zmax=zmax,\n","                showlegend=True,\n","                colorscale=\"Thermal\",\n","                showscale=showscale,\n","                visible=True if i == 0 else \"legendonly\",\n","            )\n","        )\n","    return traces\n","\n","\n","def create_heatmap_traces_for_all_scores(\n","    args_list, bin_size, scores=None, force_zmax=None\n","):\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    traces = []\n","    functor = (\n","        lambda x: discretize_data(*x[:-1])\n","        if scores is None\n","        else discretize_data_wscores(*x)\n","    )\n","    zmax = max(\n","        [functor([data, boundaries, bin_size, scores])[0].max() for data in data_list]\n","    )\n","    if force_zmax is not None:\n","        zmax = force_zmax\n","    zmin = min(\n","        [functor([data, boundaries, bin_size, scores])[0].min() for data in data_list]\n","    )\n","    for i, (data, name) in enumerate(args_list):\n","        hist, xcenters, ycenters = functor([data, boundaries, bin_size, scores])\n","        traces.append(\n","            go.Heatmap(\n","                x=xcenters,\n","                y=ycenters,\n","                z=hist,\n","                name=name,\n","                zmin=zmin,\n","                zmax=zmax,\n","                showlegend=True,\n","                colorscale=\"Thermal\",\n","                visible=True if i == 0 else \"legendonly\",\n","            )\n","        )\n","    return traces\n","\n","\n","def plot_heatmap(\n","    args_list,\n","    title,\n","    xtitle,\n","    ytitle,\n","    figure_path: str,\n","    figure_fname: str,\n","    difference=False,\n","    bin_size=10,\n","    width=1280,\n","    height=720,\n","    all_differences=False,\n","    scores=None,\n","    force_zmax=None,\n","):\n","    fig = go.Figure()\n","    graph = Graph(f\"{BASE_PATH}plots/reducers/\")\n","\n","    if difference:\n","        if all_differences:\n","            traces = create_heatmap_traces_for_all_differences(args_list, bin_size)\n","        else:\n","            traces = [create_heatmap_trace_for_difference(args_list, bin_size)]\n","    else:\n","        traces = create_heatmap_traces_for_all_scores(\n","            args_list, bin_size, scores, force_zmax\n","        )\n","    for trace in traces:\n","        fig.add_trace(trace)\n","\n","    graph.update_parameters(\n","        dict(\n","            title=title,\n","            xaxis_title=xtitle,\n","            yaxis_title=ytitle,\n","            width=width,\n","            height=height,\n","            showlegend=len(traces) > 1,\n","        )\n","    )\n","    graph.style_figure(fig)\n","    fig.update_layout(\n","        legend=dict(x=1.2, y=1),\n","    )\n","    graph.save_figure(fig, figure_path, figure_fname, html=True)\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visualize_pickle_heatmap(\n","    fname: str,\n","    method: str,\n","    desc_type: str,\n","    figure_fname: str,\n","    force_zmax: int = None,\n","    bin_size: float = 2,\n","    width: int = 900,\n","    height: int = 500,\n","    data_index=None,\n","):\n","    data = pickle.load(\n","        open(\n","            f\"{BASE_PATH}reducers/{fname}.pkl\",\n","            \"rb\",\n","        ),\n","    )\n","    custom_scores = data[0][1].copy()\n","    # custom_scores[custom_scores < 11] = 0\n","    if data_index is not None:\n","        args_list = [(data[data_index][0], data[data_index][2])]\n","    else:\n","        args_list = [(data[i][0], data[i][2]) for i in range(len(data))]\n","    return plot_heatmap(\n","        args_list=args_list,\n","        title=f\"Scored molecules in {desc_type} space reduced with {method} (bin_size={bin_size})\",\n","        xtitle=f\"{method} Component 1\",\n","        ytitle=f\"{method} Component 2\",\n","        figure_path=f\"{BASE_PATH}plots/reducers/\",\n","        figure_fname=figure_fname,\n","        bin_size=bin_size,\n","        width=width,\n","        height=height,\n","        scores=custom_scores,\n","        force_zmax=force_zmax,\n","    )\n","\n","\n","def visualize_pickle_scatter(\n","    fname: str,\n","    method: str,\n","    desc_type: str,\n","    figure_fname: str,\n","    width: int = 900,\n","    height: int = 500,\n","    data_index=None,\n","    yscale: float = 1.05,\n","):\n","    data = pickle.load(\n","        open(\n","            f\"{BASE_PATH}reducers/{fname}.pkl\",\n","            \"rb\",\n","        ),\n","    )\n","    custom_scores = data[0][1].copy()\n","    with open(f\"{BASE_PATH}analysis/target_to_color_16.yaml\", \"r\") as f:\n","        target_to_color = yaml.load(f, Loader=yaml.FullLoader)\n","    colors = list(map(lambda x: target_to_color[x], custom_scores))\n","    if data_index is not None:\n","        data = [data[data_index]]\n","    return plot_2d_scatterplot(\n","        datapoints=data,\n","        title=f\"Distribution of binders (Kd < 100 nM) in {desc_type} space after reduction with {method}\",\n","        reduction_type=method,\n","        force_colors=colors,\n","        figure_path=f\"{BASE_PATH}plots/bindingdb_analysis/\",\n","        figure_fname=figure_fname,\n","        width=width,\n","        height=height,\n","        yscale=yscale,\n","        # scores=custom_scores,\n","        # force_zmax=force_zmax,\n","    )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize scored and BindingDB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scored_mix = load_all_scored_in_desc_space(\"mix\", \"mqn100upto5_10upto4_mix_descs\")\n","scored_mqn = load_all_scored_in_desc_space(\"mqn\", \"mix100upto5_10upto3_mqn_descs\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bindingdb_mix = add_targets_to_descriptors(\n","    \"no_rare_targets_Kd<100_freq>100\", \"bindingdb_mix_Kd<100_freq>100\"\n",")\n","bindingdb_mqn = add_targets_to_descriptors(\n","    \"no_rare_targets_Kd<100_freq>100\", \"bindingdb_mqn_Kd<100_freq>100\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["explore_umap(\n","    bindingdb_mix,\n","    \"mix\",\n","    [60, 70, 80, 90, 100],\n","    [0.99],\n","    prefix=\"bindingdb\",\n","    perform_pca=42,\n","    score_key=\"Target Name\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["explore_tsne(\n","    data=scored_mqn,\n","    desc_type=\"mqn\",\n","    perplexities=[30, 60, 90],\n","    early_exaggerations=[40],\n","    # prefix=\"bindingdb\",\n","    prefix=\"scored\",\n","    # score_key=\"Target Name\",\n","    score_key=\"score\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["configs = [\n","    # (bindingdb_mix, \"mix\", \"bindingdb\", \"Target Name\"),\n","    (bindingdb_mqn, \"mqn\", \"bindingdb\", \"Target Name\"),\n","    # (scored_mix, \"mix\", \"scored\", \"score\"),\n","    (scored_mqn, \"mqn\", \"scored\", \"score\"),\n","]\n","for dataset, desc_type, prefix, score_key in configs:\n","    explore_pca2d(\n","        data=dataset,\n","        desc_type=desc_type,\n","        prefix=prefix,\n","        score_key=score_key,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_pickle_scatter(\n","    # fname=f\"bindingdb_mix_tsne_perp30,60,90_ee40\",\n","    fname=f\"bindingdb_mqn_tsne_perp30,60,90_ee40\",\n","    method=\"t-SNE\",\n","    # desc_type=\"MIX\",\n","    desc_type=\"MQN\",\n","    # figure_fname=f\"bindingdb_mqn_tsne_perp60_ee40\",\n","    # data_index=1,\n","    width=800,\n","    height=600,\n","    yscale=1.1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_pickle_heatmap(\n","    # fname=f\"scored_mix_tsne_perp30,60,90_ee40\",\n","    fname=f\"scored_mqn_tsne_perp30,60,90_ee40\",\n","    method=\"t-SNE\",\n","    # desc_type=\"MIX\",\n","    desc_type=\"MQN\",\n","    figure_fname=f\"scored_mqn_tsne_perp60_ee40\",\n","    force_zmax=50,\n","    bin_size=1,\n","    width=700,\n","    height=500,\n","    data_index=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_pickle_scatter(\n","    fname=f\"bindingdb_mqn_pca\",\n","    method=\"PCA\",\n","    desc_type=\"MQN\",\n","    figure_fname=f\"bindingdb_mqn_pca2d\",\n","    # data_index=1,\n","    width=800,\n","    height=600,\n","    yscale=1.1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_pickle_heatmap(\n","    fname=f\"scored_mqn_pca\",\n","    method=\"PCA\",\n","    # desc_type=\"MIX\",\n","    desc_type=\"MQN\",\n","    figure_fname=f\"scored_mqn_pca2d\",\n","    force_zmax=50,\n","    bin_size=0.2,\n","    width=700,\n","    height=500,\n","    # data_index=1,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Diffusion Plots"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import List, Union\n","\n","\n","def prepare_heatmap_subplot_df(\n","    df_list: List[pd.DataFrame], verbose: bool = False, sample: Union[bool, int] = False\n","):\n","    if sample:\n","        sampled_dfs = []\n","        for df in df_list:\n","            sampled_dfs.append(df.sample(sample, replace=False, random_state=42))\n","        df_list = sampled_dfs\n","    combined_df = pd.concat(df_list).drop_duplicates(subset=[\"smiles\"])\n","    reduced, name = reduce_dataframe(\n","        combined_df,\n","        \"t-SNE\",\n","        \"mix\",\n","        dict(perplexity=60, early_exaggeration=90, verbose=verbose, n_jobs=8),\n","    )\n","    mapping = dict(zip(combined_df[\"smiles\"], reduced))\n","    return df_list, combined_df, reduced, mapping\n","\n","\n","df_list, combined_df, reduced, mapping = prepare_heatmap_subplot_df(\n","    generations, verbose=True, sample=10_000\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import graph\n","import importlib\n","\n","importlib.reload(graph)\n","from graph import Graph\n","\n","\n","def plot_heatmap_wsubplots(\n","    df_list, combined_df, mapping, width: int = 600, height: int = 400\n","):\n","    graph = Graph(f\"{BASE_PATH}plots/diffusion/\")\n","    boundaries = get_data_boundaries([reduced])\n","    rows, cols = 2, 3\n","    fig = make_subplots(\n","        rows, cols, subplot_titles=[f\"Plot {i}\" for i in range(len(df_list))]\n","    )\n","    for i, df in enumerate(df_list):\n","        df_reduced = np.array([mapping[smile] for smile in df[\"smiles\"]])\n","        fig.add_trace(\n","            create_heatmap_traces(\n","                [(df_reduced, \"name\")],\n","                boundaries,\n","                bin_size=1,\n","                force_zmax=30,\n","                showscale=i == 0,\n","            )[0],\n","            row=i // cols + 1,\n","            col=i % cols + 1,\n","        )\n","    graph.update_parameters(\n","        dict(\n","            # title=title,\n","            # xaxis_title=xtitle,\n","            # yaxis_title=ytitle,\n","            width=width,\n","            height=height,\n","            showlegend=True,\n","        )\n","    )\n","    graph.style_figure(fig)\n","    fig.update_layout(\n","        legend=dict(x=1.2, y=1),\n","    )\n","    # graph.save_figure(fig, figure_path, figure_fname, html=True)\n","    return fig\n","\n","\n","plot_heatmap_wsubplots(df_list, combined_df, mapping, width=1000, height=600)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generations = get_descriptors_for_smiles(\n","    prefix=\"model7\",\n","    descriptors_type=\"mix\",\n","    n_clusters=100,\n","    n_iters=5,\n","    channel=\"softsub\",\n","    selection=\"threshold11_softmax_sub\",\n","    # smiles_loader=load_altrain_all_iters,\n","    smiles_loader=load_generations_all_iters,\n","    drop_duplicates=False,\n","    behavior=\"list\",\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.graph_objs as go\n","\n","\n","def plot_correlation_circle(pca, features):\n","    pcs = pca.components_\n","\n","    # Create a trace for the variable vectors\n","    vectors = go.Scatter(\n","        x=pcs[0, :],\n","        y=pcs[1, :],\n","        mode=\"lines+markers+text\",\n","        text=features,\n","        textposition=\"top center\",\n","        line=dict(color=\"red\"),\n","        marker=dict(size=10, color=\"blue\"),\n","        textfont=dict(size=8),\n","    )\n","\n","    # Create a trace for the unit circle\n","    circle = go.Scatter(\n","        x=np.cos(np.linspace(0, 2 * np.pi, 100)),\n","        y=np.sin(np.linspace(0, 2 * np.pi, 100)),\n","        mode=\"lines\",\n","        line=dict(color=\"blue\", width=1),\n","        showlegend=False,\n","    )\n","\n","    layout = go.Layout(\n","        title=\"Correlation Circle\",\n","        autosize=False,\n","        width=800,\n","        height=800,\n","        showlegend=False,\n","        xaxis=dict(\n","            title=f\"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\",\n","            range=[-1.1, 1.1],\n","            zeroline=False,\n","            showgrid=True,\n","            domain=[0, 1],\n","        ),\n","        yaxis=dict(\n","            title=f\"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\",\n","            range=[-1.1, 1.1],\n","            zeroline=False,\n","            showgrid=True,\n","            domain=[0, 1],\n","        ),\n","    )\n","\n","    fig = go.Figure(data=[vectors, circle], layout=layout)\n","    fig.show()\n","\n","\n","# Assuming pca is your PCA model fitted with sklearn and df is the pandas dataframe with your original data\n","plot_correlation_circle(pca, descriptors.columns.values)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN0LSJZYW8oGeOa28jfcLb3","collapsed_sections":["AJJ6x8zuwPM6","giWJddP_21Qw","MHqNMSWvHLmu","gl2kNWZi3cA8","zDT4LDmYUOXA"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
