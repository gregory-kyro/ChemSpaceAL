{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22548,"status":"ok","timestamp":1692042174420,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"qhUCZq9nvmGP","outputId":"d2e18ac9-803d-40b4-be07-bef1059deb8f"},"outputs":[],"source":["# mount google drive\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6040,"status":"ok","timestamp":1692042180454,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"McHGEQv_u-8r"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from rdkit import Chem\n","from rdkit.Chem import Descriptors\n","from rdkit.Chem import rdMolDescriptors\n","import plotly.graph_objects as go\n","import pickle\n","from pprint import pprint as pp\n","from tqdm import tqdm\n","import itertools\n","import umap\n","from sklearn.manifold import TSNE\n","import scipy\n","\n","\n","# BASE_PATH = '/content/drive/MyDrive/Generative_ML/current_data/' #@param {type:\"string\"}\n","BASE_PATH = \"/Users/morgunov/batista/Summer/pipeline/\"\n","\n","PRETRAINING_PATH = BASE_PATH + \"1. Pretraining/\"\n","GENERATION_PATH = BASE_PATH + \"2. Generation/\"\n","SAMPLING_PATH = BASE_PATH + \"3. Sampling/\"\n","DIFFDOCK_PATH = BASE_PATH + \"4. DiffDock/\"\n","SCORING_PATH = BASE_PATH + \"5. Scoring/\"\n","AL_PATH = BASE_PATH + \"6. ActiveLearning/\"\n","PICKLES = BASE_PATH + \"Archive/pickle/\""]},{"cell_type":"markdown","metadata":{"id":"tqjuAeINwMDW"},"source":["# Processing dataset descriptors"]},{"cell_type":"markdown","metadata":{"id":"AJJ6x8zuwPM6"},"source":["## Convert each dictionary to a dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7sdUnA39c2X"},"outputs":[],"source":["def split_the_dictionary(fname):\n","    with open(PICKLES + f\"{fname}.pkl\", \"rb\") as f:\n","        smiles_to_descriptors = pickle.load(f)\n","    smiles = list(smiles_to_descriptors.keys())\n","    half_index = int(len(smiles) // 2)\n","    pt1 = {}\n","    pt2 = {}\n","    for i, smile in enumerate(smiles):\n","        if i < half_index:\n","            pt1[smile] = smiles_to_descriptors[smile]\n","        else:\n","            pt2[smile] = smiles_to_descriptors[smile]\n","    print(len(pt1), len(pt2), len(pt1) + len(pt2), len(smiles_to_descriptors))\n","    pickle.dump(pt1, open(PICKLES + f\"{fname}_subpt1.pkl\", \"wb\"))\n","    pickle.dump(pt2, open(PICKLES + f\"{fname}_subpt2.pkl\", \"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KC_EdZfO9gFI"},"outputs":[],"source":["split_the_dictionary(\"smile_to_descriptors_pt1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8fjkgHHwPeo"},"outputs":[],"source":["def pickle_to_csv(fname):\n","    with open(PICKLES + f\"{fname}.pkl\", \"rb\") as f:\n","        smiles_to_descriptors = pickle.load(f)\n","    keyToData = {}\n","    keys = pickle.load(open(PICKLES + \"descriptors_list.pkl\", \"rb\"))\n","    pbar = tqdm(\n","        smiles_to_descriptors.items(), total=len(smiles_to_descriptors), desc=fname\n","    )\n","    for smile, descriptors in pbar:\n","        keyToData.setdefault(\"smile\", []).append(smile)\n","        for key in keys:\n","            keyToData.setdefault(key, []).append(descriptors[key])\n","    df = pd.DataFrame(keyToData)\n","    df.to_pickle(PICKLES + \"_\".join(fname.split(\"_\")[2:]) + \".pkl\")\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zx1dTYunJJVb"},"outputs":[],"source":["pickle_to_csv(\"smile_to_descriptors_pt1_subpt1\")  # done\n","pickle_to_csv(\"smile_to_descriptors_pt1_subpt2\")  # done\n","pickle_to_csv(\"smile_to_descriptors_pt2_subpt1\")  # done\n","pickle_to_csv(\"smile_to_descriptors_pt2_subpt2\")\n","pickle_to_csv(\"smile_to_descriptors_pt3\")  # done"]},{"cell_type":"markdown","metadata":{"id":"giWJddP_21Qw"},"source":["## Combine dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wt-JolBT27FL"},"outputs":[],"source":["def merge_parts():\n","    pt1_sbpt1 = pd.read_pickle(PICKLES + \"descriptors_pt1_subpt1.pkl\")\n","    pt1_sbpt2 = pd.read_pickle(PICKLES + \"descriptors_pt1_subpt2.pkl\")\n","    pt2_sbpt1 = pd.read_pickle(PICKLES + \"descriptors_pt2_subpt1.pkl\")\n","    pt2_sbpt2 = pd.read_pickle(PICKLES + \"descriptors_pt2_subpt2.pkl\")\n","    pt3 = pd.read_pickle(PICKLES + \"descriptors_pt3.pkl\")\n","    print(pt1_sbpt1.shape, pt1_sbpt2.shape, pt2_sbpt1.shape, pt2_sbpt2.shape, pt3.shape)\n","    merged_df = pd.concat([pt1_sbpt1, pt1_sbpt2, pt2_sbpt1, pt2_sbpt2, pt3])\n","    print(merged_df.shape)\n","    merged_df.to_pickle(PICKLES + \"descriptors_combined.pkl\")\n","\n","\n","merge_parts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkqyAb_SjrHu"},"outputs":[],"source":["def analyze_training_sets():\n","    def combine_sets(train_fname, valid_fname):\n","        train_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{train_fname}.csv.gz\")\n","        valid_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{valid_fname}.csv.gz\")\n","        combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n","        return combined_df\n","\n","    train_100 = combine_sets(\n","        \"combined_processed_freq100_block133_train\",\n","        \"combined_processed_freq100_block133_val\",\n","    )\n","    train_1000 = combine_sets(\n","        \"combined_processed_freq1000_block133_train\",\n","        \"combined_processed_freq1000_block133_val\",\n","    )\n","    print(f\"{train_100.shape=}, {train_1000.shape=}\")\n","    in100 = set(train_100[\"smiles\"]) - set(train_1000[\"smiles\"])\n","    in1000 = set(train_1000[\"smiles\"]) - set(train_100[\"smiles\"])\n","    print(f\"{len(in100)=}, {len(in1000)=}\")\n","\n","\n","analyze_training_sets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNsSvD1kRwqQ"},"outputs":[],"source":["def fill_descriptors_for_training_set(\n","    all_descriptors_path, train_fname, valid_fname, extra_mols_path=None\n","):\n","    descriptors_wsmiles = pd.read_pickle(PICKLES + \"descriptors_combined.pkl\")\n","    descriptors_wsmiles.rename(columns={\"smile\": \"smiles\"}, inplace=True)\n","    print(f\"Descriptors df has shape {descriptors_wsmiles.shape}\")\n","    train_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{train_fname}.csv.gz\")\n","    valid_df = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{valid_fname}.csv.gz\")\n","    combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n","    # assert len(set(descriptors_wsmiles['smiles'])-set(descriptors_wsmiles['smiles'])) == 0, \"Combined df\"\n","    print(f\"Training/validation df has shape {combined_df.shape}\")\n","    merged_df = combined_df.merge(descriptors_wsmiles, on=\"smiles\", how=\"inner\")\n","    extra_mols = set(combined_df[\"smiles\"]) - set(merged_df[\"smiles\"])\n","    if len(extra_mols) != 0:\n","        print(f\"Extra molecules in the training set detected\")\n","        if extra_mols_path is None:\n","            pd.DataFrame({\"smiles\": list(extra_mols)}).to_csv(\n","                f\"{PRETRAINING_PATH}descriptors/{train_fname[:-6]}_extra_mols.csv\"\n","            )\n","            print(\n","                f\"Saved extra mols to {PRETRAINING_PATH}descriptors/{train_fname[:-6]}_extra_mols.csv, abort execution\"\n","            )\n","            return\n","        else:\n","            extra_descriptors = pd.read_pickle(extra_mols_path)\n","            assert (\n","                len(set(extra_descriptors[\"smiles\"]) - extra_mols) == 0\n","            ), \"Extra descriptors do not cover all extra_mols\"\n","            extra_descriptors = extra_descriptors[\n","                extra_descriptors[\"smiles\"].isin(extra_mols)\n","            ]\n","\n","    final_df = pd.concat([merged_df, extra_descriptors])\n","    print(f\"Final descriptors df has shape {final_df.shape}\")\n","    assert (\n","        final_df.shape[0] == combined_df.shape[0]\n","    ), \"Number of molecules in training/valid set and descriptors df do not match\"\n","    final_df.to_pickle(f\"{PRETRAINING_PATH}descriptors/{train_fname[:-6]}.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDbUmjrUb8qC"},"outputs":[],"source":["fill_descriptors_for_training_set(\n","    f\"{PICKLES}descriptors_combined.pkl\",\n","    \"combined_processed_freq1000_block133_train\",\n","    \"combined_processed_freq1000_block133_val\",\n","    f\"{PRETRAINING_PATH}descriptors/combined_processed_freq1000_block133_extra_mols.pkl\",\n",")\n","\n","# For 100\n","# Descriptors df has shape (5770637, 210)\n","# Training/validation df has shape (5560874, 2)\n","# Extra molecules in the training set detected\n","# Final descriptors df has shape (5560874, 211)"]},{"cell_type":"markdown","metadata":{"id":"MHqNMSWvHLmu"},"source":["# Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1692042180455,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"m-TMXhRT3bob"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK5Pz7eT3j_H"},"outputs":[],"source":["def fit_pca(dataframe, n=3, sigma=None, whiten=False):\n","    scaler = StandardScaler()\n","    scaled_data = scaler.fit_transform(dataframe)\n","    if sigma is not None:\n","        scaled_data = scaled_data[(scaled_data <= sigma).all(axis=1)]\n","    pca = PCA(n_components=n, whiten=whiten)\n","    pca.fit(scaled_data)\n","    return scaler, pca\n","\n","\n","def pca_transform(pca, dataframe, n):\n","    assert (\n","        pca.n_components_ >= n\n","    ), f\"PCA was fitted on {pca.n_components_} components, but {n} were requested.\"\n","    transformed = pca.transform(dataframe)\n","    return [transformed[:, i] for i in range(n)]\n","\n","\n","def plot_pca(scaler, pca, datapoints, yscale=1.05):\n","    fig = go.Figure()\n","    minX, minY, maxX, maxY = float(\"inf\"), float(\"inf\"), float(\"-inf\"), float(\"-inf\")\n","    traces = []\n","    for data, color, label in datapoints:\n","        transformed = pca.transform(\n","            scaler.transform(data[scaler.get_feature_names_out()])\n","        )\n","        xarr = transformed[:, 0]\n","        yarr = transformed[:, 1]\n","        minX = min(minX, min(xarr))\n","        minY = min(minY, min(yarr))\n","        maxX = max(maxX, max(xarr))\n","        maxY = max(maxY, max(yarr))\n","        fig.add_trace(\n","            go.Scatter(\n","                x=xarr,\n","                y=yarr,\n","                mode=\"markers\",\n","                name=label,\n","                marker=dict(\n","                    size=5,\n","                    color=color,\n","                    showscale=True if isinstance(color, (list, np.ndarray)) else False,\n","                    colorscale=\"Viridis\",\n","                    opacity=0.5,\n","                ),\n","            )\n","        )\n","\n","    fig.update_layout(\n","        xaxis=dict(\n","            title=\"PCA Component 1\",\n","            autorange=False,\n","            range=[yscale * minX, yscale * maxX],\n","        ),\n","        yaxis=dict(\n","            title=\"PCA Component 2\",\n","            autorange=False,\n","            range=[yscale * minY, yscale * maxY],\n","        ),\n","    )\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBQQugKc3p4t"},"outputs":[],"source":["def get_data_boundaries(data_list):\n","    combined = np.vstack(\n","        data_list\n","    )  # Combine both datasets to get overall min and max values\n","\n","    min_val = (\n","        np.floor(np.min(combined, axis=0) / 10.0) * 10\n","    )  # Round to the nearest number divisible by 10\n","    max_val = np.ceil(np.max(combined, axis=0) / 10.0) * 10\n","    return min_val, max_val\n","\n","\n","def discretize_data(data, boundaries, bin_size):\n","    # Use 2D histogram to discretize data\n","    bins = [np.arange(boundaries[0][i], boundaries[1][i], bin_size) for i in range(2)]\n","    hist_data, xedges, yedges = np.histogram2d(data[:, 0], data[:, 1], bins=bins)\n","\n","    # Compute bin centers\n","    xcenters = (xedges[:-1] + xedges[1:]) / 2\n","    ycenters = (yedges[:-1] + yedges[1:]) / 2\n","\n","    return hist_data.T, xcenters, ycenters\n","\n","\n","def plot_heatmap(\n","    args_list,\n","    difference=False,\n","    bin_size=10,\n","    width=1280,\n","    height=720,\n","    all_differences=False,\n","):\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    fig = go.Figure()\n","\n","    if difference:\n","        if all_differences:\n","            traces = []\n","            for i, ((data_before, name_before), (data_after, name_after)) in enumerate(\n","                itertools.combinations(args_list, 2)\n","            ):\n","                hist_before, xcenters, ycenters = discretize_data(\n","                    data_before, boundaries, bin_size\n","                )\n","                hist_after, _, _ = discretize_data(data_after, boundaries, bin_size)\n","                diff = hist_after - hist_before\n","                label = f\"|{name_after}|<br>-|{name_before}|\"\n","                traces.append(\n","                    go.Heatmap(\n","                        x=xcenters,\n","                        y=ycenters,\n","                        z=diff,\n","                        zmid=0,\n","                        zmax=110,\n","                        zmin=-110,\n","                        colorscale=\"RdBu\",\n","                        name=label,\n","                        showlegend=True,\n","                        visible=True if i == 0 else \"legendonly\",\n","                    )\n","                )\n","            for trace in traces:\n","                fig.add_trace(trace)\n","        else:\n","            assert (\n","                len(data_list) == 2\n","            ), f\"To plot a difference, please provide only 2 data sources\"\n","            hist_before, xcenters, ycenters = discretize_data(\n","                data_list[0], boundaries, bin_size\n","            )\n","            hist_after, _, _ = discretize_data(data_list[1], boundaries, bin_size)\n","            diff = hist_after - hist_before\n","            label = f\"|{name_list[1]}|<br>-|{name_list[0]}|\"\n","            fig.add_trace(\n","                go.Heatmap(\n","                    x=xcenters,\n","                    y=ycenters,\n","                    z=diff,\n","                    zmid=0,\n","                    colorscale=\"RdBu\",\n","                    name=label,\n","                    showlegend=True,\n","                )\n","            )\n","    else:\n","        traces = []\n","        zmax = max(\n","            [discretize_data(data, boundaries, bin_size)[0].max() for data in data_list]\n","        )\n","        zmin = min(\n","            [discretize_data(data, boundaries, bin_size)[0].min() for data in data_list]\n","        )\n","        for i, (data, name) in enumerate(args_list):\n","            hist, xcenters, ycenters = discretize_data(data, boundaries, bin_size)\n","            if \"al1 good\" in name:\n","                mult = 1 / 50\n","            else:\n","                mult = 1\n","            traces.append(\n","                go.Heatmap(\n","                    x=xcenters,\n","                    y=ycenters,\n","                    z=hist,\n","                    name=name,\n","                    zmin=zmin,\n","                    zmax=mult * zmax,\n","                    showlegend=True,\n","                    visible=True if i == 0 else \"legendonly\",\n","                )\n","            )\n","        for trace in traces:\n","            fig.add_trace(trace)\n","\n","    fig.update_layout(\n","        title=f\"Difference in distribution: # of datapoints per bin ({bin_size=})\",\n","        xaxis_title=\"X\",\n","        yaxis_title=\"Y\",\n","        width=width,\n","        height=height,\n","        legend=dict(x=1.2, y=1),\n","    )\n","\n","    return fig"]},{"cell_type":"markdown","metadata":{"id":"K4dxpsH33a5m"},"source":["# PCA Analysis"]},{"cell_type":"markdown","metadata":{"id":"EZDzTkI4U0Xw"},"source":["## preprocessing & fitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFnBoUPrPwUe"},"outputs":[],"source":["def remove_mols_with_nan_descriptors(descriptors_fname):\n","    descriptors = pd.read_pickle(\n","        f\"{PRETRAINING_PATH}descriptors/{descriptors_fname}.pkl\"\n","    ).drop(columns=[\"Unnamed: 0\"])\n","    dataset = pd.read_csv(f\"{PRETRAINING_PATH}datasets/{descriptors_fname}.csv.gz\")\n","    assert set(descriptors[\"smiles\"]) == set(\n","        dataset[\"smiles\"]\n","    ), \"Descriptors object contains smiles different from the training dataset\"\n","    nan_smiles = set(descriptors[descriptors.isna().any(axis=1)][\"smiles\"])\n","    print(f\"There are {len(nan_smiles)=}\")\n","    descriptors_nonan = descriptors[~descriptors[\"smiles\"].isin(nan_smiles)]\n","    dataset_nonan = dataset[~dataset[\"smiles\"].isin(nan_smiles)]\n","    print(f\"Had {descriptors.shape} before\")\n","    print(f\"Now have {descriptors_nonan.shape} after\")\n","    assert set(descriptors_nonan[\"smiles\"]) == set(\n","        dataset_nonan[\"smiles\"]\n","    ), \"Descriptors object contains smiles different from the training dataset\"\n","    descriptors_nonan.to_pickle(\n","        f\"{PRETRAINING_PATH}descriptors/{descriptors_fname}_nonan.pkl\"\n","    )\n","    dataset_nonan.to_csv(f\"{PRETRAINING_PATH}datasets/{descriptors_fname}_nonan.csv.gz\")\n","\n","\n","def fit_pca_on_dataset(descriptors_fname, n_comps=100, drop_nan=False):\n","    descriptors = pd.read_pickle(\n","        f\"{PRETRAINING_PATH}descriptors/{descriptors_fname}.pkl\"\n","    ).drop(columns=[\"smiles\", \"Ipc\"])\n","    nan_cols = descriptors.columns[descriptors.isna().any()]\n","    if drop_nan:\n","        descriptors.drop(columns=nan_cols, inplace=True)\n","    scaler, pca = fit_pca(descriptors, n=n_comps)\n","    pickle.dump(\n","        (scaler, pca),\n","        open(\n","            f\"{SAMPLING_PATH}pca_weights/scaler_pca_{descriptors_fname}_{n_comps}.pkl\",\n","            \"wb\",\n","        ),\n","    )\n","    return scaler, pca\n","\n","\n","nan_rows = remove_mols_with_nan_descriptors(\"combined_processed_freq1000_block133\")\n","# scaler, pca = fit_pca_on_dataset(\"combined_processed_freq1000_block133\", n_comps=120, drop_nan=True)\n","scaler, pca = fit_pca_on_dataset(\n","    \"combined_processed_freq1000_block133_nonan\", n_comps=120, drop_nan=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1691525435350,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"NF5NwescXhJ0","outputId":"e39aabcf-1c56-4912-c41d-c5b708a8ed41"},"outputs":[],"source":["# scaler, pca = pickle.load(open(f\"{SAMPLING_PATH}pca_weights/scaler_pca_combined_processed_freq1000_block133_nonan\", 'rb'))\n","import matplotlib.pyplot as plt\n","\n","var_arr = pca.explained_variance_ratio_\n","print(sum(var_arr))\n","print(sum(var_arr[:2]))\n","cum_varr = np.cumsum(var_arr)\n","plt.plot([1 for _ in range(len(var_arr))])\n","plt.plot(cum_varr)\n","print(sum(var_arr[:100]))\n","print(np.where(cum_varr > 0.99))\n","print(np.where(cum_varr > 0.995))\n","print(np.where(cum_varr > 0.999))"]},{"cell_type":"markdown","metadata":{"id":"8do8XB19U6Zc"},"source":["## visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["biscale = {\n","    \"blue\": (\"#03045e\", \"#023e8a\"),\n","    \"purple\": (\"#7b2cbf\", \"#c77dff\"),\n","    \"green\": (\"#008000\", \"#70e000\"),\n","    \"red\": (\"#a4133c\", \"#ff4d6d\"),\n","}\n","from IPython.display import Markdown, display\n","\n","biscale.update(\n","    {\n","        \"orange\": (\"#e85d04\", \"#faa307\"),\n","        # \"pink\": (\"#d00000\", \"#ff6f69\"),\n","        # \"yellow\": (\"#c5a880\", \"#ffdd00\"),\n","        \"dark_grey\": (\"#2e2e2e\", \"#5a5a5a\"),\n","        # \"gray\": (\"#6d6875\", \"#a5a5a5\"),\n","        \"teal\": (\"#006a71\", \"#48cae4\"),\n","        # \"violet\": (\"#3a0ca3\", \"#6f23ff\"),\n","        \"lime\": (\"#679436\", \"#aee833\"),\n","        # \"indigo\": (\"#3c096c\", \"#5e60ce\"),\n","        \"gold\": (\"#b08d57\", \"#f0e442\"),\n","        # \"cyan\": (\"#0077b6\", \"#00b4d8\"),\n","        # \"magenta\": (\"#7209b7\", \"#e43f5a\"),\n","        \"beige\": (\"#bfa58a\", \"#f5deb3\"),\n","        # \"turquoise\": (\"#00707b\", \"#00a19d\"),\n","        # \"olive\": (\"#6a994e\", \"#a7c957\"),\n","        \"maroon\": (\"#4a0000\", \"#800000\"),\n","        # \"coral\": (\"#ff4b5c\", \"#ff6b6b\")\n","    }\n",")\n","\n","for scale, colors in biscale.items():\n","    print(scale)\n","    display(\n","        Markdown(\n","            \"<br>\".join(\n","                f'<span style=\"font-family: monospace\">{color} <span style=\"color: {color}\">████████</span></span>'\n","                for color in colors\n","            )\n","        )\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["biscale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sy449iwptvMU"},"outputs":[],"source":["scaler, pca = pickle.load(\n","    open(\n","        f\"{SAMPLING_PATH}pca_weights/scaler_pca_combined_processed_freq100_block133.pkl\",\n","        \"rb\",\n","    )\n",")\n","columns = pickle.load(\n","    open(\n","        f\"{SAMPLING_PATH}descriptors/combined_processed_freq100_block133_columnlist.pkl\",\n","        \"rb\",\n","    )\n",")\n","# descriptors = pd.read_pickle(f\"{PRETRAINING_PATH}descriptors/combined_processed_freq100_block133.pkl\")\n","gen100 = pd.read_pickle(f\"{SAMPLING_PATH}descriptors/model4_baseline_temp1.0.pkl\")\n","gen1000 = pd.read_pickle(f\"{SAMPLING_PATH}descriptors/model5_baseline_temp1.0.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0urfSS03uxoQ"},"outputs":[],"source":["sample = 5_000\n","seed = 42\n","colors = {\n","    \"gunmetal\": \"#31393C\",\n","    \"blue\": \"#4361EE\",\n","    \"plum\": \"#8E338C\",\n","    \"grape\": \"#7209B7\",\n","    \"red\": \"#D90429\",\n","    \"orange\": \"#FF7B00\",\n","    \"yellow\": \"#FFBA08\",\n","    \"mindaro\": \"#CBFF8C\",\n","}\n","\n","plot_pca(\n","    [\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gen100[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"gunmetal\"],\n","            f\"100 freq\",\n","        ),\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gen1000[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"grape\"],\n","            f\"1000 freq\",\n","        ),\n","    ],\n","    yscale=1.5,\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zFp9UAGZRVR"},"outputs":[],"source":["scaler, pca = pickle.load(\n","    open(f\"{SAMPLING_PATH}pca_weights/scaler_pca_moses+bindingdb.pkl\", \"rb\")\n",")\n","columns = pickle.load(\n","    open(f\"{SAMPLING_PATH}descriptors/descriptors_moses+bindingdb_columnlist.pkl\", \"rb\")\n",")\n","load_baseline = lambda fname: pd.read_pickle(\n","    f\"{SAMPLING_PATH}descriptors/{fname}.pkl\"\n",").drop_duplicates(subset=\"smiles\")\n","gpt_base = load_baseline(\"model1_baseline_temp1.0\")\n","gpt_sub1 = load_baseline(\"model1_softsub_al1_temp1.0\")\n","gpt_div1 = load_baseline(\"model1_softdiv_al1_temp1.0\")\n","gpt_sub2 = load_baseline(\"model1_softsub_al2_temp1.0\")\n","gpt_div2 = load_baseline(\"model1_softdiv_al2_temp1.0\")\n","\n","pre_base = \"model1_baseline_threshold11\"\n","pre_sub1 = \"model1_softsub_al1_threshold11\"\n","pre_div1 = \"model1_softdiv_al1_threshold11\"\n","pre_sub2 = \"model1_softsub_al2_threshold11\"\n","pre_div2 = \"model1_softdiv_al2_threshold11\"\n","l_base = lambda fname: gpt_base[\n","    gpt_base[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_base}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_sub1 = lambda fname: gpt_sub1[\n","    gpt_sub1[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_sub1}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_div1 = lambda fname: gpt_div1[\n","    gpt_div1[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_div1}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_sub2 = lambda fname: gpt_sub2[\n","    gpt_sub2[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_sub2}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]\n","l_div2 = lambda fname: gpt_div2[\n","    gpt_div2[\"smiles\"].isin(\n","        pd.read_csv(f\"{AL_PATH}training_sets/{pre_div2}_{fname}.csv\")[\"smiles\"].unique()\n","    )\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhRKcAFhlKw5"},"outputs":[],"source":["print(\n","    l_base(\"linear\").shape,\n","    l_base(\"linear_noscore\").shape,\n","    l_base(\"softmax_divf0.25\").shape,\n","    l_base(\"softmax_sub\").shape,\n",")\n","print(\n","    l_sub1(\"softmax_sub\").shape,\n","    l_sub1(\"softmax_sub_noscore\").shape,\n","    l_div1(\"softmax_divf0.25\").shape,\n","    l_div1(\"softmax_divf0.25_noscore\").shape,\n",")\n","print(\n","    l_sub2(\"softmax_sub\").shape,\n","    l_sub2(\"softmax_sub_noscore\").shape,\n","    l_div2(\"softmax_divf0.25\").shape,\n","    l_div2(\"softmax_divf0.25_noscore\").shape,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDeBBcapZq5_"},"outputs":[],"source":["sample = 5_000\n","seed = 42\n","colors = {\n","    \"gunmetal\": \"#31393C\",\n","    \"blue\": \"#4361EE\",\n","    \"plum\": \"#8E338C\",\n","    \"grape\": \"#7209B7\",\n","    \"red\": \"#D90429\",\n","    \"orange\": \"#FF7B00\",\n","    \"yellow\": \"#FFBA08\",\n","    \"mindaro\": \"#CBFF8C\",\n","}\n","\n","scatter = lambda loader, fname: pca_transform(\n","    pca, scaler.transform(loader(fname)[columns]), n=2\n",")\n","plot_pca(\n","    [\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gpt_base[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"gunmetal\"],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gpt_sub1[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"grape\"],\n","            f\"GPT Softmax Sub\",\n","        ),\n","        (\n","            *pca_transform(\n","                pca,\n","                scaler.transform(gpt_div1[columns].sample(n=sample, random_state=seed)),\n","                n=2,\n","            ),\n","            colors[\"red\"],\n","            f\"GPT Softmax Div0.25\",\n","        ),\n","        # (*scatter('linear'), colors[\"blue\"], f'AL1 Linear'),\n","        (*scatter(l_base, \"linear_noscore\"), colors[\"mindaro\"], f\"AL1 Diffusion\"),\n","        # (*scatter(l_base, 'softmax_divf0.25'), colors[\"grape\"], f'AL1 Softmax Div0.25'),\n","        # (*scatter(l_base, 'softmax_sub'), colors[\"red\"], f'AL1 Softmax Sub'),\n","    ],\n","    yscale=1.5,\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_udJYnMowyyF"},"outputs":[],"source":["# @title\n","import matplotlib.pyplot as plt\n","\n","var_arr = pca.explained_variance_ratio_\n","print(sum(var_arr))\n","print(sum(var_arr[:2]))\n","cum_varr = np.cumsum(var_arr)\n","plt.plot(var_arr)\n","plt.plot([1 for _ in range(len(var_arr))])\n","plt.plot(cum_varr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coTkRXm0VDqH"},"outputs":[],"source":["heat = lambda loader, fname: pca.transform(scaler.transform(loader(fname)[columns]))[\n","    :, :2\n","]\n","plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (heat(l_base, \"linear_noscore\"), f\"Base Diffusion\"),\n","        # (pca.transform(scaler.transform(al1_softmax_div_nosc[columns]))[:, :2], f'AL1 Diffusion Softmax Div0.25'),\n","        # (pca.transform(scaler.transform(al1_softmax_sub_nosc[columns]))[:, :2], f'AL1 Diffusion Softmax Sub'),\n","        (heat(l_base, \"linear\"), f\"AL1 Linear\"),\n","        (heat(l_base, \"softmax_divf0.25\"), f\"AL1 Softmax Div0.25\"),\n","        (heat(l_base, \"softmax_sub\"), f\"AL1 Softmax Sub\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-ABhzYUVEK6"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_sub1[columns].sample(n=5_406, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Sub1\",\n","        ),\n","        (heat(l_sub1, \"softmax_sub_noscore\"), f\"Sub1 Diffusion\"),\n","        (heat(l_sub1, \"softmax_sub\"), f\"AL2 Softmax Sub\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2Cdc-yBgYUv"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_div1[columns].sample(n=5_363, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Div1\",\n","        ),\n","        (heat(l_div1, \"softmax_divf0.25_noscore\"), f\"Div1 Diffusion\"),\n","        (heat(l_div1, \"softmax_divf0.25\"), f\"AL2 Softmax Div\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiX5wKedFQU7"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_sub2[columns].sample(n=5_406, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Sub2\",\n","        ),\n","        (heat(l_sub2, \"softmax_sub_noscore\"), f\"Sub2 Diffusion\"),\n","        (heat(l_sub2, \"softmax_sub\"), f\"AL3 Softmax Sub\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FO97WCiwzT_s"},"outputs":[],"source":["l_div2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbhu1-6pFuAI"},"outputs":[],"source":["plot_heatmap(\n","    [\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_base[columns].sample(n=5_304, random_state=seed))\n","            )[:, :2],\n","            f\"GPT baseline\",\n","        ),\n","        (\n","            pca.transform(\n","                scaler.transform(gpt_div2[columns].sample(n=5_363, random_state=seed))\n","            )[:, :2],\n","            f\"GPT Div2\",\n","        ),\n","        (heat(l_div2, \"softmax_divf0.25_noscore\"), f\"Div2 Diffusion\"),\n","        (heat(l_div2, \"softmax_divf0.25\"), f\"AL3 Softmax Div\"),\n","    ],\n","    difference=True,\n","    all_differences=True,\n","    bin_size=1.5,\n","    width=900,\n","    height=500,\n",")"]},{"cell_type":"markdown","metadata":{"id":"B7iDVys3ArTo"},"source":["# Distribution Analysis"]},{"cell_type":"markdown","metadata":{"id":"zDT4LDmYUOXA"},"source":["## Definitions"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":812,"status":"ok","timestamp":1692042181262,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"6UcS6dFJUPNi"},"outputs":[],"source":["import numpy as np\n","import plotly.graph_objects as go\n","from scipy.stats import gaussian_kde\n","from graph import Graph\n","import pprint\n","\n","pp = pprint.PrettyPrinter(indent=1, compact=False, width=100)\n","\n","\n","def load_dist(fname):\n","    return pd.read_csv(f\"{SCORING_PATH}scored_dataframes/{fname}.csv\")[\n","        \"score\"\n","    ].to_numpy()\n","\n","\n","def compute_cluster_scores(fname):\n","    good_data = pd.read_csv(f\"{SCORING_PATH}scored_dataframes/{fname}.csv\")\n","    cluster_to_scores = {}\n","    for index, row in good_data.iterrows():\n","        cluster_to_scores.setdefault(row[\"cluster_id\"], []).append(row[\"score\"])\n","    cluster_to_score = {\n","        cluster_id: np.mean(scores) for cluster_id, scores in cluster_to_scores.items()\n","    }\n","    return np.array(list(cluster_to_score.values()))\n","\n","\n","# dark to light\n","biscale = {\n","    \"blue\": (\"#03045e\", \"#023e8a\"),\n","    \"purple\": (\"#7b2cbf\", \"#c77dff\"),\n","    \"green\": (\"#008000\", \"#70e000\"),\n","    \"red\": (\"#a4133c\", \"#ff4d6d\"),\n","    \"orange\": (\"#e85d04\", \"#faa307\"),\n","    \"teal\": (\"#006a71\", \"#48cae4\"),\n","    \"lime\": (\"#679436\", \"#aee833\"),\n","    \"gold\": (\"#b08d57\", \"#f0e442\"),\n","    \"beige\": (\"#bfa58a\", \"#f5deb3\"),\n","    \"maroon\": (\"#4a0000\", \"#800000\"),\n","    \"dark_grey\": (\"#000000\", \"#2e2e2e\"),\n","}\n","\n","\n","def create_hist_trace(i, data, label, color, threshold, bin_step, trace_opacity):\n","    # Generate KDE for data\n","    density = gaussian_kde(data)\n","    xs = np.linspace(np.min(data), np.max(data), 200)\n","    density.covariance_factor = lambda: 0.25\n","    density._compute_covariance()\n","\n","    hist_vals, bin_edges = np.histogram(\n","        data, bins=range(0, int(np.max(data)) + 2, bin_step), density=True\n","    )\n","    hist = go.Bar(\n","        x=bin_edges[:-1],\n","        y=hist_vals,\n","        name=label,\n","        opacity=trace_opacity,\n","        marker=dict(color=biscale[color][1]),\n","        hovertemplate=[f\"[{int(i)}, {int(i + bin_step)})\" for i in bin_edges[:-1]],\n","    )\n","    density_curve = go.Scatter(\n","        x=xs,\n","        y=density(xs),\n","        mode=\"lines\",\n","        name=label + \" Density\",\n","        line=dict(color=biscale[color][0]),\n","    )\n","\n","    above_threshold_pct = np.sum(data >= threshold) / len(data) * 100\n","    q25, q50, q75 = np.percentile(data, [25, 50, 75])\n","    f = lambda x: f\"{x:0>4.1f}\" if x < 10 else f\"{x:.1f}\"\n","\n","    annotation = dict(\n","        x=0.95,\n","        y=1.0 - 0.04 * i,\n","        xref=\"paper\",\n","        yref=\"paper\",\n","        text=f\"{label}: % > threshold = {f(above_threshold_pct)}, Q25 = {f(q25)}, Q50 = {f(q50)}, Mean = {f(data.mean())}, Q75 = {f(q75)}, max = {f(data.max())}\",\n","        showarrow=False,\n","        font=dict(size=12),\n","    )\n","\n","    return [hist, density_curve], annotation, max(hist_vals)\n","\n","\n","def plot_hist_density(\n","    prefix,\n","    descriptors_type,\n","    n_clusters,\n","    n_iters,\n","    selection,\n","    colors,\n","    title_spec,\n","    threshold=11,\n","    bin_step=2,\n","    trace_opacity=0.6,\n","    width=1280,\n","    height=600,\n","):\n","    fnames = [\n","        f\"{prefix}_baseline_{descriptors_type}_k{n_clusters}\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{selection}_al{i}_{descriptors_type}_k{n_clusters}\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    labels = [\n","        f\"{descriptors_type} k{n_clusters} baseline\",\n","        *(\n","            f\"{descriptors_type} k{n_clusters} {selection} AL{i}\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    graph = Graph(BASE_PATH)\n","    traces, annotations = [], []\n","    freq = 0\n","    loader = load_dist if title_spec == \"ligand\" else compute_cluster_scores\n","    for i, (fname, label, color) in enumerate(zip(fnames, labels, colors)):\n","        new_traces, annotation, max_freq = create_hist_trace(\n","            i, loader(fname), label, color, threshold, bin_step, trace_opacity\n","        )\n","        traces.extend(new_traces)\n","        annotations.append(annotation)\n","        freq = max(freq, max_freq)\n","\n","    # Create figure and add traces\n","    fig = go.Figure(\n","        data=traces,\n","        layout=go.Layout(\n","            bargap=0.2,\n","            barmode=\"overlay\",\n","            shapes=[\n","                dict(\n","                    type=\"line\",\n","                    x0=threshold,\n","                    x1=threshold,\n","                    y0=0,\n","                    y1=1,\n","                    yref=\"paper\",  # refers to the entire plot for the y-dimension\n","                    line=dict(color=\"grey\", width=2, dash=\"longdash\"),\n","                )\n","            ],\n","            annotations=annotations,\n","        ),\n","    )\n","\n","    # Show figure\n","    graph.update_parameters(\n","        dict(\n","            title=f\"Distribution of {title_spec} scores based on DiffDock Poses\",\n","            xaxis_title=\"Prolif Score\",\n","            yaxis_title=\"Rel. Frequency\",\n","            width=width,\n","            height=height,\n","            yrange=[0, freq * 1.05],\n","        )\n","    )\n","    graph.style_figure(fig)\n","    graph.save_figure(\n","        figure=fig,\n","        path=f\"{BASE_PATH}/plots/{title_spec}_distribution/\",\n","        fname=f\"{descriptors_type}_k{n_clusters}_{selection}_al{n_iters}\",\n","        html=True,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"ffD04c6JTKco"},"source":["## Individual Scores"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"elapsed":4838,"status":"ok","timestamp":1692033103275,"user":{"displayName":"Batista Colab","userId":"04968770630191453479"},"user_tz":240},"id":"-ScFG6N2l2qc","outputId":"62aad8bd-2613-40f7-9ef8-514eb750685e"},"outputs":[],"source":["configs = [\n","    (\"mix\", 100, 5, \"softsub\"),\n","    (\"mix\", 10, 3, \"softsub\"),\n","    (\"mqn\", 100, 5, \"softsub\"),\n","    (\"mqn\", 10, 4, \"softsub\"),\n","]\n","for calc_type in [\"ligand\", \"cluster\"]:\n","    for descriptor_type, n_clusters, n_iters, selection in configs:\n","        plot_hist_density(\n","            prefix=\"model7\",\n","            descriptors_type=descriptor_type,\n","            n_clusters=n_clusters,\n","            n_iters=n_iters,\n","            selection=selection,\n","            colors=[\"dark_grey\", \"teal\", \"orange\", \"purple\", \"green\", \"red\"],\n","            title_spec=calc_type,\n","            bin_step=2 if calc_type == \"ligand\" else 1,\n","            threshold=11,\n","            trace_opacity=0.6,\n","            width=1280,\n","            height=600,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pt36t_Tp6WhG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nro27H6V6eWf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69VoPEtA7dZ_"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# UMAP \\ t-SNE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["[1, *(i for i in range(3))]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pprint\n","import pickle\n","\n","pp = pprint.PrettyPrinter(indent=2, compact=False, width=100)\n","\n","\n","def open_scored_file(fname: str) -> pd.DataFrame:\n","    return pd.read_csv(f\"{SCORING_PATH}scored_dataframes/{fname}.csv\")[\n","        [\"smiles\", \"score\"]\n","    ]\n","\n","\n","def load_scored_all_iters(\n","    prefix: str, descriptors_type: str, n_clusters: int, n_iters: int, selection: str\n",") -> pd.DataFrame:\n","    fnames = [\n","        f\"{prefix}_baseline_{descriptors_type}_k{n_clusters}\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{selection}_al{i}_{descriptors_type}_k{n_clusters}\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    stacked = pd.concat([open_scored_file(fname) for fname in fnames])\n","    assert (\n","        stacked.drop_duplicates().shape == stacked.shape\n","    ), \"There are duplicates in the array\"\n","    return stacked\n","\n","\n","def open_descriptors_file(fname: str) -> pd.DataFrame:\n","    return pickle.load(open(f\"{SAMPLING_PATH}descriptors/{fname}.pkl\", \"rb\"))\n","\n","\n","def load_descriptors(\n","    prefix: str, descriptors_type: str, n_clusters: int, n_iters: int, selection: str\n",") -> pd.DataFrame:\n","    fnames = [\n","        f\"{prefix}_baseline_{descriptors_type}_temp1.0\",\n","        *(\n","            f\"{prefix}_{descriptors_type}{n_clusters}_{selection}_al{i}_{descriptors_type}_temp1.0\"\n","            for i in range(1, n_iters + 1)\n","        ),\n","    ]\n","    merged = pd.concat([open_descriptors_file(fname) for fname in fnames])\n","    print(f\"{merged.shape[0]} descriptors loaded\")\n","    merged.drop_duplicates(inplace=True)\n","    print(f\"{merged.shape[0]} descriptors after dropping duplicates\")\n","    return merged\n","\n","\n","def get_descriptors_for_scored(\n","    prefix: str, descriptors_type: str, n_clusters: int, n_iters: int, selection: str\n",") -> pd.DataFrame:\n","    scored = load_scored_all_iters(\n","        prefix, descriptors_type, n_clusters, n_iters, selection\n","    )\n","    descriptors = load_descriptors(\n","        prefix, descriptors_type, n_clusters, n_iters, selection\n","    )\n","    return pd.merge(scored, descriptors, on=\"smiles\", how=\"inner\")\n","\n","\n","def load_nona_column_names(pca_fname: str) -> np.ndarray:\n","    scaler, _ = pickle.load(open(f\"{SAMPLING_PATH}pca_weights/{pca_fname}.pkl\", \"rb\"))\n","    return scaler.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_smile(smile, desc_type):\n","    mol = Chem.MolFromSmiles(smile)\n","    if mol is None:\n","        return None\n","    if desc_type == \"mix\":\n","        return Descriptors.CalcMolDescriptors(mol, missingVal=None, silent=False)\n","    else:\n","        descriptors = rdMolDescriptors.MQNs_(mol)\n","        assert (\n","            len(descriptors) == 42\n","        ), f\"Expected 42 descriptors, got {len(descriptors)}\"\n","        return {f\"MQN{i}\": descriptor for i, descriptor in enumerate(descriptors)}\n","\n","\n","def calculate_descriptors(smiles, desc_type, fname):\n","    keyToData = {}\n","    for smile in tqdm(smiles, total=len(smiles)):\n","        data = process_smile(smile, desc_type)\n","        if data is None:\n","            continue\n","        else:\n","            keyToData.setdefault(\"smiles\", []).append(smile)\n","            for descriptor, value in data.items():\n","                keyToData.setdefault(descriptor, []).append(value)\n","    pd.DataFrame(keyToData).to_pickle(f\"{SAMPLING_PATH}descriptors/{fname}.pkl\")\n","\n","\n","def load_mqn_smiles():\n","    smiles_mqn100 = load_scored_all_iters(\"model7\", \"mqn\", 100, 5, \"softsub\")\n","    smiles_mqn10 = load_scored_all_iters(\"model7\", \"mqn\", 10, 4, \"softsub\")\n","    smiles_mqn = pd.concat([smiles_mqn100, smiles_mqn10])\n","    smiles_mqn.drop_duplicates(inplace=True)\n","    return smiles_mqn[\"smiles\"].to_list()\n","\n","\n","def load_mix_smiles():\n","    smiles_mix100 = load_scored_all_iters(\"model7\", \"mix\", 100, 5, \"softsub\")\n","    smiles_mix10 = load_scored_all_iters(\"model7\", \"mix\", 10, 3, \"softsub\")\n","    smiles_mix = pd.concat([smiles_mix100, smiles_mix10])\n","    smiles_mix.drop_duplicates(inplace=True)\n","    return smiles_mix[\"smiles\"].to_list()\n","\n","\n","# calculate_descriptors(load_mqn_smiles(), \"mix\", \"mqn100upto5_10upto4_mix_descs\")\n","calculate_descriptors(load_mix_smiles(), \"mqn\", \"mix100upto5_10upto3_mqn_descs\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scored_mix100 = get_descriptors_for_scored(\"model7\", \"mix\", 100, 5, \"softsub\")\n","scored_mix10 = get_descriptors_for_scored(\"model7\", \"mix\", 10, 3, \"softsub\")\n","scored_mqn100 = get_descriptors_for_scored(\"model7\", \"mqn\", 100, 5, \"softsub\")\n","scored_mqn10 = get_descriptors_for_scored(\"model7\", \"mqn\", 10, 4, \"softsub\")\n","print(scored_mix100.shape, scored_mix10.shape, scored_mqn100.shape, scored_mqn10.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scored = np.vstack([scored_mix100, scored_mix10])  # , scored_mqn100, scored_mqn10])\n","print(scored.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import umap\n","from sklearn.preprocessing import StandardScaler\n","import plotly.graph_objects as go\n","from typing import List, Tuple, Union\n","\n","\n","def plot_2d_scatterplot(\n","    datapoints: List[Tuple[np.ndarray, Union[str, np.ndarray], str]],\n","    reduction_type: str,\n","    yscale: float = 1.05,\n",") -> go.Figure:\n","    fig = go.Figure()\n","    minX, minY, maxX, maxY = float(\"inf\"), float(\"inf\"), float(\"-inf\"), float(\"-inf\")\n","    for i, (data, color, label) in enumerate(datapoints):\n","        xarr = data[:, 0]\n","        yarr = data[:, 1]\n","        minX = min(minX, min(xarr))\n","        minY = min(minY, min(yarr))\n","        maxX = max(maxX, max(xarr))\n","        maxY = max(maxY, max(yarr))\n","        fig.add_trace(\n","            go.Scatter(\n","                x=xarr,\n","                y=yarr,\n","                mode=\"markers\",\n","                name=label,\n","                visible=\"legendonly\" if i > 0 else True,\n","                marker=dict(\n","                    size=5,\n","                    color=color,\n","                    showscale=True if isinstance(color, (list, np.ndarray)) else False,\n","                    colorscale=\"Hot\",\n","                    opacity=0.5,\n","                ),\n","            )\n","        )\n","    fig.update_layout(\n","        showlegend=True,\n","        xaxis=dict(\n","            title=f\"{reduction_type} Component 1\",\n","            autorange=False,\n","            range=[yscale * minX, yscale * maxX],\n","        ),\n","        yaxis=dict(\n","            title=f\"{reduction_type} Component 2\",\n","            autorange=False,\n","            range=[yscale * minY, yscale * maxY],\n","        ),\n","    )\n","    return fig\n","\n","\n","def reduce_scored(data: pd.DataFrame, reduction: str, reduction_parameters=None):\n","    assert reduction in {\"PCA\", \"UMAP\", \"t-SNE\"}\n","    scores = data[\"score\"].to_numpy()\n","\n","    pca_fname = \"scaler_pca_combined_processed_freq1000_block133_120\"\n","    match reduction:\n","        case \"PCA\":\n","            scaler, pca = pickle.load(\n","                open(f\"{SAMPLING_PATH}pca_weights/{pca_fname}.pkl\", \"rb\")\n","            )\n","            scaled_data = scaler.transform(data[load_nona_column_names(pca_fname)])\n","            transformed_data = pca.transform(scaled_data)\n","            datapoints = [(transformed_data, scores, \"score\")]\n","        case \"UMAP\":\n","            scaler = StandardScaler()\n","            scaled_data = scaler.fit_transform(data[load_nona_column_names(pca_fname)])\n","            reducer = umap.UMAP(\n","                n_neighbors=100,\n","                min_dist=0.4,\n","                metric=\"euclidean\",\n","                n_components=2,\n","                random_state=42,\n","                verbose=True,\n","            )\n","            transformed_data = reducer.fit_transform(scaled_data)\n","            datapoints = [(transformed_data, scores, \"score\")]\n","        case \"t-SNE\":\n","            if reduction_parameters is None:\n","                reduction_parameters = {}\n","            datapoints = []\n","            for kwargs in tqdm(reduction_parameters, total=len(reduction_parameters)):\n","                scaler = StandardScaler()\n","                scaled_data = scaler.fit_transform(\n","                    data[load_nona_column_names(pca_fname)]\n","                )\n","                tsne = TSNE(\n","                    n_components=2,\n","                    random_state=42,\n","                    metric=\"euclidean\",\n","                    **kwargs,\n","                )\n","                transformed_data = tsne.fit_transform(scaled_data)\n","                datapoints.append(\n","                    (\n","                        transformed_data,\n","                        scores,\n","                        f\"Perplexity {kwargs['perplexity']}, Early Exaggeration {kwargs['early_exaggeration']}\",\n","                    )\n","                )\n","    return datapoints"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reduce_and_plot_scored(scored, \"PCA\")\n","reduce_and_plot_scored(scored, \"UMAP\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scored_tsne = reduce_scored(\n","    scored,\n","    \"t-SNE\",\n","    [\n","        {\"perplexity\": 10 * iperp, \"early_exaggeration\": 12}\n","        for iee in [10, 50, 100, 200]\n","        for iperp in [1, 2, 3, 4, 5, 10, 40, 100]\n","    ],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_data_boundaries(data_list):\n","    combined = np.vstack(\n","        data_list\n","    )  # Combine both datasets to get overall min and max values\n","\n","    min_val = (\n","        np.floor(np.min(combined, axis=0) / 10.0) * 10\n","    )  # Round to the nearest number divisible by 10\n","    max_val = np.ceil(np.max(combined, axis=0) / 10.0) * 10\n","    return min_val, max_val\n","\n","\n","def discretize_data_wscores(data, boundaries, bin_size, scores):\n","    bins = [np.arange(boundaries[0][i], boundaries[1][i], bin_size) for i in range(2)]\n","    hist_data, xedges, yedges, _ = scipy.stats.binned_statistic_2d(\n","        data[:, 0], data[:, 1], scores, statistic=\"mean\", bins=bins\n","    )\n","    hist_data = np.nan_to_num(hist_data, nan=0)\n","\n","    xcenters = (xedges[:-1] + xedges[1:]) / 2\n","    ycenters = (yedges[:-1] + yedges[1:]) / 2\n","    return hist_data.T, xcenters, ycenters\n","\n","\n","def discretize_data(data, boundaries, bin_size):\n","    # Use 2D histogram to discretize data\n","    bins = [np.arange(boundaries[0][i], boundaries[1][i], bin_size) for i in range(2)]\n","    hist_data, xedges, yedges = np.histogram2d(data[:, 0], data[:, 1], bins=bins)\n","    # Compute bin centers\n","    xcenters = (xedges[:-1] + xedges[1:]) / 2\n","    ycenters = (yedges[:-1] + yedges[1:]) / 2\n","    return hist_data.T, xcenters, ycenters\n","\n","\n","def create_heatmap_traces_for_all_differences(args_list, bin_size):\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    traces = []\n","    for i, ((data_before, name_before), (data_after, name_after)) in enumerate(\n","        itertools.combinations(args_list, 2)\n","    ):\n","        hist_before, xcenters, ycenters = discretize_data(\n","            data_before, boundaries, bin_size\n","        )\n","        hist_after, _, _ = discretize_data(data_after, boundaries, bin_size)\n","        diff = hist_after - hist_before\n","        label = f\"|{name_after}|<br>-|{name_before}|\"\n","        traces.append(\n","            go.Heatmap(\n","                x=xcenters,\n","                y=ycenters,\n","                z=diff,\n","                zmid=0,\n","                zmax=110,\n","                zmin=-110,\n","                colorscale=\"RdBu\",\n","                name=label,\n","                showlegend=True,\n","                visible=True if i == 0 else \"legendonly\",\n","            )\n","        )\n","    return traces\n","\n","\n","def create_heatmap_trace_for_difference(args_list, bin_size):\n","    assert (\n","        len(data_list) == 2\n","    ), f\"To plot a difference, please provide only 2 data sources\"\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    hist_before, xcenters, ycenters = discretize_data(\n","        data_list[0], boundaries, bin_size\n","    )\n","    hist_after, _, _ = discretize_data(data_list[1], boundaries, bin_size)\n","    diff = hist_after - hist_before\n","    label = f\"|{name_list[1]}|<br>-|{name_list[0]}|\"\n","    return go.Heatmap(\n","        x=xcenters,\n","        y=ycenters,\n","        z=diff,\n","        zmid=0,\n","        colorscale=\"RdBu\",\n","        name=label,\n","        showlegend=True,\n","    )\n","\n","\n","def create_heatmap_traces_for_all_scores(args_list, bin_size, scores):\n","    data_list, name_list = zip(*args_list)\n","    boundaries = get_data_boundaries(data_list)\n","    traces = []\n","    functor = (\n","        lambda x: discretize_data(*x[:-1])\n","        if scores is None\n","        else discretize_data_wscores(*x)\n","    )\n","    zmax = max(\n","        [functor([data, boundaries, bin_size, scores])[0].max() for data in data_list]\n","    )\n","    zmin = min(\n","        [functor([data, boundaries, bin_size, scores])[0].min() for data in data_list]\n","    )\n","    for i, (data, name) in enumerate(args_list):\n","        hist, xcenters, ycenters = functor([data, boundaries, bin_size, scores])\n","        traces.append(\n","            go.Heatmap(\n","                x=xcenters,\n","                y=ycenters,\n","                z=hist,\n","                name=name,\n","                zmin=zmin,\n","                zmax=zmax,\n","                showlegend=True,\n","                colorscale=\"Thermal\",\n","                visible=True if i == 0 else \"legendonly\",\n","            )\n","        )\n","    return traces\n","\n","\n","def plot_heatmap(\n","    args_list,\n","    difference=False,\n","    bin_size=10,\n","    width=1280,\n","    height=720,\n","    all_differences=False,\n","    scores=None,\n","):\n","    fig = go.Figure()\n","\n","    if difference:\n","        if all_differences:\n","            traces = create_heatmap_traces_for_all_differences(args_list, bin_size)\n","            for trace in traces:\n","                fig.add_trace(trace)\n","        else:\n","            fig.add_trace(create_heatmap_trace_for_difference(args_list, bin_size))\n","    else:\n","        traces = create_heatmap_traces_for_all_scores(args_list, bin_size, scores)\n","        for trace in traces:\n","            fig.add_trace(trace)\n","\n","    fig.update_layout(\n","        title=f\"Difference in distribution: # of datapoints per bin ({bin_size=})\",\n","        xaxis_title=\"X\",\n","        yaxis_title=\"Y\",\n","        width=width,\n","        height=height,\n","        legend=dict(x=1.2, y=1),\n","    )\n","\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_2d_scatterplot(\n","    scored_perp10to40,\n","    \"t-SNE\",\n","    yscale=1.5,\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scored_perp10to40"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_fname = \"scaler_pca_combined_processed_freq1000_block133_120\"\n","custom_scores = scored_perp10to40[0][1].copy()\n","# custom_scores[custom_scores < 11] = 0\n","print(custom_scores)\n","plot_heatmap(\n","    [\n","        (scored_perp10to40[i][0], scored_perp10to40[i][2])\n","        for i in range(len(scored_perp10to40))\n","    ],\n","    bin_size=2,\n","    width=900,\n","    height=500,\n","    scores=custom_scores,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.graph_objs as go\n","\n","\n","def plot_correlation_circle(pca, features):\n","    pcs = pca.components_\n","\n","    # Create a trace for the variable vectors\n","    vectors = go.Scatter(\n","        x=pcs[0, :],\n","        y=pcs[1, :],\n","        mode=\"lines+markers+text\",\n","        text=features,\n","        textposition=\"top center\",\n","        line=dict(color=\"red\"),\n","        marker=dict(size=10, color=\"blue\"),\n","        textfont=dict(size=8),\n","    )\n","\n","    # Create a trace for the unit circle\n","    circle = go.Scatter(\n","        x=np.cos(np.linspace(0, 2 * np.pi, 100)),\n","        y=np.sin(np.linspace(0, 2 * np.pi, 100)),\n","        mode=\"lines\",\n","        line=dict(color=\"blue\", width=1),\n","        showlegend=False,\n","    )\n","\n","    layout = go.Layout(\n","        title=\"Correlation Circle\",\n","        autosize=False,\n","        width=800,\n","        height=800,\n","        showlegend=False,\n","        xaxis=dict(\n","            title=f\"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\",\n","            range=[-1.1, 1.1],\n","            zeroline=False,\n","            showgrid=True,\n","            domain=[0, 1],\n","        ),\n","        yaxis=dict(\n","            title=f\"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\",\n","            range=[-1.1, 1.1],\n","            zeroline=False,\n","            showgrid=True,\n","            domain=[0, 1],\n","        ),\n","    )\n","\n","    fig = go.Figure(data=[vectors, circle], layout=layout)\n","    fig.show()\n","\n","\n","# Assuming pca is your PCA model fitted with sklearn and df is the pandas dataframe with your original data\n","plot_correlation_circle(pca, descriptors.columns.values)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN0LSJZYW8oGeOa28jfcLb3","collapsed_sections":["AJJ6x8zuwPM6","giWJddP_21Qw","MHqNMSWvHLmu","gl2kNWZi3cA8","zDT4LDmYUOXA"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
