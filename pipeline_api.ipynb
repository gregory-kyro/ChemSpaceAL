{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/Users/morgunov/batista/Summer/pipeline/'\n",
    "PRETRAINING_PATH = BASE_PATH + '1. Pretraining/'\n",
    "GENERATION_PATH = BASE_PATH + '2. Generation/'\n",
    "SAMPLING_PATH = BASE_PATH + '3. Sampling/'\n",
    "DIFFDOCK_PATH = BASE_PATH + '4. DiffDock/'\n",
    "SCORING_PATH = BASE_PATH + '5. Scoring/'\n",
    "AL_PATH = BASE_PATH + '6. ActiveLearning/'\n",
    "MODE = 'Active Learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume data is a numpy array of shape (n, d)\n",
    "# data = np.random.rand(1000, 110)  # Uncomment this line to test with random data\n",
    "\n",
    "def run_tsne(data, n_components=2, perplexity=30):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "    tsne_results = tsne.fit_transform(data)\n",
    "    return tsne_results\n",
    "\n",
    "def run_umap(data, n_components=2, n_neighbors=15):\n",
    "    reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors)\n",
    "    umap_results = reducer.fit_transform(data)\n",
    "    return umap_results\n",
    "\n",
    "def plot_results(results, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(results[:, 0], results[:, 1])\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "\n",
    "# Run t-SNE and UMAP, and plot the results\n",
    "tsne_results = run_tsne(data)\n",
    "plot_results(tsne_results, 't-SNE results')\n",
    "\n",
    "umap_results = run_umap(data)\n",
    "plot_results(umap_results, 'UMAP results')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 85 functional group column, 110 left\n",
      "Removed 19 count columns, 176 left\n",
      "Removed 104 count and functional group columns, 91 left\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "all_descriptors = pickle.load(open(f\"{SAMPLING_PATH}descriptors/descriptors_moses+bindingdb.pkl\", 'rb'))\n",
    "valid_columns = pickle.load(open(f\"{SAMPLING_PATH}descriptors/descriptors_moses+bindingdb_columnlist.pkl\", 'rb'))\n",
    "export_columns_to_yaml(valid_columns, \"valid_columns\")\n",
    "valid_descriptors = all_descriptors[valid_columns]\n",
    "no_fr, no_counts, no_fr_counts = process_columns(valid_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from graph import Graph\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "\n",
    "def export_columns_to_yaml(columns, fname):\n",
    "    with open(f\"pca_study/columns/{fname}.yaml\", 'w') as f:\n",
    "        yaml.dump(columns, f)\n",
    "\n",
    "# Load from yaml file\n",
    "def load_columns_from_yaml(fname):\n",
    "    with open(f\"pca_study/columns/{fname}.yaml\", 'r') as f:\n",
    "        columns = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return columns\n",
    "def process_columns(columns):\n",
    "    # Remove all the functional group counts\n",
    "    no_fr = [col for col in columns if not col.startswith('fr_')]\n",
    "    print(f\"Removed {len(columns) - len(no_fr)} functional group column, {len(no_fr)} left\")\n",
    "    # Remove all the counts descriptors\n",
    "    no_counts = [col for col in columns if \"count\" not in col.lower() and \"num\" not in col.lower()]\n",
    "    print(f\"Removed {len(columns) - len(no_counts)} count columns, {len(no_counts)} left\")\n",
    "    # do the dumps\n",
    "    export_columns_to_yaml(no_fr, 'no_fr')\n",
    "    export_columns_to_yaml(no_counts, 'no_counts')\n",
    "    no_fr_counts = sorted(list(set(no_fr) & set(no_counts)))\n",
    "    print(f\"Removed {len(columns) - len(no_fr_counts)} count and functional group columns, {len(no_fr_counts)} left\")\n",
    "    export_columns_to_yaml(no_fr_counts, 'no_fr_counts')\n",
    "    return no_fr, no_counts, no_fr_counts\n",
    "\n",
    "def fit_pca(data, n_comps=2, whiten=False):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    pca = PCA(n_components=n_comps, whiten=whiten)\n",
    "    pca.fit(scaled_data)\n",
    "    return scaler, pca\n",
    "\n",
    "def plot_explained_variance(pca, title, suffix=None):\n",
    "    explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    x = np.arange(1, len(explained_variance) + 1)\n",
    "\n",
    "    graph = Graph()\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=explained_variance, mode='markers', name='Explained variance'))\n",
    "    fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=100, y0=1, y1=1)\n",
    "\n",
    "    # Calculate the number of principal components explaining certain percentages of the variance\n",
    "    percentages = [50, 75, 90, 95, 99, 99.99]\n",
    "    n_components = [np.ceil(1+np.argmax(explained_variance >= p / 100.0)) for p in percentages]\n",
    "    text_block = '<br>'.join(f'{p}% of variance is explained by {n} components' for p, n in zip(percentages, n_components))\n",
    "\n",
    "    # Add text block\n",
    "    fig.add_annotation(x=0.8, y=0.1, xref='paper', yref='paper', text=text_block, showarrow=False)\n",
    "    graph.update_parameters(dict(width=800, height=400, title=title, xaxis_title='Number of components', yaxis_title='Explained variance'))\n",
    "    graph.style_figure(fig)\n",
    "    if suffix is None: suffix = ''\n",
    "    fig.write_html(f\"pca_study/plots/explained_variance{suffix}.html\")\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_circle(pca, features, title, suffix):\n",
    "    pcs = pca.components_\n",
    "    graph = Graph()\n",
    "\n",
    "    # Create a trace for the variable vectors\n",
    "    vectors = go.Scatter(\n",
    "        x=pcs[0, :],\n",
    "        y=pcs[1, :],\n",
    "        mode='lines+markers+text',\n",
    "        text=features,\n",
    "        textposition='top center',\n",
    "        line=dict(color='red'),\n",
    "        marker=dict(size=10, color='blue'),\n",
    "        textfont=dict(size=8)\n",
    "    )\n",
    "\n",
    "    # Create a trace for the unit circle\n",
    "    circle = go.Scatter(\n",
    "        x=np.cos(np.linspace(0, 2*np.pi, 100)),\n",
    "        y=np.sin(np.linspace(0, 2*np.pi, 100)),\n",
    "        mode='lines',\n",
    "        line=dict(color='blue', width=1),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[vectors, circle])\n",
    "    graph.update_parameters(dict(width=600, height=600, title=title,\n",
    "                                 xaxis_title=f\"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\",\n",
    "                                 yaxis_title=f\"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\",\n",
    "                                 showlegend=False,))\n",
    "    graph.style_figure(fig)\n",
    "    fig.write_html(f\"pca_study/plots/corr_circle{suffix}.html\")\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 85 functional group column, 110 left\n",
      "Removed 19 count columns, 176 left\n",
      "Removed 104 count and functional group columns, 91 left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing _no_fr_counts: 100%|██████████| 4/4 [00:00<00:00, 25.11it/s]\n"
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    dict(cols=valid_columns, title=f'Explained variance by PCA on df with {len(valid_columns)} valid columns', suffix='_valid_columns'),\n",
    "    dict(cols=cols[0], title=f'Explained variance by PCA on df with {len(cols[0])} columns (no functional groups)', suffix='_no_fr'),\n",
    "    dict(cols=cols[1], title=f'Explained variance by PCA on df with {len(cols[1])} columns (no counts)', suffix='_no_counts'),\n",
    "    dict(cols=cols[2], title=f'Explained variance by PCA on df with {len(cols[2])} columns (no counts and functional groups)', suffix='_no_fr_counts'),\n",
    "]\n",
    "pbar = tqdm(configs, total=len(configs))\n",
    "for config in pbar:\n",
    "    pbar.set_description(f\"Processing {config['suffix']}\")\n",
    "    # n_comps = min(100, len(config['cols']))\n",
    "    # scaler, pca = fit_pca(valid_descriptors[config['cols']], n_comps=n_comps, whiten=False)\n",
    "    scaler, pca = pickle.load(open(f\"pca_study/checkpoints/scaler_pca{config['suffix']}.pkl\", 'rb'))\n",
    "    plot_explained_variance(pca, title=config['title'], suffix=config['suffix'])\n",
    "    plot_correlation_circle(pca, config['cols'], config['title'].replace('Explained variance by PCA', 'Correlation circle for PCA'), suffix=config['suffix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2adb63b50>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "torch.manual_seed(42)  # or any other seed you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2894910, 110])\n",
      "2315928 289491 289491\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(all_descriptors[no_fr]) #.sample(n=200_000, random_state=42)\n",
    "scaled_tensor = torch.tensor(scaled).float().to('mps')\n",
    "print(scaled_tensor.shape)\n",
    "train_size = int(0.8*scaled_tensor.shape[0])\n",
    "test_size = int(0.1*scaled_tensor.shape[0])\n",
    "val_size = scaled_tensor.shape[0] - train_size - test_size\n",
    "print(train_size, val_size, test_size)\n",
    "train_split, val_split, test_split = torch.utils.data.random_split(scaled_tensor, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=110, out_features=48, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=48, out_features=24, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=24, out_features=2, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=24, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=24, out_features=48, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=48, out_features=110, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, latent_dim, do_norm_layer=False, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        units = [input_dim, *hidden_units, latent_dim]\n",
    "        rev_units = units[::-1]\n",
    "        encode_layers, decode_layers = [], []\n",
    "        for layer_i, layer_ip1 in zip(units, units[1:]):\n",
    "            encode_layers.append(nn.Linear(layer_i, layer_ip1))\n",
    "            if layer_ip1 != latent_dim:\n",
    "                if do_norm_layer:\n",
    "                    encode_layers.append(nn.LayerNorm(layer_ip1))\n",
    "                encode_layers.append(activation())\n",
    "\n",
    "        for layer_i, layer_ip1 in zip(rev_units, rev_units[1:]):\n",
    "            decode_layers.append(nn.Linear(layer_i, layer_ip1))\n",
    "            if layer_ip1 != input_dim:\n",
    "                if do_norm_layer:\n",
    "                    decode_layers.append(nn.LayerNorm(layer_ip1))\n",
    "                decode_layers.append(activation())\n",
    "\n",
    "        self.encoder = nn.Sequential(*encode_layers)\n",
    "        self.decoder = nn.Sequential(*decode_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "def run_one_epoch(model, mode, data_loader, criterion, optimizer, wandb, nbatches_step_loss):\n",
    "    assert mode in {\"train\", \"val\"}\n",
    "    is_train = mode == \"train\"\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "\n",
    "    epoch_losses = []\n",
    "    step_losses = []\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for batch_ind, batch in pbar:\n",
    "        batch.to('mps')\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "        epoch_losses.append(current_loss)\n",
    "        step_losses.append(current_loss)\n",
    "\n",
    "        if batch_ind != 0 and batch_ind % nbatches_step_loss == 0:\n",
    "            average_step_loss = np.mean(step_losses)\n",
    "            wandb.log({f\"{mode}_loss\": average_step_loss})\n",
    "            step_losses = []  # reset the list after logging\n",
    "\n",
    "    average_epoch_loss = np.mean(epoch_losses)\n",
    "    return average_epoch_loss, epoch_losses\n",
    "\n",
    "def train(model, train_data, valid_data, wandb, num_epochs=5, learning_rate=0.001, \n",
    "          do_validation=True, nbatches_step_loss=50, ckpt_fname='train_ckpt', do_warmup=False, do_decay=False, warmup_epochs=0, decay_epochs=0):\n",
    "    assert warmup_epochs + decay_epochs <= num_epochs, \"The sum of warmup and decay epochs should not exceed total epochs\"\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        if do_warmup and epoch < warmup_epochs:  # learning rate warmup phase\n",
    "            lr = learning_rate * (epoch + 1) / warmup_epochs\n",
    "        elif do_decay and epoch < warmup_epochs + decay_epochs:  # learning rate decay phase\n",
    "            lr = learning_rate * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / decay_epochs))\n",
    "        else:  # constant learning rate phase\n",
    "            lr = learning_rate\n",
    "\n",
    "        # apply new learning rate to the optimizer\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        epoch_train_loss, _ = run_one_epoch(model, \"train\", train_data, criterion, optimizer, wandb, nbatches_step_loss)\n",
    "        wandb.log({\"epoch_train_loss\": epoch_train_loss, \"learning_rate\": lr, \"epoch\": epoch})\n",
    "\n",
    "        if do_validation:\n",
    "            epoch_validation_loss, _ = run_one_epoch(model, \"val\", valid_data, criterion, optimizer, wandb, nbatches_step_loss)\n",
    "            wandb.log({\"epoch_validation_loss\": epoch_validation_loss})\n",
    "            \n",
    "            if epoch_validation_loss < best_loss:\n",
    "                best_loss = epoch_validation_loss\n",
    "                torch.save(model.state_dict(), f'autoencoder/checkpoints/{ckpt_fname}.pth')\n",
    "            \n",
    "        else:\n",
    "            if epoch_train_loss < best_loss:\n",
    "                best_loss = epoch_train_loss\n",
    "                torch.save(model.state_dict(), f'autoencoder/checkpoints/{ckpt_fname}.pth')\n",
    "\n",
    "Autoencoder(input_dim=scaled_tensor.shape[1], hidden_units=[48, 24], latent_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_split, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_split, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_split, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = [\n",
    "    dict(name=\"lat_110_hid_none\", hidden_units=[], latent_dim=110),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in CONFIGS:\n",
    "    model = Autoencoder(input_dim=scaled_tensor.shape[1], hidden_units=config['hidden_units'], latent_dim=config['latent_dim'])\n",
    "    model.to('mps')\n",
    "    torch.compile(model)\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    wandb.init(project='autoencoder', entity='generative_ml', name=config['name'], config={'num_params': num_params})\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    print(f'Number of parameters: {num_params}')\n",
    "    out = train(model, train_loader, val_loader, wandb, num_epochs=10, learning_rate=0.01, nbatches_step_loss=250, ckpt_fname=config['name'],\n",
    "                do_warmup=False, do_decay=False, warmup_epochs=0, decay_epochs=0)\n",
    "    wandb.finish()\n",
    "\n",
    "# wandb API 5be14d5930441de4707f6a58e4f7c2e229dab1d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process GPT Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem\n",
    "import rdkit.Chem.Descriptors\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def descriptors_for_gpt_predictions(path_to_predicted, path_to_save):\n",
    "    gpt_mols = pd.read_csv(path_to_predicted)\n",
    "    keySet = None\n",
    "    keyToData = {}\n",
    "    pbar = tqdm(gpt_mols.iterrows(), total=len(gpt_mols))\n",
    "    for index, row in pbar:\n",
    "        smile = row['smiles']\n",
    "        mol = rdkit.Chem.MolFromSmiles(smile)\n",
    "        if not mol: continue\n",
    "        mol_data = rdkit.Chem.Descriptors.CalcMolDescriptors(mol)\n",
    "        if keySet is None:\n",
    "            keySet = set(mol_data.keys())\n",
    "        for key in keySet:\n",
    "            keyToData.setdefault(key, []).append(mol_data[key])\n",
    "        keyToData.setdefault('smiles', []).append(smile)\n",
    "    gpt_df = pd.DataFrame(keyToData)\n",
    "    gpt_df.to_pickle(path_to_save)\n",
    "    return gpt_df\n",
    "\n",
    "BASE = '/Users/morgunov/batista/Summer/'\n",
    "CHECKPOINTS = BASE + 'bindingDB/checkpoints/'\n",
    "GPT_DATA = BASE + 'data/'\n",
    "gpt_df = descriptors_for_gpt_predictions(path_to_predicted=GPT_DATA + 'molgpt_generated_nocond_06_10_fintetune2.csv', path_to_save=CHECKPOINTS+f'gptMols_ft2.pickle')\n",
    "gpt_df = descriptors_for_gpt_predictions(path_to_predicted=GPT_DATA + 'molgpt_generated_nocond_06_10.csv', path_to_save=CHECKPOINTS+f'gptMols.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA-Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99095, 100), (99095,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def project_into_pca_space(path_to_pca, path_to_mols):\n",
    "    scaler, pca = pickle.load(open(path_to_pca, 'rb'))\n",
    "    gptMols = pd.read_pickle(path_to_mols)#.sample(n=10)\n",
    "    return gptMols['smiles'], pca.transform(scaler.transform(gptMols[scaler.get_feature_names_out()]))\n",
    "\n",
    "gpt_smiles, pca_transformed = project_into_pca_space(path_to_pca=PICKLES + 'scaler_pca_moses+bindingdb.pkl', path_to_mols=INFERENCES + 'GPT_pretrain_inference_07_14_23_39_1end_ignore_moses+bindingdb_temp1.0_descriptors.pkl')\n",
    "pca_transformed.shape, gpt_smiles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring KMeans clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "def _cluster_mols_experimental_loss(mols, n_clusters, n_iter):\n",
    "    min_loss, best_kmeans = float('inf'), None\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        if kmeans.inertia_ < min_loss:\n",
    "            min_loss = kmeans.inertia_\n",
    "            best_kmeans = kmeans\n",
    "    return best_kmeans\n",
    "\n",
    "def _cluster_mols_experimental_variance(mols, n_clusters, n_iter):\n",
    "    max_variance, best_kmeans = float('-inf'), None\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n",
    "        if (variance:=np.var(counts)) > max_variance:\n",
    "            max_variance = variance\n",
    "            best_kmeans = kmeans\n",
    "    return best_kmeans\n",
    "\n",
    "def _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile):\n",
    "    inertias = []\n",
    "    variances = []\n",
    "    km_objs = []\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n",
    "        variances.append(np.var(counts))\n",
    "        km_objs.append(kmeans)\n",
    "    loss_var_kmeans_triples = sorted(zip(inertias, variances, km_objs), key=lambda x: x[0])\n",
    "    lowest_n = loss_var_kmeans_triples[:int(len(loss_var_tuples) * mixed_objective_loss_quantile)]\n",
    "    sorted_by_variance = sorted(lowest_n, key=lambda x: x[1])\n",
    "    return sorted_by_variance[0][2]\n",
    "\n",
    "def _cluster_mols_experimental(mols, n_clusters, save_path, n_iter=1, objective='loss', mixed_objective_loss_quantile=0.1):\n",
    "    if n_iter == 1:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "    elif objective == 'loss':\n",
    "        kmeans = _cluster_mols_experimental_loss(mols, n_clusters, n_iter)\n",
    "    elif kmeans == 'variance':\n",
    "        kmeans = _cluster_mols_experimental_variance(mols, n_clusters, n_iter)\n",
    "    elif objective == 'mixed':\n",
    "        kmeans = _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown objective {objective}')\n",
    "\n",
    "    pickle.dump(best_kmeans, open(save_path, 'wb'))\n",
    "    return kmeans\n",
    "\n",
    "out = _cluster_mols_experimental(mols=pca_transformed, n_clusters=100, n_iter=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.title_size = 20\n",
    "        self.axis_title_size = 14\n",
    "        self.tick_font_size = 12\n",
    "        self.text_color=\"#333333\"\n",
    "        self.background = \"white\"\n",
    "        self.grid_color = \"#e2e2e2\"\n",
    "        self.line_color = \"#000000\"\n",
    "        self.font_family = 'Helvetica'\n",
    "        self.width = 600\n",
    "        self.height = 400\n",
    "        self.title = ''\n",
    "        self.xaxis_title = ''\n",
    "        self.yaxis_title = ''\n",
    "    \n",
    "    def update_parameters(self, params):\n",
    "        for key, val in params.items():\n",
    "            setattr(self, key, val)\n",
    "        \n",
    "\n",
    "    def style_figure(self, figure):\n",
    "        figure.update_layout({\n",
    "            'margin': {'t': 50, 'b': 50, 'l': 50, 'r': 50},\n",
    "            'plot_bgcolor': self.background,\n",
    "            'paper_bgcolor': self.background,\n",
    "            'title': {\n",
    "                'text': self.title,\n",
    "                'font': {\n",
    "                    'size': self.title_size,\n",
    "                    'color': self.text_color,\n",
    "                    'family': self.font_family\n",
    "                },\n",
    "            },\n",
    "            'height': self.height,  # Set fixed size ratio 3:4\n",
    "            'width': self.width, \n",
    "            'font': {\n",
    "                'family': self.font_family,\n",
    "                'size': self.tick_font_size,\n",
    "                'color': self.text_color\n",
    "            },\n",
    "            'legend': {\n",
    "                'font': {\n",
    "                    'family': self.font_family,\n",
    "                    'size': self.tick_font_size,\n",
    "                    'color': self.text_color\n",
    "                },\n",
    "            },\n",
    "        })\n",
    "\n",
    "        # Setting the title size and color and grid for both x and y axes\n",
    "        figure.update_xaxes(\n",
    "            title=self.xaxis_title,\n",
    "            title_font={'size': self.axis_title_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            tickfont={'size': self.tick_font_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor=self.grid_color,\n",
    "            linecolor=self.line_color,  # make x axis line visible\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "        figure.update_yaxes(\n",
    "            title=self.yaxis_title,\n",
    "            title_standoff=0,\n",
    "            title_font={'size': self.axis_title_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            tickfont={'size': self.tick_font_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor=self.grid_color,\n",
    "            linecolor=self.line_color,  # make y axis line visible\n",
    "            linewidth=2\n",
    "        )\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "loss_to_var = {loss:var for loss, var in zip(out[0], out[1])}\n",
    "sort_loss, variances = zip(*sorted(loss_to_var.items(), key=lambda x: x[0]))\n",
    "\n",
    "graph = Graph()\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(sort_loss)), y=sort_loss, mode='markers', name='Loss'), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(sort_loss)), y=variances, mode='markers', name='Variances'), secondary_y=True)\n",
    "graph.style_figure(fig)\n",
    "fig.show()\n",
    "fig.write_html(CHECKPOINTS + 'kmeans_sort_loss_vs_variance.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_to_var = {loss:var for loss, var in zip(out[0], out[1])}\n",
    "loss, sort_variances = zip(*sorted(loss_to_var.items(), key=lambda x: x[1]))\n",
    "\n",
    "graph = Graph()\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss)), y=loss, mode='markers', name='Loss'), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss)), y=sort_variances, mode='markers', name='Variances'), secondary_y=True)\n",
    "graph.style_figure(fig)\n",
    "fig.show()\n",
    "fig.write_html(CHECKPOINTS + 'kmeans_sort_loss_vs_variance.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_based_on_distance_percentiles(elements, distances, n_samples, n_percentiles):\n",
    "    assert len(elements) == len(distances), \"Elements and distances lists must be of the same length\"\n",
    "    assert n_samples <= len(elements), \"Number of samples cannot exceed the total number of elements\"\n",
    "    assert n_percentiles > 0, \"Number of percentiles must be a positive integer\"\n",
    "    \n",
    "    # Sort elements and distances together based on ascending order of distances\n",
    "    distances, elements = zip(*sorted(zip(distances, elements)))\n",
    "    \n",
    "    # Compute the percentiles\n",
    "    percentile_values = [np.percentile(distances, p * 100 / n_percentiles) for p in range(1, n_percentiles)]\n",
    "    percentile_values.append(np.inf)  # the highest percentile encompasses all remaining points\n",
    "    \n",
    "    # Divide data into percentiles\n",
    "    elements_by_percentile = []\n",
    "    start_index = 0\n",
    "    for percentile_value in percentile_values:\n",
    "        end_index = start_index\n",
    "        while end_index < len(distances) and distances[end_index] <= percentile_value:\n",
    "            end_index += 1\n",
    "        elements_by_percentile.append(elements[start_index:end_index])\n",
    "        start_index = end_index\n",
    "    \n",
    "    # Sample from each percentile\n",
    "    samples_per_percentile = n_samples // n_percentiles\n",
    "    remaining_samples = n_samples % n_percentiles\n",
    "    samples = []\n",
    "    for i, percentile_elements in enumerate(elements_by_percentile):\n",
    "        if len(percentile_elements) <= samples_per_percentile:\n",
    "            # If we don't have enough elements in this percentile, take them all and\n",
    "            # add the deficit to remaining_samples so it can be distributed among subsequent percentiles\n",
    "            samples += percentile_elements\n",
    "            remaining_samples += samples_per_percentile - len(percentile_elements)\n",
    "        else:\n",
    "            # Sample elements from this percentile\n",
    "            samples += list(np.random.choice(percentile_elements, size=samples_per_percentile, replace=False))\n",
    "        \n",
    "        # Distribute remaining_samples among the last n_percentiles\n",
    "        if i >= n_percentiles - remaining_samples:\n",
    "            extra_samples = min(len(percentile_elements) - samples_per_percentile, 1)\n",
    "            samples += list(np.random.choice([el for el in percentile_elements if el not in samples], size=extra_samples, replace=False))\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint as pp \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def _cluster_mols(mols, n_clusters, save_path, n_iter=1):\n",
    "    \"\"\"\n",
    "        Performs K-Means clustering on a given list of molecules and saves the model to a specified file.\n",
    "\n",
    "        This function will apply the K-Means algorithm to the input list of molecules. If n_iter is set to 1 (default), the function will perform the clustering once and return the KMeans object. If n_iter is set to more than 1, the function will perform the clustering n_iter times and return the KMeans object with the lowest inertia (i.e., the sum of squared distances of samples to their closest cluster center). The function will save the KMeans object to a file at the specified save_path using pickle.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "            mols : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            The input samples where n_samples is the number of samples and n_features is the number of features.\n",
    "\n",
    "            n_clusters : int\n",
    "            The number of clusters to form as well as the number of centroids to generate.\n",
    "\n",
    "            save_path : str\n",
    "            The path (including file name) where the resulting KMeans object should be saved.\n",
    "\n",
    "            n_iter : int, optional (default=1)\n",
    "            The number of times to perform the clustering. If greater than 1, the function will return the KMeans object with the lowest inertia.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            kmeans : sklearn.cluster._kmeans.KMeans\n",
    "            A KMeans instance trained on the input molecules. If n_iter is greater than 1, it's the best performing model (lowest inertia) from all iterations.\n",
    "\n",
    "    \"\"\"\n",
    "    if n_iter == 1:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        pickle.dump(kmeans, open(save_path, 'wb'))\n",
    "        return kmeans\n",
    "    best_kmeans = None\n",
    "    best_inertia = float('inf')\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        if kmeans.inertia_ < best_inertia:\n",
    "            best_kmeans = kmeans\n",
    "            best_inertia = kmeans.inertia_\n",
    "    pickle.dump(best_kmeans, open(save_path, 'wb'))\n",
    "    return best_kmeans\n",
    "\n",
    "def cluster_and_sample(mols, mols_smiles, n_clusters, n_samples, kmeans_save_path, clusters_save_path, diffdock_save_path, \n",
    "                        ensure_correctness=False, path_to_pca=None, probabilistic_sampling=True, load_kmeans=False,\n",
    "                        percentile_sampling=True, n_percentiles=1):\n",
    "    \"\"\"\n",
    "        Clusters a given list of molecules, samples from each cluster, and saves the resulting data to specified files.\n",
    "\n",
    "        This function performs K-Means clustering on the input list of molecules and then samples a specified number of molecules \n",
    "        from each cluster. The function ensures that the number of samples requested from each cluster doesn't exceed the total number \n",
    "        of available molecules. The clustered data and sampled data are saved to specified file paths using pickle.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mols : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            The input samples where n_samples is the number of samples and n_features is the number of features.\n",
    "\n",
    "        mols_smiles : list of str\n",
    "            A list of SMILES strings corresponding to the input molecules.\n",
    "\n",
    "        n_clusters : int\n",
    "            The number of clusters to form as well as the number of centroids to generate.\n",
    "\n",
    "        n_samples : int\n",
    "            The number of samples to draw from each cluster.\n",
    "\n",
    "        kmeans_save_path : str\n",
    "            The path (including file name) where the resulting KMeans object should be saved.\n",
    "\n",
    "        clusters_save_path : str\n",
    "            The path (including file name) where the resulting clusters should be saved.\n",
    "\n",
    "        ensure_correctness : bool, optional (default=False)\n",
    "            If True, performs additional correctness checks, such as comparing SMILES string derived features to features in mols array. \n",
    "            This requires 'path_to_pca' to be set.\n",
    "\n",
    "        path_to_pca : str, optional (default=None)\n",
    "            If ensure_correctness is True, this should be the path to a PCA model used to transform the molecules' descriptors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_to_samples : dict\n",
    "            A dictionary where the keys are cluster labels and the values are lists of sampled SMILES strings from each cluster.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If the number of requested samples exceeds the total number of molecules provided.\n",
    "            If ensure_correctness is True but path_to_pca is None.\n",
    "            If the number of labels returned by the KMeans algorithm differs from the number of molecules.\n",
    "            If features calculated from a smile string differ from features in the mols array.\n",
    "            If the total number of sampled molecules doesn't equal to n_clusters * n_samples.\n",
    "\n",
    "    \"\"\"\n",
    "    assert n_clusters * n_samples <= len(mols), f\"{n_clusters=} * {n_samples=} = {n_clusters*n_samples} requested but only {len(mols)} molecules provided\"\n",
    "    if ensure_correctness:\n",
    "        assert path_to_pca is not None, \"path_to_pca must be provided to ensure correctness\"\n",
    "        scaler, pca = pickle.load(open(path_to_pca, 'rb'))\n",
    "\n",
    "    if load_kmeans:\n",
    "        kmeans = pickle.load(open(kmeans_save_path, 'rb'))\n",
    "    else:\n",
    "        kmeans = _cluster_mols(mols=mols, n_clusters=n_clusters, save_path=kmeans_save_path)\n",
    "        assert len(kmeans.labels_) == len(mols_smiles), \"Number of labels differs from number of molecules\"\n",
    "    distances = kmeans.transform(mols)\n",
    "\n",
    "    cluster_to_mols = {}\n",
    "    cluster_to_distances = {}\n",
    "    for mol, distance, label, smile in zip(mols, distances, kmeans.labels_, mols_smiles):\n",
    "        cluster_to_mols.setdefault(label, []).append(smile)\n",
    "        cluster_to_distances.setdefault(label, []).append(distance.min())\n",
    "        if ensure_correctness: # recalculate descriptors from a smile string and compare to the descriptors in the array\n",
    "            smile_features = pca.transform(scaler.transform(pd.DataFrame({k: [v] for k, v in rdkit.Chem.Descriptors.CalcMolDescriptors(rdkit.Chem.MolFromSmiles(smile)).items()})[scaler.get_feature_names_out()]))\n",
    "            assert np.allclose(smile_features[0], mol), \"Features calculated from a smile string differ from features in the array\"\n",
    "\n",
    "    pickle.dump((kmeans.labels_, cluster_to_distances), open(clusters_save_path.split('.')[0]+'_cl_to_d.pickle', 'wb'))\n",
    "    # What happens below is sampling from each cluster. All the extra code is to ensure that the number of samples requested from each cluster\n",
    "    # doesn't exceed the total number of available molecules. This is done by calculating the average number of molecules per cluster and then\n",
    "    # calculating the number of extra molecules that need to be sampled from each cluster. The extra molecules are then distributed among the\n",
    "    # clusters uniformly. If the number of extra molecules is greater than the number of molecules in a cluster, all\n",
    "    # molecules from that cluster are sampled.\n",
    "    avg_len = np.mean([len(v) for v in cluster_to_mols.values()])\n",
    "    cluster_to_samples = {}\n",
    "    extra_mols = 0\n",
    "    left_to_sample = n_clusters*n_samples\n",
    "    cluster_to_len = {cluster:len(mols) for cluster, mols in cluster_to_mols.items()}\n",
    "    for i, (cluster, _) in enumerate(sorted(cluster_to_len.items(), key=lambda x: x[1], reverse=False)):\n",
    "        smiles = cluster_to_mols[cluster]\n",
    "        if extra_mols > 0:\n",
    "            cur_extra = int(1+extra_mols/(len(cluster_to_mols) - i) * len(smiles)/avg_len)\n",
    "            cur_samples = n_samples + cur_extra\n",
    "            extra_mols -= cur_extra\n",
    "        else:\n",
    "            cur_samples = n_samples\n",
    "        if cur_samples > left_to_sample:\n",
    "            cur_samples = left_to_sample\n",
    "\n",
    "        if len(smiles) > cur_samples:\n",
    "            if probabilistic_sampling:\n",
    "                cluster_to_samples[cluster] = np.random.choice(smiles, cur_samples, p=cluster_to_distances[cluster]/np.sum(cluster_to_distances[cluster]), replace=False)\n",
    "            elif percentile_sampling:\n",
    "                cluster_to_samples[cluster] = sample_based_on_distance_percentiles(smiles, cluster_to_distances[cluster], n_samples=cur_samples, n_percentiles=n_percentiles)\n",
    "            else:\n",
    "                cluster_to_samples[cluster] = np.random.choice(smiles, cur_samples, replace=False)\n",
    "            left_to_sample -= cur_samples\n",
    "        else:\n",
    "            cluster_to_samples[cluster] = smiles\n",
    "            left_to_sample -= len(smiles)\n",
    "            extra_mols += cur_samples - len(smiles)\n",
    "\n",
    "    assert (n_sampled:=sum(len(vals) for vals in cluster_to_samples.values())) == n_clusters*n_samples, f\"Sampled {n_sampled} but were requested {n_clusters*n_samples}\"\n",
    "    pickle.dump(cluster_to_mols, open(clusters_save_path, 'wb'))\n",
    "    pickle.dump(cluster_to_samples, open(clusters_save_path.split('.')[0] + '_samples.pickle', 'wb'))\n",
    "    keyToData = {}\n",
    "    for cluster, mols in cluster_to_samples.items():\n",
    "        for mol in mols:\n",
    "            keyToData.setdefault('smiles', []).append(mol)\n",
    "            keyToData.setdefault('cluster_id', []).append(cluster)\n",
    "    pd.DataFrame(keyToData).to_csv(diffdock_save_path)\n",
    "    return cluster_to_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclusters = 100\n",
    "# bkmeans = _cluster_mols(mols=pca_transformed, n_clusters=nclusters, save_path=CHECKPOINTS + 'k100means_07_11.pickle')\n",
    "c_to_s = cluster_and_sample(mols=pca_transformed, mols_smiles=gpt_smiles, n_clusters=nclusters, n_samples=10, \n",
    "                   kmeans_save_path=PICKLES + 'k100means_07_14_23_39_1end_ignore_moses+bindingdb.pickle', ensure_correctness=False, path_to_pca=PICKLES + 'scaler_pca_moses+bindingdb.pickle',\n",
    "                   clusters_save_path=PICKLES + 'cluster_to_samples_07_16.pickle', probabilistic_sampling=False, percentile_sampling=True, n_percentiles=4,\n",
    "                   diffdock_save_path=INFERENCES + f'samples_percentiles_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, cluster_to_distances = pickle.load(open(CHECKPOINTS + 'cluster_to_samples_07_11_cl_to_d.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = np.array([distance for distances in cluster_to_distances.values() if len(distances) > 10 for distance in distances])\n",
    "min_d, max_d = np.min(all_distances), np.max(all_distances)\n",
    "hist, bin_edges = np.histogram(all_distances, bins=10)\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "sine_distances = np.sin(2*np.pi*bin_centers) * max(all_distances)\n",
    "# sine_distances = np.sin(all_distances*2*np.pi/np.sum(all_distances))\n",
    "len(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Plot a histogram based on all distances (which are python lists) in cluster_to_distances\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(x=all_distances, xbins=dict(start=min_d, end=max_d, size=0.01*(max_d-min_d)), name='all_distances'))\n",
    "fig.add_trace(go.Scatter(x=bin_centers, y=sine_distances, name='sine transformed distances'))\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Plot a histogram based on all distances (which are python lists) in cluster_to_distances\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pca_transformed[:, 7], y=pca_transformed[:, 50], marker=dict(color=labels), text=labels, mode='markers', name='gpt generated'))\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training dataset for active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _preprocess_scores_uniformly(scores, remove_positives=False, lowest_score=1):\n",
    "    \"\"\"\n",
    "        Preprocesses a dictionary of scores by negating and normalizing them.\n",
    "\n",
    "        The function negates all scores and optionally removes positive scores. If the minimum value among the negated scores \n",
    "        is less than zero, it shifts all values by subtracting the minimum value and adding 'lowest_score'. The final step is \n",
    "        to normalize the scores so that their total sum equals to 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores : dict\n",
    "            A dictionary of scores where the keys are identifiers and the values are their corresponding scores.\n",
    "\n",
    "        remove_positives : bool, optional (default=False)\n",
    "            If True, all positive scores are removed after negation.\n",
    "\n",
    "        lowest_score : int, optional (default=1)\n",
    "            This value is added to all scores if the minimum score is less than zero.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        normalized : dict\n",
    "            The normalized dictionary of scores.\n",
    "\n",
    "    \"\"\"\n",
    "    negated = {k: -v for k, v in scores.items()}\n",
    "    min_value = min(negated.values())\n",
    "    if min_value < 0:\n",
    "        if remove_positives:\n",
    "            negated = {k: v for k, v in negated.items() if v > 0}\n",
    "        else:\n",
    "            negated = {k: v - min_value + lowest_score for k, v in negated.items()}\n",
    "    total = sum(negated.values())\n",
    "    normalized = {k: v / total for k, v in negated.items()}\n",
    "    return normalized\n",
    "\n",
    "def _preprocess_scores_softmax(scores):\n",
    "    negated = {k: -v for k, v in scores.items()}\n",
    "    max_value = max(negated.values())\n",
    "    exponentiate = {k: np.exp(v - max_value) for k, v in negated.items()}\n",
    "    total = sum(exponentiate.values())\n",
    "    softmax = {k: v / total for k, v in exponentiate.items()}\n",
    "    return softmax\n",
    "\n",
    "def balance_cluster_to_n(cluster_to_n, cluster_to_len):\n",
    "    \"\"\"\n",
    "        Balances the target number of samples for each cluster to ensure it doesn't exceed the actual size of the cluster.\n",
    "\n",
    "        The function first calculates the surplus (i.e., the excess of the target number over the actual size) for each cluster. \n",
    "        Then, it distributes the total surplus proportionally among the clusters that have a deficit (i.e., the target number is less than the actual size). \n",
    "        If after this distribution, there's still a deficit (i.e., the sum of target numbers is less than the sum of actual sizes), the function \n",
    "        increases the target number of the largest clusters one by one until the sum of target numbers equals to the sum of actual sizes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_to_n : dict\n",
    "            A dictionary mapping cluster identifiers to their target number of samples.\n",
    "\n",
    "        cluster_to_len : dict\n",
    "            A dictionary mapping cluster identifiers to the actual size of each cluster.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        balanced : dict\n",
    "            A dictionary mapping cluster identifiers to their balanced target number of samples.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If the sum of target numbers before and after balancing don't match.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    surplus = {key: cluster_to_n[key] - cluster_to_len[key] for key in cluster_to_n if cluster_to_n[key] > cluster_to_len[key]}\n",
    "    balanced = {k:v for k, v in cluster_to_n.items()}\n",
    "    n_to_cluster = {v: k for k, v in cluster_to_n.items()}\n",
    "\n",
    "    for key in surplus:\n",
    "        balanced[key] = cluster_to_len[key]\n",
    "\n",
    "    total_surplus = sum(surplus.values())\n",
    "    initial_n_sum = sum(n for key, n in cluster_to_n.items() if key not in surplus)\n",
    "\n",
    "    for key in balanced:\n",
    "        if key in surplus: continue\n",
    "        surplus_to_add = total_surplus * cluster_to_n[key] / initial_n_sum\n",
    "        new_n = int(cluster_to_n[key] + surplus_to_add)\n",
    "        balanced[key] = min(new_n, cluster_to_len[key])\n",
    "\n",
    "    deficit = sum(cluster_to_n.values()) - sum(balanced.values())\n",
    "\n",
    "    while deficit > 0:\n",
    "        for initial_n in sorted(n_to_cluster, reverse=True):\n",
    "            if (cluster:=n_to_cluster[initial_n]) in surplus: continue\n",
    "            if balanced[cluster] < cluster_to_len[cluster]:\n",
    "                balanced[cluster] += 1\n",
    "                deficit -= 1\n",
    "    \n",
    "    assert sum(cluster_to_n.values()) == sum(balanced.values()), f\"Before balancing had {sum(cluster_to_n.values())}, post balancing = {sum(balanced.values())}\"\n",
    "    return balanced\n",
    "\n",
    "def sample_clusters_for_active_learning(cluster_to_scores, n_samples, path_to_clusters, probability_type='softmax', remove_positives=False, lowest_score=1):\n",
    "    \"\"\"\n",
    "        Sample molecules from clusters for active learning purposes, considering previously docked molecules and balancing the sampling among clusters.\n",
    "\n",
    "        This function uses either softmax or uniform probabilities to determine how many molecules to sample from each cluster. The function then samples \n",
    "        the required number of new molecules (i.e., those not present in docked_mols) from each cluster. The sampling is balanced to ensure the target number \n",
    "        doesn't exceed the actual size of the cluster.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_to_scores : dict\n",
    "            A dictionary mapping cluster identifiers to their scores.\n",
    "\n",
    "        n_samples : int\n",
    "            The total number of molecules to sample.\n",
    "\n",
    "        path_to_clusters : str\n",
    "            The path to a pickle file storing a dictionary that maps each cluster to a list of molecules.\n",
    "\n",
    "        probability_type : str, optional (default='softmax')\n",
    "            The type of probability distribution used to determine the number of samples per cluster. \n",
    "            Options are 'softmax' and 'uniform'.\n",
    "\n",
    "        remove_positives : bool, optional (default=False)\n",
    "            Only used when probability_type is 'uniform'. If True, positive scores are removed after negation.\n",
    "\n",
    "        lowest_score : int, optional (default=1)\n",
    "            Only used when probability_type is 'uniform'. This value is added to all scores if the minimum score is less than zero.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        training : list\n",
    "            A list of randomly sampled molecules for active learning.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        KeyError\n",
    "            If an unsupported probability_type is provided.\n",
    "        AssertionError\n",
    "            If the number of sampled molecules doesn't equal to n_samples.\n",
    "\n",
    "    \"\"\"\n",
    "    if probability_type == 'softmax':\n",
    "        probability_function = _preprocess_scores_softmax \n",
    "    elif probability_type == 'uniform':\n",
    "        probability_function = lambda x: _preprocess_scores_uniformly(x, remove_positives, lowest_score)\n",
    "    else:\n",
    "        raise KeyError(\"Only uniform and softmax probabilities are supported\")\n",
    "    cluster_to_mols = pickle.load(open(path_to_clusters, 'rb'))\n",
    "    cluster_to_samples = pickle.load(open(path_to_clusters.split('.')[0] + '_samples.pickle', 'rb'))\n",
    "    docked_mols = {smile for smiles in cluster_to_samples.values() for smile in smiles}\n",
    "    cluster_to_new_mols = {k: [smile for smile in v if smile not in docked_mols] for k, v in cluster_to_mols.items()}\n",
    "\n",
    "    probabilities = probability_function(cluster_to_scores)\n",
    "    cluster_to_n = {k: int(v * n_samples) for k, v in probabilities.items()}\n",
    "    max_cluster_id, max_prob = None, 0\n",
    "    for cluster, prob in probabilities.items():\n",
    "        if prob > max_prob:\n",
    "            max_cluster_id, max_prob = cluster, prob\n",
    "    cluster_to_n[max_cluster_id] += n_samples - sum(cluster_to_n.values())\n",
    "\n",
    "    cluster_to_len = {k: len(v) for k, v in cluster_to_new_mols.items()}\n",
    "    balanced = balance_cluster_to_n(cluster_to_n, cluster_to_len)\n",
    "\n",
    "    training = []\n",
    "    for i, (cluster, n) in enumerate(balanced.items()):\n",
    "        training.extend(np.random.choice(cluster_to_new_mols[cluster], n, replace=False))\n",
    "        \n",
    "    assert len(training) == n_samples, f\"{len(training)=} != {n_samples=}\"\n",
    "    return training\n",
    "\n",
    "sample_clusters_for_active_learning({k: random.random() for k in range(nclusters)}, n_samples=10, path_to_clusters=CHECKPOINTS + 'cluster_to_samples_07_07.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
