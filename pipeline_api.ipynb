{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/Users/morgunov/batista/Summer/pipeline/'\n",
    "PRETRAINING_PATH = BASE_PATH + '1. Pretraining/'\n",
    "GENERATION_PATH = BASE_PATH + '2. Generation/'\n",
    "SAMPLING_PATH = BASE_PATH + '3. Sampling/'\n",
    "DIFFDOCK_PATH = BASE_PATH + '4. DiffDock/'\n",
    "SCORING_PATH = BASE_PATH + '5. Scoring/'\n",
    "AL_PATH = BASE_PATH + '6. ActiveLearning/'\n",
    "MODE = 'Active Learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG_DICT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrequested \u001b[39m\u001b[39m{\u001b[39;00mMODE\u001b[39m}\u001b[39;00m\u001b[39m but only Pretraining and Active Learning are supported\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m inference_parameters \u001b[39m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m64\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgen_size\u001b[39m\u001b[39m\"\u001b[39m: NUM_TO_GENERATE,\n\u001b[1;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgeneration_context\u001b[39m\u001b[39m\"\u001b[39m: CONTEXT,\n\u001b[1;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mload_ckpt_path\u001b[39m\u001b[39m\"\u001b[39m: LOAD_CKPT_PATH,\n\u001b[1;32m     22\u001b[0m }\n\u001b[0;32m---> 23\u001b[0m CONFIG_DICT\u001b[39m.\u001b[39mupdate({\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgeneration_path\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mGENERATION_PATH\u001b[39m}\u001b[39;00m\u001b[39msmiles/\u001b[39m\u001b[39m{\u001b[39;00mCURRENT_CYCLE_PREFIX\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minference_temp\u001b[39m\u001b[39m\"\u001b[39m: TEMPERATURE,})\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGeneration will use the following dataset descriptors\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(CONFIG_DICT[\u001b[39m'\u001b[39m\u001b[39mdesc_path\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m6\u001b[39m:]))\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m... and following model weights\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inference_parameters[\u001b[39m'\u001b[39m\u001b[39mload_ckpt_path\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m6\u001b[39m:]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CONFIG_DICT' is not defined"
     ]
    }
   ],
   "source": [
    "# @title Run this cell and check all parameters!\n",
    "CONTEXT = \"!\" # @param {type:\"string\"}\n",
    "TEMPERATURE = 1.0 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "VAL_FNAME = \"moses_and_binding_no_rare_tokens_test.csv.gz\"\n",
    "LOAD_CKPT_NAME = \"model1_softsub_al2.pt\" #@param [\"GPT_pretrain_07_14_23:39_1end_ignore_moses+bindingdb.pt\", \"model1_softsub_al1.pt\", \"model1_softsub_al2.pt\"] {allow-input: true}\n",
    "NUM_TO_GENERATE = 100_000 #@param\n",
    "# @markdown Please use the following naming scheme: \"model1_al{round of AL}\"\n",
    "# GENERATION_FNAME = \"model1_al1\" #@param {type:\"string\"}\n",
    "#\n",
    "if MODE == 'Pretraining':\n",
    "    LOAD_CKPT_PATH = f\"{PRETRAINING_PATH}model_weights/{LOAD_CKPT_NAME}\"\n",
    "elif MODE == 'Active Learning':\n",
    "    LOAD_CKPT_PATH = f\"{AL_PATH}model_weights/{LOAD_CKPT_NAME}\"\n",
    "else:\n",
    "    raise KeyError(f'requested {MODE} but only Pretraining and Active Learning are supported')\n",
    "\n",
    "inference_parameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"gen_size\": NUM_TO_GENERATE,\n",
    "    \"generation_context\": CONTEXT,\n",
    "    \"load_ckpt_path\": LOAD_CKPT_PATH,\n",
    "}\n",
    "CONFIG_DICT.update({\n",
    "    \"generation_path\": f\"{GENERATION_PATH}smiles/{CURRENT_CYCLE_PREFIX}\",\n",
    "    \"inference_temp\": TEMPERATURE,})\n",
    "print(\"Generation will use the following dataset descriptors\\n\", '/'.join(CONFIG_DICT['desc_path'].split('/')[6:]))\n",
    "print(\"... and following model weights\\n\", '/'.join(inference_parameters['load_ckpt_path'].split('/')[6:]))\n",
    "print(\"... and molecules will be saved to\\n\", '/'.join(CONFIG_DICT['generation_path'].split('/')[6:])+ f\"_temp{TEMPERATURE}_processed\" +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_SMILES(config_dict, inference_parameters):\n",
    "    regex = re.compile(REGEX_PATTERN)\n",
    "    dataset = SMILESDataset()\n",
    "    dataset.load_desc_attributes(config_dict[\"desc_path\"])\n",
    "\n",
    "    mconf = GPTConfig(dataset.vocab_size, dataset.block_size, **config_dict)\n",
    "    model = GPT(mconf).to(config_dict[\"device\"])\n",
    "    model.load_state_dict(torch.load(inference_parameters[\"load_ckpt_path\"], map_location=torch.device(config_dict[\"device\"])))\n",
    "    model.to(config_dict[\"device\"])\n",
    "    torch.compile(model)\n",
    "\n",
    "    # load parameters into the model\n",
    "    block_size = model.get_block_size()\n",
    "    assert (block_size == dataset.block_size), \"Warning: model block size and dataset block size are different\"\n",
    "    gen_iter = int(np.ceil(inference_parameters[\"gen_size\"] / inference_parameters[\"batch_size\"]))\n",
    "    stoi = dataset.stoi  # define dictionary to map strings to integers\n",
    "    itos = dataset.itos  # define dictionary to map integers to strings\n",
    "    molecules = []\n",
    "    completions = []\n",
    "    for i in tqdm(range(gen_iter)):\n",
    "        # create an input tensor by converting 'context' to a tensor of token indices, repeat this batch times along the batch dimension\n",
    "        x = (torch.tensor([stoi[s] for s in regex.findall(inference_parameters[\"generation_context\"])], dtype=torch.long,)[None, ...]\n",
    "            .repeat(inference_parameters[\"batch_size\"], 1).to(config_dict[\"device\"]))\n",
    "        y = sample(model, x, block_size, temperature=config_dict[\"inference_temp\"])\n",
    "        for gen_mol in y:\n",
    "            completion = \"\".join([itos[int(i)] for i in gen_mol])  # convert generated molecule from list of integers to list of strings and concatenate to one string\n",
    "            completions.append(completion)\n",
    "            if \"~\" not in completion: continue\n",
    "            mol_string = completion[1 : completion.index(\"~\")]\n",
    "            mol = get_mol(mol_string)  # convert the string representation of the molecule to an rdkit Mol object\n",
    "            if mol is not None:\n",
    "                molecules.append(mol)\n",
    "    completions_df = pd.DataFrame({\"smiles\": completions})\n",
    "    completions_df.to_csv(config_dict[\"generation_path\"]+ f\"_temp{config_dict['inference_temp']}_completions.csv\")\n",
    "    molecules_df = pd.DataFrame([{\"smiles\": Chem.MolToSmiles(i)} for i in molecules])\n",
    "\n",
    "    # canon_smiles = [canonic_smiles(s) for s in molecules_df[\"smiles\"]]\n",
    "    unique_smiles = list(set(molecules_df['smiles']))\n",
    "    data = pd.read_csv(config_dict[\"train_path\"])  # load training data\n",
    "    novel_ratio = check_novelty(unique_smiles, set(data[config_dict[\"smiles_key\"]]))  # calculate novelty ratio from generated SMILES and training SMILES\n",
    "\n",
    "    molecules_df[\"validity\"] = np.round(len(molecules_df) / len(completions), 3)\n",
    "    molecules_df[\"unique\"] = np.round(len(unique_smiles) / len(molecules_df), 3)\n",
    "    molecules_df[\"novelty\"] = np.round(novel_ratio / 100, 3)\n",
    "    molecules_df.to_csv(config_dict[\"generation_path\"]+ f\"_temp{config_dict['inference_temp']}_processed.csv\")\n",
    "    # print all evaluation metrics using function from moses package\n",
    "    # print(moses.get_all_metrics(list(results['smiles'].values), device=config_dict['device']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process GPT Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem\n",
    "import rdkit.Chem.Descriptors\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def descriptors_for_gpt_predictions(path_to_predicted, path_to_save):\n",
    "    gpt_mols = pd.read_csv(path_to_predicted)\n",
    "    keySet = None\n",
    "    keyToData = {}\n",
    "    pbar = tqdm(gpt_mols.iterrows(), total=len(gpt_mols))\n",
    "    for index, row in pbar:\n",
    "        smile = row['smiles']\n",
    "        mol = rdkit.Chem.MolFromSmiles(smile)\n",
    "        if not mol: continue\n",
    "        mol_data = rdkit.Chem.Descriptors.CalcMolDescriptors(mol)\n",
    "        if keySet is None:\n",
    "            keySet = set(mol_data.keys())\n",
    "        for key in keySet:\n",
    "            keyToData.setdefault(key, []).append(mol_data[key])\n",
    "        keyToData.setdefault('smiles', []).append(smile)\n",
    "    gpt_df = pd.DataFrame(keyToData)\n",
    "    gpt_df.to_pickle(path_to_save)\n",
    "    return gpt_df\n",
    "\n",
    "BASE = '/Users/morgunov/batista/Summer/'\n",
    "CHECKPOINTS = BASE + 'bindingDB/checkpoints/'\n",
    "GPT_DATA = BASE + 'data/'\n",
    "gpt_df = descriptors_for_gpt_predictions(path_to_predicted=GPT_DATA + 'molgpt_generated_nocond_06_10_fintetune2.csv', path_to_save=CHECKPOINTS+f'gptMols_ft2.pickle')\n",
    "gpt_df = descriptors_for_gpt_predictions(path_to_predicted=GPT_DATA + 'molgpt_generated_nocond_06_10.csv', path_to_save=CHECKPOINTS+f'gptMols.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA-Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99095, 100), (99095,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def project_into_pca_space(path_to_pca, path_to_mols):\n",
    "    scaler, pca = pickle.load(open(path_to_pca, 'rb'))\n",
    "    gptMols = pd.read_pickle(path_to_mols)#.sample(n=10)\n",
    "    return gptMols['smiles'], pca.transform(scaler.transform(gptMols[scaler.get_feature_names_out()]))\n",
    "\n",
    "gpt_smiles, pca_transformed = project_into_pca_space(path_to_pca=PICKLES + 'scaler_pca_moses+bindingdb.pkl', path_to_mols=INFERENCES + 'GPT_pretrain_inference_07_14_23_39_1end_ignore_moses+bindingdb_temp1.0_descriptors.pkl')\n",
    "pca_transformed.shape, gpt_smiles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring KMeans clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "def _cluster_mols_experimental_loss(mols, n_clusters, n_iter):\n",
    "    min_loss, best_kmeans = float('inf'), None\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        if kmeans.inertia_ < min_loss:\n",
    "            min_loss = kmeans.inertia_\n",
    "            best_kmeans = kmeans\n",
    "    return best_kmeans\n",
    "\n",
    "def _cluster_mols_experimental_variance(mols, n_clusters, n_iter):\n",
    "    max_variance, best_kmeans = float('-inf'), None\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n",
    "        if (variance:=np.var(counts)) > max_variance:\n",
    "            max_variance = variance\n",
    "            best_kmeans = kmeans\n",
    "    return best_kmeans\n",
    "\n",
    "def _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile):\n",
    "    inertias = []\n",
    "    variances = []\n",
    "    km_objs = []\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        counts = np.unique(kmeans.labels_, return_counts=True)[1]\n",
    "        variances.append(np.var(counts))\n",
    "        km_objs.append(kmeans)\n",
    "    loss_var_kmeans_triples = sorted(zip(inertias, variances, km_objs), key=lambda x: x[0])\n",
    "    lowest_n = loss_var_kmeans_triples[:int(len(loss_var_tuples) * mixed_objective_loss_quantile)]\n",
    "    sorted_by_variance = sorted(lowest_n, key=lambda x: x[1])\n",
    "    return sorted_by_variance[0][2]\n",
    "\n",
    "def _cluster_mols_experimental(mols, n_clusters, save_path, n_iter=1, objective='loss', mixed_objective_loss_quantile=0.1):\n",
    "    if n_iter == 1:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "    elif objective == 'loss':\n",
    "        kmeans = _cluster_mols_experimental_loss(mols, n_clusters, n_iter)\n",
    "    elif kmeans == 'variance':\n",
    "        kmeans = _cluster_mols_experimental_variance(mols, n_clusters, n_iter)\n",
    "    elif objective == 'mixed':\n",
    "        kmeans = _cluster_mols_experimental_mixed(mols, n_clusters, n_iter, mixed_objective_loss_quantile)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown objective {objective}')\n",
    "\n",
    "    pickle.dump(best_kmeans, open(save_path, 'wb'))\n",
    "    return kmeans\n",
    "\n",
    "out = _cluster_mols_experimental(mols=pca_transformed, n_clusters=100, n_iter=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.title_size = 20\n",
    "        self.axis_title_size = 14\n",
    "        self.tick_font_size = 12\n",
    "        self.text_color=\"#333333\"\n",
    "        self.background = \"white\"\n",
    "        self.grid_color = \"#e2e2e2\"\n",
    "        self.line_color = \"#000000\"\n",
    "        self.font_family = 'Helvetica'\n",
    "        self.width = 600\n",
    "        self.height = 400\n",
    "        self.title = ''\n",
    "        self.xaxis_title = ''\n",
    "        self.yaxis_title = ''\n",
    "    \n",
    "    def update_parameters(self, params):\n",
    "        for key, val in params.items():\n",
    "            setattr(self, key, val)\n",
    "        \n",
    "\n",
    "    def style_figure(self, figure):\n",
    "        figure.update_layout({\n",
    "            'margin': {'t': 50, 'b': 50, 'l': 50, 'r': 50},\n",
    "            'plot_bgcolor': self.background,\n",
    "            'paper_bgcolor': self.background,\n",
    "            'title': {\n",
    "                'text': self.title,\n",
    "                'font': {\n",
    "                    'size': self.title_size,\n",
    "                    'color': self.text_color,\n",
    "                    'family': self.font_family\n",
    "                },\n",
    "            },\n",
    "            'height': self.height,  # Set fixed size ratio 3:4\n",
    "            'width': self.width, \n",
    "            'font': {\n",
    "                'family': self.font_family,\n",
    "                'size': self.tick_font_size,\n",
    "                'color': self.text_color\n",
    "            },\n",
    "            'legend': {\n",
    "                'font': {\n",
    "                    'family': self.font_family,\n",
    "                    'size': self.tick_font_size,\n",
    "                    'color': self.text_color\n",
    "                },\n",
    "            },\n",
    "        })\n",
    "\n",
    "        # Setting the title size and color and grid for both x and y axes\n",
    "        figure.update_xaxes(\n",
    "            title=self.xaxis_title,\n",
    "            title_font={'size': self.axis_title_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            tickfont={'size': self.tick_font_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor=self.grid_color,\n",
    "            linecolor=self.line_color,  # make x axis line visible\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "        figure.update_yaxes(\n",
    "            title=self.yaxis_title,\n",
    "            title_standoff=0,\n",
    "            title_font={'size': self.axis_title_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            tickfont={'size': self.tick_font_size, 'color': self.text_color, 'family': self.font_family},\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor=self.grid_color,\n",
    "            linecolor=self.line_color,  # make y axis line visible\n",
    "            linewidth=2\n",
    "        )\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "loss_to_var = {loss:var for loss, var in zip(out[0], out[1])}\n",
    "sort_loss, variances = zip(*sorted(loss_to_var.items(), key=lambda x: x[0]))\n",
    "\n",
    "graph = Graph()\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(sort_loss)), y=sort_loss, mode='markers', name='Loss'), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(sort_loss)), y=variances, mode='markers', name='Variances'), secondary_y=True)\n",
    "graph.style_figure(fig)\n",
    "fig.show()\n",
    "fig.write_html(CHECKPOINTS + 'kmeans_sort_loss_vs_variance.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_to_var = {loss:var for loss, var in zip(out[0], out[1])}\n",
    "loss, sort_variances = zip(*sorted(loss_to_var.items(), key=lambda x: x[1]))\n",
    "\n",
    "graph = Graph()\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss)), y=loss, mode='markers', name='Loss'), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss)), y=sort_variances, mode='markers', name='Variances'), secondary_y=True)\n",
    "graph.style_figure(fig)\n",
    "fig.show()\n",
    "fig.write_html(CHECKPOINTS + 'kmeans_sort_loss_vs_variance.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_based_on_distance_percentiles(elements, distances, n_samples, n_percentiles):\n",
    "    assert len(elements) == len(distances), \"Elements and distances lists must be of the same length\"\n",
    "    assert n_samples <= len(elements), \"Number of samples cannot exceed the total number of elements\"\n",
    "    assert n_percentiles > 0, \"Number of percentiles must be a positive integer\"\n",
    "    \n",
    "    # Sort elements and distances together based on ascending order of distances\n",
    "    distances, elements = zip(*sorted(zip(distances, elements)))\n",
    "    \n",
    "    # Compute the percentiles\n",
    "    percentile_values = [np.percentile(distances, p * 100 / n_percentiles) for p in range(1, n_percentiles)]\n",
    "    percentile_values.append(np.inf)  # the highest percentile encompasses all remaining points\n",
    "    \n",
    "    # Divide data into percentiles\n",
    "    elements_by_percentile = []\n",
    "    start_index = 0\n",
    "    for percentile_value in percentile_values:\n",
    "        end_index = start_index\n",
    "        while end_index < len(distances) and distances[end_index] <= percentile_value:\n",
    "            end_index += 1\n",
    "        elements_by_percentile.append(elements[start_index:end_index])\n",
    "        start_index = end_index\n",
    "    \n",
    "    # Sample from each percentile\n",
    "    samples_per_percentile = n_samples // n_percentiles\n",
    "    remaining_samples = n_samples % n_percentiles\n",
    "    samples = []\n",
    "    for i, percentile_elements in enumerate(elements_by_percentile):\n",
    "        if len(percentile_elements) <= samples_per_percentile:\n",
    "            # If we don't have enough elements in this percentile, take them all and\n",
    "            # add the deficit to remaining_samples so it can be distributed among subsequent percentiles\n",
    "            samples += percentile_elements\n",
    "            remaining_samples += samples_per_percentile - len(percentile_elements)\n",
    "        else:\n",
    "            # Sample elements from this percentile\n",
    "            samples += list(np.random.choice(percentile_elements, size=samples_per_percentile, replace=False))\n",
    "        \n",
    "        # Distribute remaining_samples among the last n_percentiles\n",
    "        if i >= n_percentiles - remaining_samples:\n",
    "            extra_samples = min(len(percentile_elements) - samples_per_percentile, 1)\n",
    "            samples += list(np.random.choice([el for el in percentile_elements if el not in samples], size=extra_samples, replace=False))\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint as pp \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def _cluster_mols(mols, n_clusters, save_path, n_iter=1):\n",
    "    \"\"\"\n",
    "        Performs K-Means clustering on a given list of molecules and saves the model to a specified file.\n",
    "\n",
    "        This function will apply the K-Means algorithm to the input list of molecules. If n_iter is set to 1 (default), the function will perform the clustering once and return the KMeans object. If n_iter is set to more than 1, the function will perform the clustering n_iter times and return the KMeans object with the lowest inertia (i.e., the sum of squared distances of samples to their closest cluster center). The function will save the KMeans object to a file at the specified save_path using pickle.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "            mols : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            The input samples where n_samples is the number of samples and n_features is the number of features.\n",
    "\n",
    "            n_clusters : int\n",
    "            The number of clusters to form as well as the number of centroids to generate.\n",
    "\n",
    "            save_path : str\n",
    "            The path (including file name) where the resulting KMeans object should be saved.\n",
    "\n",
    "            n_iter : int, optional (default=1)\n",
    "            The number of times to perform the clustering. If greater than 1, the function will return the KMeans object with the lowest inertia.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            kmeans : sklearn.cluster._kmeans.KMeans\n",
    "            A KMeans instance trained on the input molecules. If n_iter is greater than 1, it's the best performing model (lowest inertia) from all iterations.\n",
    "\n",
    "    \"\"\"\n",
    "    if n_iter == 1:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        pickle.dump(kmeans, open(save_path, 'wb'))\n",
    "        return kmeans\n",
    "    best_kmeans = None\n",
    "    best_inertia = float('inf')\n",
    "    for _ in range(n_iter):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init='k-means++').fit(mols)\n",
    "        if kmeans.inertia_ < best_inertia:\n",
    "            best_kmeans = kmeans\n",
    "            best_inertia = kmeans.inertia_\n",
    "    pickle.dump(best_kmeans, open(save_path, 'wb'))\n",
    "    return best_kmeans\n",
    "\n",
    "def cluster_and_sample(mols, mols_smiles, n_clusters, n_samples, kmeans_save_path, clusters_save_path, diffdock_save_path, \n",
    "                        ensure_correctness=False, path_to_pca=None, probabilistic_sampling=True, load_kmeans=False,\n",
    "                        percentile_sampling=True, n_percentiles=1):\n",
    "    \"\"\"\n",
    "        Clusters a given list of molecules, samples from each cluster, and saves the resulting data to specified files.\n",
    "\n",
    "        This function performs K-Means clustering on the input list of molecules and then samples a specified number of molecules \n",
    "        from each cluster. The function ensures that the number of samples requested from each cluster doesn't exceed the total number \n",
    "        of available molecules. The clustered data and sampled data are saved to specified file paths using pickle.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mols : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            The input samples where n_samples is the number of samples and n_features is the number of features.\n",
    "\n",
    "        mols_smiles : list of str\n",
    "            A list of SMILES strings corresponding to the input molecules.\n",
    "\n",
    "        n_clusters : int\n",
    "            The number of clusters to form as well as the number of centroids to generate.\n",
    "\n",
    "        n_samples : int\n",
    "            The number of samples to draw from each cluster.\n",
    "\n",
    "        kmeans_save_path : str\n",
    "            The path (including file name) where the resulting KMeans object should be saved.\n",
    "\n",
    "        clusters_save_path : str\n",
    "            The path (including file name) where the resulting clusters should be saved.\n",
    "\n",
    "        ensure_correctness : bool, optional (default=False)\n",
    "            If True, performs additional correctness checks, such as comparing SMILES string derived features to features in mols array. \n",
    "            This requires 'path_to_pca' to be set.\n",
    "\n",
    "        path_to_pca : str, optional (default=None)\n",
    "            If ensure_correctness is True, this should be the path to a PCA model used to transform the molecules' descriptors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_to_samples : dict\n",
    "            A dictionary where the keys are cluster labels and the values are lists of sampled SMILES strings from each cluster.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If the number of requested samples exceeds the total number of molecules provided.\n",
    "            If ensure_correctness is True but path_to_pca is None.\n",
    "            If the number of labels returned by the KMeans algorithm differs from the number of molecules.\n",
    "            If features calculated from a smile string differ from features in the mols array.\n",
    "            If the total number of sampled molecules doesn't equal to n_clusters * n_samples.\n",
    "\n",
    "    \"\"\"\n",
    "    assert n_clusters * n_samples <= len(mols), f\"{n_clusters=} * {n_samples=} = {n_clusters*n_samples} requested but only {len(mols)} molecules provided\"\n",
    "    if ensure_correctness:\n",
    "        assert path_to_pca is not None, \"path_to_pca must be provided to ensure correctness\"\n",
    "        scaler, pca = pickle.load(open(path_to_pca, 'rb'))\n",
    "\n",
    "    if load_kmeans:\n",
    "        kmeans = pickle.load(open(kmeans_save_path, 'rb'))\n",
    "    else:\n",
    "        kmeans = _cluster_mols(mols=mols, n_clusters=n_clusters, save_path=kmeans_save_path)\n",
    "        assert len(kmeans.labels_) == len(mols_smiles), \"Number of labels differs from number of molecules\"\n",
    "    distances = kmeans.transform(mols)\n",
    "\n",
    "    cluster_to_mols = {}\n",
    "    cluster_to_distances = {}\n",
    "    for mol, distance, label, smile in zip(mols, distances, kmeans.labels_, mols_smiles):\n",
    "        cluster_to_mols.setdefault(label, []).append(smile)\n",
    "        cluster_to_distances.setdefault(label, []).append(distance.min())\n",
    "        if ensure_correctness: # recalculate descriptors from a smile string and compare to the descriptors in the array\n",
    "            smile_features = pca.transform(scaler.transform(pd.DataFrame({k: [v] for k, v in rdkit.Chem.Descriptors.CalcMolDescriptors(rdkit.Chem.MolFromSmiles(smile)).items()})[scaler.get_feature_names_out()]))\n",
    "            assert np.allclose(smile_features[0], mol), \"Features calculated from a smile string differ from features in the array\"\n",
    "\n",
    "    pickle.dump((kmeans.labels_, cluster_to_distances), open(clusters_save_path.split('.')[0]+'_cl_to_d.pickle', 'wb'))\n",
    "    # What happens below is sampling from each cluster. All the extra code is to ensure that the number of samples requested from each cluster\n",
    "    # doesn't exceed the total number of available molecules. This is done by calculating the average number of molecules per cluster and then\n",
    "    # calculating the number of extra molecules that need to be sampled from each cluster. The extra molecules are then distributed among the\n",
    "    # clusters uniformly. If the number of extra molecules is greater than the number of molecules in a cluster, all\n",
    "    # molecules from that cluster are sampled.\n",
    "    avg_len = np.mean([len(v) for v in cluster_to_mols.values()])\n",
    "    cluster_to_samples = {}\n",
    "    extra_mols = 0\n",
    "    left_to_sample = n_clusters*n_samples\n",
    "    cluster_to_len = {cluster:len(mols) for cluster, mols in cluster_to_mols.items()}\n",
    "    for i, (cluster, _) in enumerate(sorted(cluster_to_len.items(), key=lambda x: x[1], reverse=False)):\n",
    "        smiles = cluster_to_mols[cluster]\n",
    "        if extra_mols > 0:\n",
    "            cur_extra = int(1+extra_mols/(len(cluster_to_mols) - i) * len(smiles)/avg_len)\n",
    "            cur_samples = n_samples + cur_extra\n",
    "            extra_mols -= cur_extra\n",
    "        else:\n",
    "            cur_samples = n_samples\n",
    "        if cur_samples > left_to_sample:\n",
    "            cur_samples = left_to_sample\n",
    "\n",
    "        if len(smiles) > cur_samples:\n",
    "            if probabilistic_sampling:\n",
    "                cluster_to_samples[cluster] = np.random.choice(smiles, cur_samples, p=cluster_to_distances[cluster]/np.sum(cluster_to_distances[cluster]), replace=False)\n",
    "            elif percentile_sampling:\n",
    "                cluster_to_samples[cluster] = sample_based_on_distance_percentiles(smiles, cluster_to_distances[cluster], n_samples=cur_samples, n_percentiles=n_percentiles)\n",
    "            else:\n",
    "                cluster_to_samples[cluster] = np.random.choice(smiles, cur_samples, replace=False)\n",
    "            left_to_sample -= cur_samples\n",
    "        else:\n",
    "            cluster_to_samples[cluster] = smiles\n",
    "            left_to_sample -= len(smiles)\n",
    "            extra_mols += cur_samples - len(smiles)\n",
    "\n",
    "    assert (n_sampled:=sum(len(vals) for vals in cluster_to_samples.values())) == n_clusters*n_samples, f\"Sampled {n_sampled} but were requested {n_clusters*n_samples}\"\n",
    "    pickle.dump(cluster_to_mols, open(clusters_save_path, 'wb'))\n",
    "    pickle.dump(cluster_to_samples, open(clusters_save_path.split('.')[0] + '_samples.pickle', 'wb'))\n",
    "    keyToData = {}\n",
    "    for cluster, mols in cluster_to_samples.items():\n",
    "        for mol in mols:\n",
    "            keyToData.setdefault('smiles', []).append(mol)\n",
    "            keyToData.setdefault('cluster_id', []).append(cluster)\n",
    "    pd.DataFrame(keyToData).to_csv(diffdock_save_path)\n",
    "    return cluster_to_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclusters = 100\n",
    "# bkmeans = _cluster_mols(mols=pca_transformed, n_clusters=nclusters, save_path=CHECKPOINTS + 'k100means_07_11.pickle')\n",
    "c_to_s = cluster_and_sample(mols=pca_transformed, mols_smiles=gpt_smiles, n_clusters=nclusters, n_samples=10, \n",
    "                   kmeans_save_path=PICKLES + 'k100means_07_14_23_39_1end_ignore_moses+bindingdb.pickle', ensure_correctness=False, path_to_pca=PICKLES + 'scaler_pca_moses+bindingdb.pickle',\n",
    "                   clusters_save_path=PICKLES + 'cluster_to_samples_07_16.pickle', probabilistic_sampling=False, percentile_sampling=True, n_percentiles=4,\n",
    "                   diffdock_save_path=INFERENCES + f'samples_percentiles_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, cluster_to_distances = pickle.load(open(CHECKPOINTS + 'cluster_to_samples_07_11_cl_to_d.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = np.array([distance for distances in cluster_to_distances.values() if len(distances) > 10 for distance in distances])\n",
    "min_d, max_d = np.min(all_distances), np.max(all_distances)\n",
    "hist, bin_edges = np.histogram(all_distances, bins=10)\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "sine_distances = np.sin(2*np.pi*bin_centers) * max(all_distances)\n",
    "# sine_distances = np.sin(all_distances*2*np.pi/np.sum(all_distances))\n",
    "len(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Plot a histogram based on all distances (which are python lists) in cluster_to_distances\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(x=all_distances, xbins=dict(start=min_d, end=max_d, size=0.01*(max_d-min_d)), name='all_distances'))\n",
    "fig.add_trace(go.Scatter(x=bin_centers, y=sine_distances, name='sine transformed distances'))\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Plot a histogram based on all distances (which are python lists) in cluster_to_distances\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pca_transformed[:, 7], y=pca_transformed[:, 50], marker=dict(color=labels), text=labels, mode='markers', name='gpt generated'))\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training dataset for active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _preprocess_scores_uniformly(scores, remove_positives=False, lowest_score=1):\n",
    "    \"\"\"\n",
    "        Preprocesses a dictionary of scores by negating and normalizing them.\n",
    "\n",
    "        The function negates all scores and optionally removes positive scores. If the minimum value among the negated scores \n",
    "        is less than zero, it shifts all values by subtracting the minimum value and adding 'lowest_score'. The final step is \n",
    "        to normalize the scores so that their total sum equals to 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores : dict\n",
    "            A dictionary of scores where the keys are identifiers and the values are their corresponding scores.\n",
    "\n",
    "        remove_positives : bool, optional (default=False)\n",
    "            If True, all positive scores are removed after negation.\n",
    "\n",
    "        lowest_score : int, optional (default=1)\n",
    "            This value is added to all scores if the minimum score is less than zero.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        normalized : dict\n",
    "            The normalized dictionary of scores.\n",
    "\n",
    "    \"\"\"\n",
    "    negated = {k: -v for k, v in scores.items()}\n",
    "    min_value = min(negated.values())\n",
    "    if min_value < 0:\n",
    "        if remove_positives:\n",
    "            negated = {k: v for k, v in negated.items() if v > 0}\n",
    "        else:\n",
    "            negated = {k: v - min_value + lowest_score for k, v in negated.items()}\n",
    "    total = sum(negated.values())\n",
    "    normalized = {k: v / total for k, v in negated.items()}\n",
    "    return normalized\n",
    "\n",
    "def _preprocess_scores_softmax(scores):\n",
    "    negated = {k: -v for k, v in scores.items()}\n",
    "    max_value = max(negated.values())\n",
    "    exponentiate = {k: np.exp(v - max_value) for k, v in negated.items()}\n",
    "    total = sum(exponentiate.values())\n",
    "    softmax = {k: v / total for k, v in exponentiate.items()}\n",
    "    return softmax\n",
    "\n",
    "def balance_cluster_to_n(cluster_to_n, cluster_to_len):\n",
    "    \"\"\"\n",
    "        Balances the target number of samples for each cluster to ensure it doesn't exceed the actual size of the cluster.\n",
    "\n",
    "        The function first calculates the surplus (i.e., the excess of the target number over the actual size) for each cluster. \n",
    "        Then, it distributes the total surplus proportionally among the clusters that have a deficit (i.e., the target number is less than the actual size). \n",
    "        If after this distribution, there's still a deficit (i.e., the sum of target numbers is less than the sum of actual sizes), the function \n",
    "        increases the target number of the largest clusters one by one until the sum of target numbers equals to the sum of actual sizes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_to_n : dict\n",
    "            A dictionary mapping cluster identifiers to their target number of samples.\n",
    "\n",
    "        cluster_to_len : dict\n",
    "            A dictionary mapping cluster identifiers to the actual size of each cluster.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        balanced : dict\n",
    "            A dictionary mapping cluster identifiers to their balanced target number of samples.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If the sum of target numbers before and after balancing don't match.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    surplus = {key: cluster_to_n[key] - cluster_to_len[key] for key in cluster_to_n if cluster_to_n[key] > cluster_to_len[key]}\n",
    "    balanced = {k:v for k, v in cluster_to_n.items()}\n",
    "    n_to_cluster = {v: k for k, v in cluster_to_n.items()}\n",
    "\n",
    "    for key in surplus:\n",
    "        balanced[key] = cluster_to_len[key]\n",
    "\n",
    "    total_surplus = sum(surplus.values())\n",
    "    initial_n_sum = sum(n for key, n in cluster_to_n.items() if key not in surplus)\n",
    "\n",
    "    for key in balanced:\n",
    "        if key in surplus: continue\n",
    "        surplus_to_add = total_surplus * cluster_to_n[key] / initial_n_sum\n",
    "        new_n = int(cluster_to_n[key] + surplus_to_add)\n",
    "        balanced[key] = min(new_n, cluster_to_len[key])\n",
    "\n",
    "    deficit = sum(cluster_to_n.values()) - sum(balanced.values())\n",
    "\n",
    "    while deficit > 0:\n",
    "        for initial_n in sorted(n_to_cluster, reverse=True):\n",
    "            if (cluster:=n_to_cluster[initial_n]) in surplus: continue\n",
    "            if balanced[cluster] < cluster_to_len[cluster]:\n",
    "                balanced[cluster] += 1\n",
    "                deficit -= 1\n",
    "    \n",
    "    assert sum(cluster_to_n.values()) == sum(balanced.values()), f\"Before balancing had {sum(cluster_to_n.values())}, post balancing = {sum(balanced.values())}\"\n",
    "    return balanced\n",
    "\n",
    "def sample_clusters_for_active_learning(cluster_to_scores, n_samples, path_to_clusters, probability_type='softmax', remove_positives=False, lowest_score=1):\n",
    "    \"\"\"\n",
    "        Sample molecules from clusters for active learning purposes, considering previously docked molecules and balancing the sampling among clusters.\n",
    "\n",
    "        This function uses either softmax or uniform probabilities to determine how many molecules to sample from each cluster. The function then samples \n",
    "        the required number of new molecules (i.e., those not present in docked_mols) from each cluster. The sampling is balanced to ensure the target number \n",
    "        doesn't exceed the actual size of the cluster.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_to_scores : dict\n",
    "            A dictionary mapping cluster identifiers to their scores.\n",
    "\n",
    "        n_samples : int\n",
    "            The total number of molecules to sample.\n",
    "\n",
    "        path_to_clusters : str\n",
    "            The path to a pickle file storing a dictionary that maps each cluster to a list of molecules.\n",
    "\n",
    "        probability_type : str, optional (default='softmax')\n",
    "            The type of probability distribution used to determine the number of samples per cluster. \n",
    "            Options are 'softmax' and 'uniform'.\n",
    "\n",
    "        remove_positives : bool, optional (default=False)\n",
    "            Only used when probability_type is 'uniform'. If True, positive scores are removed after negation.\n",
    "\n",
    "        lowest_score : int, optional (default=1)\n",
    "            Only used when probability_type is 'uniform'. This value is added to all scores if the minimum score is less than zero.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        training : list\n",
    "            A list of randomly sampled molecules for active learning.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        KeyError\n",
    "            If an unsupported probability_type is provided.\n",
    "        AssertionError\n",
    "            If the number of sampled molecules doesn't equal to n_samples.\n",
    "\n",
    "    \"\"\"\n",
    "    if probability_type == 'softmax':\n",
    "        probability_function = _preprocess_scores_softmax \n",
    "    elif probability_type == 'uniform':\n",
    "        probability_function = lambda x: _preprocess_scores_uniformly(x, remove_positives, lowest_score)\n",
    "    else:\n",
    "        raise KeyError(\"Only uniform and softmax probabilities are supported\")\n",
    "    cluster_to_mols = pickle.load(open(path_to_clusters, 'rb'))\n",
    "    cluster_to_samples = pickle.load(open(path_to_clusters.split('.')[0] + '_samples.pickle', 'rb'))\n",
    "    docked_mols = {smile for smiles in cluster_to_samples.values() for smile in smiles}\n",
    "    cluster_to_new_mols = {k: [smile for smile in v if smile not in docked_mols] for k, v in cluster_to_mols.items()}\n",
    "\n",
    "    probabilities = probability_function(cluster_to_scores)\n",
    "    cluster_to_n = {k: int(v * n_samples) for k, v in probabilities.items()}\n",
    "    max_cluster_id, max_prob = None, 0\n",
    "    for cluster, prob in probabilities.items():\n",
    "        if prob > max_prob:\n",
    "            max_cluster_id, max_prob = cluster, prob\n",
    "    cluster_to_n[max_cluster_id] += n_samples - sum(cluster_to_n.values())\n",
    "\n",
    "    cluster_to_len = {k: len(v) for k, v in cluster_to_new_mols.items()}\n",
    "    balanced = balance_cluster_to_n(cluster_to_n, cluster_to_len)\n",
    "\n",
    "    training = []\n",
    "    for i, (cluster, n) in enumerate(balanced.items()):\n",
    "        training.extend(np.random.choice(cluster_to_new_mols[cluster], n, replace=False))\n",
    "        \n",
    "    assert len(training) == n_samples, f\"{len(training)=} != {n_samples=}\"\n",
    "    return training\n",
    "\n",
    "sample_clusters_for_active_learning({k: random.random() for k in range(nclusters)}, n_samples=10, path_to_clusters=CHECKPOINTS + 'cluster_to_samples_07_07.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
